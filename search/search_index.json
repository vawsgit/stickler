{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"Core-Concepts/","title":"Core Concepts","text":"<p>This section covers the fundamental evaluation principles that power Stickler's comparison engine. Understanding these concepts is essential for effectively using the library to evaluate structured data.</p>"},{"location":"Core-Concepts/#whats-covered","title":"What's Covered","text":"<p>Classification Logic explains how Stickler categorizes comparisons into confusion matrix metrics (TP, FP, FN, TN, FD, FA). It defines the rules for classifying simple values, lists, and nested objects, including how null values and empty collections are handled.</p> <p>Hungarian Matching for Lists describes the algorithm used to optimally pair elements when comparing lists of structured objects. This includes detailed examples of how similarity scores determine matches and how unmatched items are classified.</p> <p>Threshold-Gated Recursive Evaluation introduces the core principle that governs when Stickler performs detailed nested field analysis. Only object pairs that meet the similarity threshold receive recursive evaluation, while poorly-matched pairs are treated atomically.</p> <p>Note These concepts work together to provide accurate, interpretable evaluation metrics. The classification logic defines what counts as a match, Hungarian matching determines optimal pairings, and threshold-gating controls the depth of analysis.</p>"},{"location":"Core-Concepts/#why-these-concepts-matter","title":"Why These Concepts Matter","text":"<p>Stickler's evaluation approach differs from simple equality checks by considering similarity thresholds, optimal matching strategies, and hierarchical structure. These concepts ensure that evaluation metrics reflect meaningful differences rather than arbitrary misalignments, particularly when comparing lists where element order may vary.</p>"},{"location":"Core-Concepts/Classification_Logic/","title":"Classification Logic for Evaluation Metrics","text":"<p>This document defines the classification logic used in the stickler library for evaluating predictions against ground truth.</p>"},{"location":"Core-Concepts/Classification_Logic/#core-definitions","title":"Core Definitions","text":"<p>The confusion matrix metrics classify comparisons into five categories:</p> Category Abbreviation Definition True Positive (TP) TP GT != null, EST != null, GT == EST (match above threshold) False Alarm (FA) FA GT == null, EST != null True Negative (TN) TN GT == null, EST == null False Negative (FN) FN GT != null, EST == null False Discovery (FD) FD GT != null, EST != null, GT != EST (match below threshold) <p>False alarm (FA) and false discovery (FD) are a subset of false positives (FP). </p> <p>FP = FA + FD</p> <p>Where: - GT = Ground Truth - EST = Estimate (Prediction)</p>"},{"location":"Core-Concepts/Classification_Logic/#classification-logic-by-data-type","title":"Classification Logic by Data Type","text":""},{"location":"Core-Concepts/Classification_Logic/#simple-values-strings-numbers-etc","title":"Simple Values (Strings, Numbers, etc.)","text":"Ground Truth Prediction Classification Explanation \"value\" \"value\" TP Exact match \"value\" \"similar\" FD Both non-null but don't match above threshold \"value\" null FN Missing prediction for existing ground truth null \"value\" FP Prediction exists but no ground truth (False Alarm) null null TN Correctly predicted absence \"\" (empty) null Treated as TN Empty strings are treated as null null \"\" (empty) Treated as TN Empty strings are treated as null"},{"location":"Core-Concepts/Classification_Logic/#lists","title":"Lists","text":"<p>For lists, we use the Hungarian algorithm to find optimal matching between elements:</p> <ol> <li>Empty Lists:</li> <li>GT = [], EST = [] \u2192 TN (both empty)</li> <li>GT = [], EST = [\"item\"] \u2192 FA (False Alarm for each prediction item)</li> <li> <p>GT = [\"item\"], EST = [] \u2192 FN (False Negative for each ground truth item)</p> </li> <li> <p>Element Matching:</p> </li> <li>Each element in GT is matched with at most one element in EST</li> <li>Each element in EST is matched with at most one element in GT</li> <li> <p>Matching maximizes overall similarity</p> </li> <li> <p>Classification of Matched Elements:</p> </li> <li>If similarity \u2265 threshold \u2192 TP</li> <li> <p>If similarity &lt; threshold \u2192 FD (False Discovery)</p> </li> <li> <p>Classification of Unmatched Elements:</p> </li> <li>Unmatched GT elements \u2192 FN</li> <li>Unmatched EST elements \u2192 FA (False Alarm)</li> </ol>"},{"location":"Core-Concepts/Classification_Logic/#example-1-mixed-matching","title":"Example 1: Mixed Matching","text":"<p>GT = [\"red\", \"blue\", \"green\"] EST = [\"red\", \"yellow\", \"orange\", \"blue\"]</p> <p>Matching: - \"red\" matches \"red\" \u2192 TP - \"blue\" matches \"blue\" \u2192 TP - \"green\" has no match \u2192 FN - \"yellow\" has no match in GT \u2192 FA (False Alarm) - \"orange\" has no match in GT \u2192 FA (False Alarm)</p> <p>Result: TP=2, FP=A, TN=0, FN=1, FD=0</p>"},{"location":"Core-Concepts/Classification_Logic/#example-2-similar-but-not-exact","title":"Example 2: Similar But Not Exact","text":"<p>GT = [\"apple\", \"banana\", \"cherry\"] EST = [\"aple\", \"bananna\", \"cheery\"]</p> <p>Matching (assuming threshold = 0.7): - \"apple\" matches \"aple\" with similarity 0.8 \u2192 TP - \"banana\" matches \"bananna\" with similarity 0.85 \u2192 TP - \"cherry\" matches \"cheery\" with similarity 0.83 \u2192 TP</p> <p>Result: TP=3, FA=0, TN=0, FN=0, FD=0</p>"},{"location":"Core-Concepts/Classification_Logic/#example-3-below-threshold","title":"Example 3: Below Threshold","text":"<p>GT = [\"apple\", \"banana\", \"cherry\"] EST = [\"appx\", \"bnn\", \"chry\"]</p> <p>Matching (assuming threshold = 0.7): - \"apple\" matches \"appx\" with similarity 0.5 \u2192 FD - \"banana\" matches \"bnn\" with similarity 0.6 \u2192 FD - \"cherry\" matches \"chry\" with similarity 0.65 \u2192 FD</p> <p>Result: TP=0, FA=0, TN=0, FN=0, FD=3</p>"},{"location":"Core-Concepts/Classification_Logic/#nested-objectsdictionaries","title":"Nested Objects/Dictionaries","text":"<p>For nested objects, we apply the classification logic recursively:</p> <ol> <li>Empty Objects:</li> <li>GT = {}, EST = {} \u2192 TN</li> <li>GT = {}, EST = {key: value} \u2192 FA (False Alarm)</li> <li> <p>GT = {key: value}, EST = {} \u2192 FN</p> </li> <li> <p>Field Matching:</p> </li> <li>Each field is evaluated independently</li> <li>Fields present in both GT and EST are compared for similarity</li> <li> <p>Fields present in only one are classified as FA or FN</p> </li> <li> <p>Classification of Fields:</p> </li> <li>If both have field and similarity \u2265 threshold \u2192 TP</li> <li>If both have field and similarity &lt; threshold \u2192 FD (False Discovery)</li> <li>If only GT has field \u2192 FN</li> <li>If only EST has field \u2192 FA (False Alarm)</li> </ol>"},{"location":"Core-Concepts/Classification_Logic/#example","title":"Example:","text":"<p>GT = {name: \"John\", age: 30, address: \"123 Main St\"} EST = {name: \"John\", age: 31, phone: \"555-1234\"}</p> <p>Field-by-field: - name: Both have it, exact match \u2192 TP - age: Both have it, but different \u2192 FD - address: Only in GT \u2192 FN - phone: Only in EST \u2192 FA (False Alarm)</p> <p>Result: TP=1, FA=1, TN=0, FN=1, FD=1</p>"},{"location":"Core-Concepts/Classification_Logic/#derived-metrics","title":"Derived Metrics","text":"<p>From the base confusion matrix counts, we derive the following metrics:</p> <ol> <li>Precision: TP / (TP + FP)</li> <li> <p>Measures how many of the predicted values are correct</p> </li> <li> <p>Recall: TP / (TP + FN)</p> </li> <li> <p>Measures how many of the ground truth values are correctly predicted</p> </li> <li> <p>F1 Score: 2 * (Precision * Recall) / (Precision + Recall)</p> </li> <li> <p>Harmonic mean of precision and recall</p> </li> <li> <p>Accuracy: (TP + TN) / (TP + TN + FP + FN)</p> </li> <li>Overall correctness of predictions</li> </ol>"},{"location":"Core-Concepts/Classification_Logic/#edge-cases-and-clarifications","title":"Edge Cases and Clarifications","text":""},{"location":"Core-Concepts/Classification_Logic/#1-null-vs-empty-equivalence","title":"1. Null vs. Empty Equivalence","text":"<p>Design Decision: Empty collections and null values are treated as equivalent in all comparisons.</p> <ul> <li>Empty strings (\"\"), empty lists ([]), and empty objects ({}) are treated as null values</li> <li>This means comparing null with an empty collection results in TN (True Negative)</li> <li>Examples:</li> <li>GT = <code>null</code>, EST = <code>[]</code> \u2192 TN (equivalent states representing \"no data\")</li> <li>GT = <code>[]</code>, EST = <code>null</code> \u2192 TN (equivalent states representing \"no data\")</li> <li>GT = <code>\"\"</code>, EST = <code>null</code> \u2192 TN (equivalent states representing \"no data\")</li> <li>GT = <code>{}</code>, EST = <code>null</code> \u2192 TN (equivalent states representing \"no data\")</li> </ul> <p>Rationale: Semantically, both null values and empty collections represent the absence of meaningful data. Distinguishing between these states would introduce unnecessary complexity and inconsistency in evaluation metrics. For practical evaluation purposes, \"no data\" should be treated uniformly regardless of its representation.</p>"},{"location":"Core-Concepts/Classification_Logic/#2-threshold-boundary","title":"2. Threshold Boundary","text":"<ul> <li>Values exactly at the threshold are considered matches (TP)</li> <li>For example, if threshold = 0.7 and similarity = 0.7, this is a TP</li> </ul>"},{"location":"Core-Concepts/Classification_Logic/#3-list-order","title":"3. List Order","text":"<ul> <li>List order doesn't matter for matching</li> <li>The Hungarian algorithm finds the optimal matching regardless of order</li> </ul>"},{"location":"Core-Concepts/Classification_Logic/#4-partial-matches-in-lists","title":"4. Partial Matches in Lists","text":"<ul> <li>For lists, we don't have \"partial credit\" for individual elements</li> <li>Each element is classified as TP, FA, FN, or FD independently</li> </ul>"},{"location":"Core-Concepts/Classification_Logic/#5-nested-lists","title":"5. Nested Lists","text":"<ul> <li>For lists of objects, we apply the Hungarian algorithm at the list level</li> <li>Each matched pair of objects is then evaluated recursively</li> </ul>"},{"location":"Core-Concepts/Classification_Logic/#6-missing-fields-vs-null-fields","title":"6. Missing Fields vs. Null Fields","text":"<ul> <li>A missing field and a field with null value are treated differently:</li> <li>Missing field in EST when GT has it \u2192 FN</li> <li>Null field in EST when GT has non-null \u2192 FN</li> <li>Missing field in GT when EST has it \u2192 FA (False Alarm)</li> <li>Null field in GT when EST has non-null \u2192 FA (False Alarm)</li> </ul>"},{"location":"Core-Concepts/Classification_Logic/#summary-of-key-points","title":"Summary of Key Points","text":"<ol> <li> <p>False Alarm (FA) occurs when the prediction includes something that doesn't exist in the ground truth (GT is null, EST is not null)</p> </li> <li> <p>False Discovery (FD) occurs when the prediction recognizes something that exists but gets it wrong (both GT and EST are not null, but they don't match)</p> </li> <li> <p>List matching uses the Hungarian algorithm to find optimal pairings, with unmatched items classified as FP or FN</p> </li> <li> <p>Nested structures are evaluated recursively, with each field or element classified independently</p> </li> <li> <p>Empty collections are generally treated as null values for classification purposes</p> </li> </ol>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/","title":"Threshold-Gated Recursive Evaluation for <code>List[StructuredModel]</code> Comparison","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#overview","title":"Overview","text":"<p>This document outlines the threshold-gated recursive evaluation approach for comparing <code>List[StructuredModel]</code> objects in the GenAIDP library. This approach optimizes evaluation by only performing detailed nested field analysis on well-matched object pairs, while treating poorly-matched and unmatched objects as atomic units.</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#core-principle","title":"Core Principle","text":"<p>Only recurse into nested field evaluation for object pairs that meet the similarity threshold.</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#algorithm-flow","title":"Algorithm Flow","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#1-hungarian-matching","title":"1. Hungarian Matching","text":"<p>Use Hungarian algorithm to find optimal pairings between ground truth and prediction lists based on overall object similarity.</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#2-threshold-classification","title":"2. Threshold Classification","text":"<p>For each matched pair, check if the similarity score meets the <code>StructuredModel.match_threshold</code>:</p> <ul> <li>similarity \u2265 threshold \u2192 TP (True Positive) + recurse into nested fields</li> <li>similarity &lt; threshold \u2192 FD (False Discovery) + stop recursion</li> </ul>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#3-unmatched-items","title":"3. Unmatched Items","text":"<p>Handle items that couldn't be matched: - GT extras \u2192 FN (False Negative) + stop recursion - Pred extras \u2192 FA (False Alarm) + stop recursion</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#code-example","title":"Code Example","text":"<pre><code>from stickler.structured_object_evaluator.models.structured_model import StructuredModel\nfrom stickler.structured_object_evaluator.models.comparable_field import ComparableField\nfrom stickler.comparators.levenshtein import LevenshteinComparator\nfrom stickler.comparators.exact import ExactComparator\nfrom typing import List\n\nclass Product(StructuredModel):\n    product_id: str = ComparableField(\n        comparator=ExactComparator(), \n        threshold=1.0,\n        weight=3.0\n    )\n    name: str = ComparableField(\n        comparator=LevenshteinComparator(),\n        threshold=0.7,\n        weight=2.0\n    )\n    price: float = ComparableField(\n        threshold=0.9,\n        weight=1.0\n    )\n\n    # Key: This threshold gates recursive evaluation\n    match_threshold = 0.8\n\nclass Order(StructuredModel):\n    order_id: str = ComparableField(\n        comparator=ExactComparator(),\n        threshold=1.0,\n        weight=2.0\n    )\n    products: List[Product] = ComparableField(\n        aggregate=True,\n        threshold=0.6,\n        weight=3.0\n    )\n\n# Example data\ngt_order = Order(\n    order_id=\"ORD-12345\",\n    products=[\n        Product(product_id=\"PROD-001\", name=\"Laptop\", price=999.99),\n        Product(product_id=\"PROD-002\", name=\"Mouse\", price=29.99),\n        Product(product_id=\"PROD-003\", name=\"Cable\", price=14.99)\n    ]\n)\n\npred_order = Order(\n    order_id=\"ORD-12345\",\n    products=[\n        Product(product_id=\"PROD-001\", name=\"Laptop Computer\", price=999.99),  # Good match (\u22650.8)\n        Product(product_id=\"PROD-002\", name=\"Different Product\", price=99.99),  # Poor match (&lt;0.8)\n        Product(product_id=\"PROD-004\", name=\"New Product\", price=19.99)  # Unmatched\n        # PROD-003 is missing \u2192 FN\n    ]\n)\n</code></pre>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#expected-behavior","title":"Expected Behavior","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#scenario-1-good-match-similarity-08","title":"Scenario 1: Good Match (similarity \u2265 0.8)","text":"<p>GT: <code>Product(product_id=\"PROD-001\", name=\"Laptop\", price=999.99)</code> Pred: <code>Product(product_id=\"PROD-001\", name=\"Laptop Computer\", price=999.99)</code></p> <p>Result: - Classification: TP (True Positive) - Nested Field Analysis: \u2705 Performed   - <code>product_id</code>: TP (exact match)   - <code>name</code>: TP (similarity ~0.9)   - <code>price</code>: TP (exact match)</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#scenario-2-poor-match-similarity-08","title":"Scenario 2: Poor Match (similarity &lt; 0.8)","text":"<p>GT: <code>Product(product_id=\"PROD-002\", name=\"Mouse\", price=29.99)</code> Pred: <code>Product(product_id=\"PROD-002\", name=\"Different Product\", price=99.99)</code></p> <p>Result: - Classification: FD (False Discovery) - Nested Field Analysis: \u274c Not performed - Rationale: Objects are too different to warrant detailed comparison</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#scenario-3-unmatched-items","title":"Scenario 3: Unmatched Items","text":"<p>GT: <code>Product(product_id=\"PROD-003\", name=\"Cable\", price=14.99)</code> \u2192 FN Pred: <code>Product(product_id=\"PROD-004\", name=\"New Product\", price=19.99)</code> \u2192 FA</p> <p>Result: - Classification: FN and FA respectively - Nested Field Analysis: \u274c Not performed - Rationale: No matching counterpart</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#edge-cases-and-handling","title":"Edge Cases and Handling","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#1-empty-lists","title":"1. Empty Lists","text":"<p>Empty lists are handled as follows: - GT=[], Pred=[] \u2192 TN - GT=[], Pred=[items] \u2192 All items are FA - GT=[items], Pred=[] \u2192 All items are FN</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#2-threshold-boundary-conditions","title":"2. Threshold Boundary Conditions","text":"<p>Decision: Use <code>similarity \u2265 threshold</code> \u2192 TP + recurse - Values exactly at the threshold are considered matches and trigger recursion</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#3-nested-list-scenarios","title":"3. Nested List Scenarios","text":"<p>Decision: Full recursion through <code>i.compare_with(j)</code> is supported - When a StructuredModel contains another <code>List[StructuredModel]</code>, the same threshold-gating applies recursively at all levels - Each nested list uses its parent StructuredModel's <code>match_threshold</code> for gating decisions</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#4-multiple-match-thresholds","title":"4. Multiple Match Thresholds","text":"<p>Decision: Different StructuredModel types can have any user-defined <code>match_threshold</code> attribute <pre><code>class Product(StructuredModel):\n    match_threshold = 0.8  # Strict matching for products\n\nclass Address(StructuredModel):\n    match_threshold = 0.6  # More lenient for addresses\n</code></pre></p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#5-aggregate-field-behavior","title":"5. Aggregate Field Behavior","text":"<p>Decision: Leave aggregate field behavior unchanged for now - Aggregate fields continue to sum metrics from all child fields as before - This interaction will be addressed in future iterations</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#6-performance-considerations","title":"6. Performance Considerations","text":"<p>Decision: Performance is not a primary concern at this time - The main goal is cleaner, more accurate metrics rather than performance optimization - Hungarian matching still requires calculating all similarity scores regardless</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#comparison-to-current-implementation","title":"Comparison to Current Implementation","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#current-behavior","title":"Current Behavior","text":"<ul> <li>Hungarian matching finds optimal pairs</li> <li>All matched pairs get recursive nested field analysis</li> <li>Results in detailed metrics for obviously poor matches</li> </ul>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#proposed-behavior","title":"Proposed Behavior","text":"<ul> <li>Hungarian matching finds optimal pairs</li> <li>Only good matches get recursive nested field analysis</li> <li>Poor matches and unmatched items are atomic</li> </ul>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#benefits","title":"Benefits","text":"<ol> <li>Cleaner Metrics: Avoids misleading nested field metrics from forced comparisons</li> <li>Conceptual Clarity: Matches human intuition about when detailed comparison is useful</li> <li>Potential Performance: Fewer recursive operations for poor matches</li> </ol>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#risks","title":"Risks","text":"<ol> <li>Information Loss: May lose insight into why objects didn't match well</li> <li>Threshold Sensitivity: Results highly dependent on threshold selection</li> <li>Debugging Difficulty: Harder to understand why objects were classified as FD</li> </ol>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#result-structure-enhancement","title":"Result Structure Enhancement","text":"<p>The result structure for <code>List[StructuredModel]</code> fields will be enhanced to include detailed non-match information:</p> <pre><code>{\n  \"products\": {\n    \"overall\": { \n      \"tp\": 1, \"fd\": 1, \"fn\": 1, \"fa\": 1,\n      \"derived\": { \"cm_precision\": 0.5, \"cm_recall\": 0.5, \"cm_f1\": 0.5 }\n    },\n    \"fields\": { \n      # Only recursive analysis for threshold-passing matches (\u2265 0.8)\n      \"product_id\": { \"tp\": 1, \"derived\": {...} },\n      \"name\": { \"tp\": 1, \"derived\": {...} },\n      \"price\": { \"tp\": 1, \"derived\": {...} }\n    },\n    \"non_matches\": [\n      { \n        \"type\": \"FD\", \n        \"gt_object\": \"Product(product_id='PROD-002', name='Mouse', price=29.99)\",\n        \"pred_object\": \"Product(product_id='PROD-002', name='Different Product', price=99.99)\",\n        \"similarity\": 0.3\n      },\n      { \n        \"type\": \"FN\", \n        \"gt_object\": \"Product(product_id='PROD-003', name='Cable', price=14.99)\",\n        \"pred_object\": null\n      },\n      { \n        \"type\": \"FA\", \n        \"gt_object\": null, \n        \"pred_object\": \"Product(product_id='PROD-004', name='New Product', price=19.99)\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#non-matches-structure","title":"Non-Matches Structure","text":"<ul> <li>FD (False Discovery): Both objects exist but similarity &lt; threshold</li> <li>FN (False Negative): GT object with no matching prediction</li> <li>FA (False Alarm): Prediction object with no matching GT</li> <li>similarity: Only included for FD cases where objects were compared</li> </ul>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#implementation-approach","title":"Implementation Approach","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#api-changes","title":"API Changes","text":"<p>Decision: Modify existing behavior globally (no new parameters) - This approach fixes an inconsistency and provides conceptually cleaner behavior - No backward compatibility concerns as this is a logic improvement</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#threshold-source","title":"Threshold Source","text":"<p>Decision: Use each StructuredModel's <code>match_threshold</code> attribute - Each model type can define its own threshold for recursive evaluation - Falls back to default <code>match_threshold = 0.7</code> if not specified</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#recursive-consistency","title":"Recursive Consistency","text":"<p>Decision: Full recursion through <code>i.compare_with(j)</code> maintains consistency - When nested StructuredModel fields are compared, they use the same threshold-gating logic - Ensures consistent behavior at all levels of nesting</p>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#implementation-plan","title":"Implementation Plan","text":""},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#phase-1-core-logic-update","title":"Phase 1: Core Logic Update","text":"<ol> <li>Modify <code>_calculate_nested_field_metrics()</code> to check similarity scores before recursion</li> <li>Only recurse for matched pairs where <code>similarity \u2265 StructuredModel.match_threshold</code></li> <li>Count below-threshold matches as FD at the object level</li> <li>Skip nested field metrics for FD, FN, and FA objects</li> </ol>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#phase-2-result-structure-enhancement","title":"Phase 2: Result Structure Enhancement","text":"<ol> <li>Add <code>non_matches</code> key to List[StructuredModel] field results</li> <li>Track specific instances that weren't matched with detailed information</li> <li>Maintain existing structure for recursive analysis of good matches</li> </ol>"},{"location":"Core-Concepts/Threshold_Gated_Recursive_Evaluation/#phase-3-testing-and-validation","title":"Phase 3: Testing and Validation","text":"<ol> <li>Update existing tests to expect new behavior</li> <li>Add comprehensive test cases for edge cases</li> <li>Validate nested list scenarios work correctly</li> <li>Test with different threshold values across model types</li> </ol>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/","title":"Hungarian Matching for <code>List[StructuredModel]</code> A Complete Guide","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#overview","title":"Overview","text":"<p>This document provides a comprehensive guide to Hungarian matching for <code>List[StructuredModel]</code> fields in the stickler library, including concrete examples, expected behavior, and validation of the current implementation.</p>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#core-algorithm-threshold-gated-recursive-evaluation","title":"Core Algorithm: Threshold-Gated Recursive Evaluation","text":"<p>The Hungarian matching algorithm for structured lists follows this principle:</p> <p>\ud83d\udd11 Only recurse into nested field evaluation for object pairs that meet the similarity threshold.</p>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Hungarian Matching: Find optimal pairings between GT and Pred lists based on overall object similarity</li> <li>Threshold Classification: For each matched pair, check if similarity \u2265 <code>StructuredModel.match_threshold</code></li> <li>similarity \u2265 threshold \u2192 TP + recurse into nested fields </li> <li>similarity &lt; threshold \u2192 FD + stop recursion (atomic)</li> <li>Unmatched Items: Handle items that couldn't be matched</li> <li>GT extras \u2192 FN + stop recursion (atomic)</li> <li>Pred extras \u2192 FA + stop recursion (atomic)</li> </ol>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#concrete-example-transaction-matching","title":"Concrete Example: Transaction Matching","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#model-definitions","title":"Model Definitions","text":"<pre><code>class Transaction(StructuredModel):\n    transaction_id: str = ComparableField(\n        comparator=ExactComparator(), \n        threshold=1.0,\n        weight=3.0\n    )\n\n    description: str = ComparableField(\n        comparator=LevenshteinComparator(),\n        threshold=0.7,\n        weight=2.0\n    )\n\n    amount: float = ComparableField(\n        threshold=0.9,\n        weight=1.0\n    )\n\n    # \u26a0\ufe0f CRITICAL: This threshold controls Hungarian matching recursion\n    match_threshold = 0.8\n\nclass Account(StructuredModel):\n    account_id: str = ComparableField(\n        comparator=ExactComparator(),\n        threshold=1.0,\n        weight=2.0\n    )\n\n    # Notice: NO threshold on this field - should use Transaction.match_threshold\n    transactions: List[Transaction] = ComparableField(weight=3.0)\n</code></pre>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#test-data","title":"Test Data","text":"<p>Ground Truth: <pre><code>gt_account = Account(\n    account_id=\"ACC-12345\",\n    transactions=[\n        Transaction(transaction_id=\"TXN-001\", description=\"Coffee shop payment\", amount=4.95),\n        Transaction(transaction_id=\"TXN-002\", description=\"Grocery store\", amount=127.43),\n        Transaction(transaction_id=\"TXN-003\", description=\"Gas station\", amount=45.67)\n    ]\n)\n</code></pre></p> <p>Prediction: <pre><code>pred_account = Account(\n    account_id=\"ACC-12345\", \n    transactions=[\n        Transaction(transaction_id=\"TXN-001\", description=\"Coffee shop\", amount=4.95),      # Good match (TP)\n        Transaction(transaction_id=\"TXN-002\", description=\"Online purchase\", amount=89.99), # Poor match (FD)  \n        Transaction(transaction_id=\"TXN-004\", description=\"Restaurant\", amount=23.45)       # Poor match (FD)\n    ]\n)\n</code></pre></p>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#step-by-step-hungarian-matching-analysis","title":"Step-by-Step Hungarian Matching Analysis","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#step-1-pairwise-similarity-calculation","title":"Step 1: Pairwise Similarity Calculation","text":"GT Index Pred Index GT Transaction Pred Transaction Similarity Above Threshold? 0 0 TXN-001: Coffee shop payment TXN-001: Coffee shop 0.860 \u2705 Yes (\u22650.8) 0 1 TXN-001: Coffee shop payment TXN-002: Online purchase 0.137 \u274c No 0 2 TXN-001: Coffee shop payment TXN-004: Restaurant 0.154 \u274c No 1 0 TXN-002: Grocery store TXN-001: Coffee shop 0.130 \u274c No 1 1 TXN-002: Grocery store TXN-002: Online purchase 0.572 \u274c No (&lt;0.8) 1 2 TXN-002: Grocery store TXN-004: Restaurant 0.135 \u274c No 2 0 TXN-003: Gas station TXN-001: Coffee shop 0.097 \u274c No 2 1 TXN-003: Gas station TXN-002: Online purchase 0.056 \u274c No 2 2 TXN-003: Gas station TXN-004: Restaurant 0.124 \u274c No"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#step-2-hungarian-algorithm-assignment","title":"Step 2: Hungarian Algorithm Assignment","text":"<p>The Hungarian algorithm finds the optimal assignment that maximizes total similarity:</p> <ul> <li>GT[0] \u2192 Pred[0]: 0.860 \u2705 Good Match</li> <li>GT[1] \u2192 Pred[1]: 0.572 \u274c Poor Match </li> <li>GT[2] \u2192 Pred[2]: 0.124 \u274c Poor Match</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#step-3-threshold-gated-classification","title":"Step 3: Threshold-Gated Classification","text":"Pair Similarity Classification Nested Field Analysis GT[0] \u2192 Pred[0] 0.860 \u2265 0.8 TP \u2705 Generate nested metrics GT[1] \u2192 Pred[1] 0.572 &lt; 0.8 FD \u274c Skip (atomic treatment) GT[2] \u2192 Pred[2] 0.124 &lt; 0.8 FD \u274c Skip (atomic treatment)"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#expected-result-structure","title":"Expected Result Structure","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#object-level-metrics-list-level","title":"Object-Level Metrics (List Level)","text":"<pre><code>\"transactions\": {\n    \"overall\": {\n        \"tp\": 1,  # GT[0] \u2192 Pred[0] good match\n        \"fd\": 2,  # GT[1] \u2192 Pred[1] AND GT[2] \u2192 Pred[2] poor matches  \n        \"fa\": 0,  # No unmatched preds (equal length lists)\n        \"fn\": 0,  # No unmatched GTs (equal length lists)\n        \"fp\": 2   # fd + fa = 2 + 0\n    },\n    \"fields\": {\n        # Only nested metrics for the TP pair (GT[0] \u2192 Pred[0])\n        \"transaction_id\": {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fn\": 0},\n        \"description\": {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fn\": 0}, \n        \"amount\": {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fn\": 0}\n    },\n    \"non_matches\": [\n        {\n            \"type\": \"FD\",\n            \"gt_object\": \"Transaction(TXN-002, Grocery store, $127.43)\",\n            \"pred_object\": \"Transaction(TXN-002, Online purchase, $89.99)\",\n            \"similarity\": 0.572\n        },\n        {\n            \"type\": \"FD\",\n            \"gt_object\": \"Transaction(TXN-003, Gas station, $45.67)\",\n            \"pred_object\": \"Transaction(TXN-004, Restaurant, $23.45)\",\n            \"similarity\": 0.124\n        }\n    ]\n}\n</code></pre>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#current-implementation-status","title":"\u2705 Current Implementation Status","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#implementation-validation-results","title":"Implementation Validation Results","text":"<p>Actual Test Results (Equal Length Lists): <pre><code>TEST CASE 1: Equal Length Lists (3x3)\nActual Metrics: TP=1, FD=2, FN=0, FA=0\nExpected Metrics: TP=1, FD=2, FN=0, FA=0\n\u2705 PASS: Metrics match corrected expected behavior\n\nTEST CASE 2: GT Longer (4x2)\nActual Metrics: TP=1, FD=1, FN=2, FA=0\nExpected Metrics: TP=1, FD=1, FN=2, FA=0\n\u2705 PASS\n\nTEST CASE 3: Pred Longer (2x4)\nActual Metrics: TP=1, FD=1, FN=0, FA=2\nExpected Metrics: TP=1, FD=1, FN=0, FA=2\n\u2705 PASS\n\nTEST CASE 4: All Above Threshold (3x3)\nActual Metrics: TP=3, FD=0, FN=0, FA=0\nExpected Metrics: TP=3, FD=0, FN=0, FA=0\n\u2705 PASS\n</code></pre></p>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#key-findings","title":"Key Findings","text":"<ol> <li> <p>\u2705 Equal Length Behavior: When GT and Pred lists have equal length, Hungarian algorithm pairs everyone up, resulting in only TP/FD classifications (no FN/FA)</p> </li> <li> <p>\u2705 Below-Threshold Treatment: Both similarity scores of 0.572 and 0.124 are correctly treated the same way as FD since both are below the 0.8 threshold</p> </li> <li> <p>\u2705 Length Mismatch Behavior: Only when lists have uneven lengths do we see FN (unmatched GT items) or FA (unmatched Pred items)</p> </li> <li> <p>\u2705 Threshold Gating: The implementation correctly uses the <code>match_threshold = 0.8</code> to determine TP vs FD classification</p> </li> </ol>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#conclusion","title":"Conclusion","text":"<p>The implementation is already correct and matches the expected behavior described in this document. The key insight was understanding that:</p> <ul> <li>Equal length lists \u2192 Only TP/FD possible (Hungarian pairs everyone)</li> <li>Uneven length lists \u2192 FN/FA occur for unpaired items</li> <li>Below threshold \u2192 All treated as FD regardless of specific similarity value</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#implementation-plan","title":"Implementation Plan","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#phase-1-create-extraction-baseline-tests","title":"Phase 1: Create Extraction Baseline Tests \u2705","text":"<ul> <li>Document current behavior with concrete examples</li> <li>Create test cases that capture existing Hungarian matching patterns</li> <li>Establish bit-for-bit compatibility baseline</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#phase-2-extract-hungarian-logic-to-dedicated-class","title":"Phase 2: Extract Hungarian Logic to Dedicated Class","text":"<ul> <li>Create <code>StructuredListComparator</code> class</li> <li>Move ~500 lines from <code>StructuredModel</code> to dedicated class:</li> <li><code>_compare_struct_list_with_scores()</code></li> <li><code>_calculate_nested_field_metrics()</code> </li> <li><code>_calculate_object_level_metrics()</code></li> <li><code>_calculate_struct_list_similarity()</code></li> <li><code>_handle_struct_list_empty_cases()</code></li> <li>Update <code>StructuredModel._dispatch_field_comparison()</code> to delegate</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#phase-3-fix-threshold-logic","title":"Phase 3: Fix Threshold Logic","text":"<ul> <li>Correct threshold source in extracted class</li> <li>Implement proper threshold-gated recursion</li> <li>Update object-level vs field-level metric separation</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#phase-4-integration-testing","title":"Phase 4: Integration Testing","text":"<ul> <li>Verify Phase 1 baseline tests still pass</li> <li>Test with different <code>match_threshold</code> values</li> <li>Validate nested list scenarios work correctly</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#phase-5-cleanup-and-documentation","title":"Phase 5: Cleanup and Documentation","text":"<ul> <li>Remove old code from <code>StructuredModel</code></li> <li>Update documentation with corrected examples</li> <li>Add integration tests for edge cases</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#key-architectural-principles","title":"Key Architectural Principles","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#1-threshold-source-hierarchy","title":"1. Threshold Source Hierarchy","text":"<pre><code>List[StructuredModel] Threshold Resolution:\n1. Use StructuredModel.match_threshold (class attribute)\n2. Fall back to default 0.7 if not specified  \n3. NEVER use ComparableField.threshold from parent list field\n</code></pre>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#2-metric-separation","title":"2. Metric Separation","text":"<pre><code>Object-Level Metrics: Count objects (TP, FD, FA, FN)\nField-Level Metrics: Count field comparisons within good matches only\n</code></pre>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#3-recursion-gating","title":"3. Recursion Gating","text":"<pre><code>IF object_similarity &gt;= match_threshold:\n    classification = TP\n    RECURSE into nested field analysis\nELSE:\n    classification = FD  \n    STOP recursion (treat as atomic)\n</code></pre>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#testing-strategy","title":"Testing Strategy","text":""},{"location":"Core-Concepts/hungarian_matching_list_of_model/#baseline-compatibility-tests","title":"Baseline Compatibility Tests","text":"<ul> <li>Capture exact current behavior before refactoring</li> <li>Bit-for-bit comparison of confusion matrix output</li> <li>Edge cases: empty lists, single items, all poor matches</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#threshold-boundary-tests","title":"Threshold Boundary Tests","text":"<ul> <li>Objects exactly at threshold (0.8000...)</li> <li>Objects slightly above/below threshold</li> <li>Different model classes with different thresholds</li> </ul>"},{"location":"Core-Concepts/hungarian_matching_list_of_model/#nested-list-scenarios","title":"Nested List Scenarios","text":"<ul> <li><code>List[List[StructuredModel]]</code> recursive structures</li> <li>Mixed threshold values across nesting levels</li> <li>Performance with large lists</li> </ul> <p>This comprehensive documentation serves as both a specification for the correct behavior and a baseline for testing the upcoming refactor.</p>"},{"location":"Getting-Started/","title":"Stickler: Structured Object Evaluation for GenAI","text":"<p>When in the course of human events, it becomes necessary to evaluate structured outputs from generative AI systems, we must acknowledge that traditional evaluation treats all fields equally. But not all fields are created equal.</p> <p>Stickler is a Python library for structured object comparison and evaluation that lets you focus on the fields your customer actually cares about, to answer the question: \"Is it doing a good job?\" </p> <p>Stickler uses specialized comparators for different data types: exact matching for critical identifiers, numeric tolerance for currency amounts, semantic similarity for text fields, and fuzzy matching for names and addresses. You can build custom comparators for domain-specific logic. The Hungarian algorithm ensures optimal list matching regardless of order, while the recursive evaluation engine handles unlimited nesting depth. Business-weighted scoring reflects actual operational impact, not just technical accuracy.</p> <p>Consider an invoice extraction agent that perfectly captures shipment numbers\u2014which must be exact or packages get routed to the wrong warehouse\u2014but sometimes garbles driver notes like \"delivered to front door\" vs \"left at entrance.\" Those note variations don't affect logistics operations at all. Traditional evaluation treats both error types identically and reports your agent as \"95% accurate\" without telling you if that 5% error rate matters. Stickler tells you exactly where the errors are and whether they're actually problems.</p> <p>Whether you're extracting data from documents, performing ETL transformations, evaluating ML model outputs, or simply trying to diff complex JSON structures, Stickler transforms evaluation from a technical afterthought into a business-aligned decision tool.</p>"},{"location":"Getting-Started/#installation","title":"Installation","text":"<pre><code>pip install stickler-eval\n</code></pre>"},{"location":"Getting-Started/#get-started-in-30-seconds","title":"Get Started in 30 Seconds","text":"<pre><code># pip install stickler-eval\nfrom typing import List\nfrom stickler import StructuredModel, ComparableField\nfrom stickler.comparators import ExactComparator, NumericComparator, LevenshteinComparator\n\n# Define your models\nclass LineItem(StructuredModel):\n    product: str = ComparableField(comparator=LevenshteinComparator(), weight=1.0)\n    quantity: int = ComparableField(weight=0.8)\n    price: float = ComparableField(comparator=NumericComparator(tolerance=0.01), weight=1.2)\n\nclass Invoice(StructuredModel):\n    shipment_id: str = ComparableField(comparator=ExactComparator(), weight=3.0)  # Critical\n    amount: float = ComparableField(comparator=NumericComparator(tolerance=0.01), weight=2.0)\n    line_items: List[LineItem] = ComparableField(weight=2.0)  # Hungarian matching!\n\n# JSON from your systems (agent output, ground truth, etc.)\nground_truth_json = {\n    \"shipment_id\": \"SHP-2024-001\",\n    \"amount\": 1247.50,\n    \"line_items\": [\n        {\"product\": \"Wireless Mouse\", \"quantity\": 2, \"price\": 29.99},\n        {\"product\": \"USB Cable\", \"quantity\": 5, \"price\": 12.99}\n    ]\n}\n\nprediction_json = {\n    \"shipment_id\": \"SHP-2024-001\",  # Perfect match\n    \"amount\": 1247.48,  # Within tolerance\n    \"line_items\": [\n        {\"product\": \"USB Cord\", \"quantity\": 5, \"price\": 12.99},  # Name variation\n        {\"product\": \"Wireless Mouse\", \"quantity\": 2, \"price\": 29.99}  # Reordered\n    ]\n}\n\n# Construct from JSON and compare\nground_truth = Invoice(**ground_truth_json)\nprediction = Invoice(**prediction_json)\nresult = ground_truth.compare_with(prediction)\n\nprint(f\"Overall Score: {result['overall_score']:.3f}\")  # 0.693\nprint(f\"Shipment ID: {result['field_scores']['shipment_id']:.3f}\")  # 1.000 - exact match\nprint(f\"Line Items: {result['field_scores']['line_items']:.3f}\")  # 0.926 - Hungarian optimal matching\n</code></pre>"},{"location":"Getting-Started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>conda (recommended)</li> </ul>"},{"location":"Getting-Started/#quick-install","title":"Quick Install","text":"<pre><code># Create conda environment\nconda create -n stickler python=3.12 -y\nconda activate stickler\n\n# Install the library\npip install -e .\n</code></pre>"},{"location":"Getting-Started/#development-install","title":"Development Install","text":"<pre><code># Install with testing dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"Getting-Started/#quick-test","title":"Quick Test","text":"<p>Run the example to verify installation: <pre><code>python examples/scripts/quick_start.py\n</code></pre></p> <p>Run tests: <pre><code>pytest tests/\n</code></pre></p>"},{"location":"Getting-Started/#basic-usage","title":"Basic Usage","text":""},{"location":"Getting-Started/#static-model-definition","title":"Static Model Definition","text":"<pre><code>from stickler import StructuredModel, ComparableField, StructuredModelEvaluator\nfrom stickler.comparators.levenshtein import LevenshteinComparator\n\n# Define your data structure\nclass Invoice(StructuredModel):\n    invoice_number: str = ComparableField(\n        comparator=LevenshteinComparator(),\n        threshold=0.9\n    )\n    total: float = ComparableField(threshold=0.95)\n\n# Compare objects\nevaluator = StructuredModelEvaluator()\nresult = evaluator.evaluate(ground_truth, prediction)\n\nprint(f\"Overall Score: {result['overall']['anls_score']:.3f}\")\n</code></pre>"},{"location":"Getting-Started/#dynamic-model-creation-new","title":"Dynamic Model Creation (New!)","text":"<p>Create models from JSON configuration for maximum flexibility:</p> <pre><code>from stickler.structured_object_evaluator.models.structured_model import StructuredModel\n\n# Define model configuration\nconfig = {\n    \"model_name\": \"Product\",\n    \"match_threshold\": 0.8,\n    \"fields\": {\n        \"name\": {\n            \"type\": \"str\",\n            \"comparator\": \"LevenshteinComparator\",\n            \"threshold\": 0.8,\n            \"weight\": 2.0\n        },\n        \"price\": {\n            \"type\": \"float\",\n            \"comparator\": \"NumericComparator\",\n            \"default\": 0.0\n        }\n    }\n}\n\n# Create dynamic model class\nProduct = StructuredModel.model_from_json(config)\n\n# Use like any Pydantic model\nproduct1 = Product(name=\"Widget\", price=29.99)\nproduct2 = Product(name=\"Gadget\", price=29.99)\n\n# Full comparison capabilities\nresult = product1.compare_with(product2)\nprint(f\"Similarity: {result['overall_score']:.2f}\")\n</code></pre>"},{"location":"Getting-Started/#complete-json-to-evaluation-workflow-new","title":"Complete JSON-to-Evaluation Workflow (New!)","text":"<p>For maximum flexibility, load both configuration AND data from JSON:</p> <pre><code># Load model config from JSON\nwith open('model_config.json') as f:\n    config = json.load(f)\n\n# Load test data from JSON  \nwith open('test_data.json') as f:\n    data = json.load(f)\n\n# Create model and instances from JSON\nModel = StructuredModel.model_from_json(config)\nground_truth = Model(**data['ground_truth'])\nprediction = Model(**data['prediction'])\n\n# Evaluate - no Python object construction needed!\nresult = ground_truth.compare_with(prediction)\n</code></pre> <p>Benefits of JSON-Driven Approach: - Zero Python object construction required - Configuration-driven model creation - A/B testing different field configurations - Runtime model generation from external schemas - Production-ready JSON-based evaluation pipeline - Full Pydantic compatibility with comparison capabilities</p> <p>See <code>examples/scripts/json_to_evaluation_demo.py</code> for a complete working example and <code>docs/StructuredModel_Dynamic_Creation.md</code> for comprehensive documentation.</p>"},{"location":"Getting-Started/#examples","title":"Examples","text":"<p>Check out the <code>examples/</code> directory for more detailed usage examples and notebooks.</p>"},{"location":"Guides/","title":"Guides","text":"<p>This section provides practical guides for working with Stickler's StructuredModel comparison system. These guides cover common use cases and advanced features with working examples.</p>"},{"location":"Guides/#whats-covered","title":"What's Covered","text":"<p>StructuredModel compare_with Method walks through how the core comparison method works, from basic usage to understanding the internal flow. It explains the recursive traversal process, field-by-field analysis, and how results are assembled.</p> <p>Universal Aggregate Field Feature describes the automatic aggregation of confusion matrix metrics at every level of the comparison tree. This feature provides field-level granularity without requiring manual configuration.</p> <p>StructuredModel Advanced Functionality provides a technical deep-dive into the internal comparison engine. It covers the recursive logic, field dispatch system, Hungarian matching integration, and score aggregation mechanisms for developers who need to extend or debug the system.</p> <p>StructuredModel Dynamic Creation explains how to create StructuredModel classes from JSON configuration. This enables configuration-driven model definitions with full comparison capabilities, including nested models and custom comparators.</p>"},{"location":"Guides/#how-to-use-these-guides","title":"How to Use These Guides","text":"<p>Start with the compare_with guide to understand basic usage and the comparison flow. The Universal Aggregate Field guide explains how to access aggregated metrics in your results. For advanced scenarios or customization, consult the Advanced Functionality guide. If you need to generate models programmatically, the Dynamic Creation guide shows how to define models using JSON configuration.</p> <p>Each guide includes code examples and practical scenarios to help you apply the concepts to your own use cases.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/","title":"StructuredModel Advanced Functionality: Technical Deep Dive","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Core Recursive Engine</li> <li>Field Dispatch System</li> <li>Specialized Comparison Handlers</li> <li>Hungarian Matching Integration</li> <li>Score Aggregation and Percolation</li> <li>Threshold-Gated Recursion</li> <li>Performance Optimizations</li> <li>Debugging and Troubleshooting</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#overview","title":"Overview","text":"<p>This document provides a technical deep-dive into the core recursive logic of StructuredModel's comparison system. It's intended for developers who need to understand, modify, debug, or extend the complex internal functions that power the comparison engine.</p> <p>The system is built around several \"monster functions\" that handle the complexity of comparing nested, heterogeneous data structures while maintaining both performance and accuracy.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#core-recursive-engine","title":"Core Recursive Engine","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#compare_recursiveself-other-structuredmodel-dict","title":"<code>compare_recursive(self, other: 'StructuredModel') -&gt; dict</code>","text":"<p>This is the heart of the comparison system - a single-traversal engine that gathers both similarity scores and confusion matrix data in one pass.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#function-signature-and-purpose","title":"Function Signature and Purpose","text":"<pre><code>def compare_recursive(self, other: 'StructuredModel') -&gt; dict:\n    \"\"\"The ONE clean recursive function that handles everything.\n\n    Enhanced to capture BOTH confusion matrix metrics AND similarity scores\n    in a single traversal to eliminate double traversal inefficiency.\n    \"\"\"\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#internal-structure","title":"Internal Structure","text":"<pre><code>result = {\n    \"overall\": {\n        \"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0,\n        \"similarity_score\": 0.0,\n        \"all_fields_matched\": False\n    },\n    \"fields\": {},\n    \"non_matches\": []\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#key-innovations","title":"Key Innovations","text":"<ol> <li> <p>Single Traversal Optimization: Instead of multiple passes through the object structure, all data gathering happens in one recursive descent.</p> </li> <li> <p>Dual-Purpose Processing: Each field comparison returns both:</p> </li> <li>Confusion matrix metrics (TP, FP, FN, etc.)</li> <li> <p>Similarity scores for aggregation</p> </li> <li> <p>Score Percolation Variables:    <pre><code>total_score = 0.0\ntotal_weight = 0.0  \nthreshold_matched_fields = set()\n</code></pre></p> </li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#processing-loop","title":"Processing Loop","text":"<pre><code>for field_name in self.__class__.model_fields:\n    if field_name == 'extra_fields':\n        continue\n\n    gt_val = getattr(self, field_name)\n    pred_val = getattr(other, field_name, None)\n\n    # Enhanced dispatch returns both metrics AND scores\n    field_result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n\n    result[\"fields\"][field_name] = field_result\n\n    # Simple aggregation to overall metrics\n    self._aggregate_to_overall(field_result, result[\"overall\"])\n\n    # Score percolation - aggregate scores upward\n    if \"similarity_score\" in field_result and \"weight\" in field_result:\n        weight = field_result[\"weight\"]\n        threshold_applied_score = field_result[\"threshold_applied_score\"]\n        total_score += threshold_applied_score * weight\n        total_weight += weight\n\n        # Track threshold-matched fields\n        info = self._get_comparison_info(field_name)\n        if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n            threshold_matched_fields.add(field_name)\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#final-score-calculation","title":"Final Score Calculation","text":"<pre><code># Calculate overall similarity score from percolated scores\nif total_weight &gt; 0:\n    result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n\n# Determine all_fields_matched\nmodel_fields_for_comparison = set(self.__class__.model_fields.keys()) - {'extra_fields'}\nresult[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(model_fields_for_comparison)\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#field-dispatch-system","title":"Field Dispatch System","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#_dispatch_field_comparisonself-field_name-str-gt_val-any-pred_val-any-dict","title":"<code>_dispatch_field_comparison(self, field_name: str, gt_val: Any, pred_val: Any) -&gt; dict</code>","text":"<p>This function routes each field to the appropriate comparison handler based on its type and content. It uses Python's <code>match</code> statements for clean, efficient dispatch.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#key-responsibilities","title":"Key Responsibilities","text":"<ol> <li>Type Detection: Determines field type (primitive, list, nested object)</li> <li>Null Handling: Manages various null/empty states</li> <li>Configuration Retrieval: Gets field-specific comparison settings</li> <li>Handler Routing: Dispatches to appropriate specialized handler</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#match-based-type-detection","title":"Match-Based Type Detection","text":"<pre><code># Get field configuration for scoring\ninfo = self._get_comparison_info(field_name)\nweight = info.weight\nthreshold = info.threshold\n\n# Check if this field is ANY list type\nis_list_field = self._is_list_field(field_name)\n\n# Get null states and hierarchical needs\ngt_is_null = self._is_truly_null(gt_val)\npred_is_null = self._is_truly_null(pred_val)\ngt_needs_hierarchy = self._should_use_hierarchical_structure(gt_val, field_name)\npred_needs_hierarchy = self._should_use_hierarchical_structure(pred_val, field_name)\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#list-field-dispatch-with-match-statements","title":"List Field Dispatch with Match Statements","text":"<pre><code>if is_list_field:\n    list_result = self._handle_list_field_dispatch(gt_val, pred_val, weight)\n    if list_result is not None:\n        return list_result\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#primitive-null-handling","title":"Primitive Null Handling","text":"<pre><code>if not (gt_needs_hierarchy or pred_needs_hierarchy):\n    gt_effectively_null_prim = self._is_effectively_null_for_primitives(gt_val)\n    pred_effectively_null_prim = self._is_effectively_null_for_primitives(pred_val)\n\n    match (gt_effectively_null_prim, pred_effectively_null_prim):\n        case (True, True):\n            return self._create_true_negative_result(weight)\n        case (True, False):\n            return self._create_false_alarm_result(weight)\n        case (False, True):\n            return self._create_false_negative_result(weight)\n        case _:\n            # Both non-null, continue to type-based dispatch\n            pass\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#type-based-handler-selection","title":"Type-Based Handler Selection","text":"<pre><code># Type-based dispatch\nif isinstance(gt_val, (str, int, float)) and isinstance(pred_val, (str, int, float)):\n    return self._compare_primitive_with_scores(gt_val, pred_val, field_name)\nelif isinstance(gt_val, list) and isinstance(pred_val, list):\n    # Check if this should be structured list\n    if gt_val and isinstance(gt_val[0], StructuredModel):\n        return self._compare_struct_list_with_scores(gt_val, pred_val, field_name)\n    else:\n        return self._compare_primitive_list_with_scores(gt_val, pred_val, field_name)\nelif isinstance(gt_val, StructuredModel) and isinstance(pred_val, StructuredModel):\n    # For recursive StructuredModel comparison\n    recursive_result = gt_val.compare_recursive(pred_val)  # PURE RECURSION\n\n    # Add scoring information to the recursive result\n    raw_score = recursive_result[\"overall\"].get(\"similarity_score\", 0.0)\n    threshold_applied_score = raw_score if raw_score &gt;= threshold or not info.clip_under_threshold else 0.0\n\n    recursive_result[\"raw_similarity_score\"] = raw_score\n    recursive_result[\"similarity_score\"] = raw_score\n    recursive_result[\"threshold_applied_score\"] = threshold_applied_score\n    recursive_result[\"weight\"] = weight\n\n    return recursive_result\nelse:\n    # Mismatched types\n    return {\n        \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0},\n        \"fields\": {},\n        \"raw_similarity_score\": 0.0,\n        \"similarity_score\": 0.0,\n        \"threshold_applied_score\": 0.0,\n        \"weight\": weight\n    }\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#specialized-comparison-handlers","title":"Specialized Comparison Handlers","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#_compare_primitive_with_scoresself-gt_val-any-pred_val-any-field_name-str-dict","title":"<code>_compare_primitive_with_scores(self, gt_val: Any, pred_val: Any, field_name: str) -&gt; dict</code>","text":"<p>Handles simple field comparisons (strings, numbers, dates) with integrated scoring and metrics.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#key-features","title":"Key Features","text":"<ol> <li>Comparator Application: Uses configured comparator (Levenshtein, exact, etc.)</li> <li>Threshold-Based Classification: Converts similarity to binary metrics</li> <li>Configurable Clipping: Respects <code>clip_under_threshold</code> settings</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#implementation","title":"Implementation","text":"<pre><code>def _compare_primitive_with_scores(self, gt_val: Any, pred_val: Any, field_name: str) -&gt; dict:\n    info = self.__class__._get_comparison_info(field_name)\n    raw_similarity = info.comparator.compare(gt_val, pred_val)\n    weight = info.weight\n    threshold = info.threshold\n\n    # For binary classification metrics, always use threshold\n    if raw_similarity &gt;= threshold:\n        metrics = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n        threshold_applied_score = raw_similarity\n    else:\n        metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n        # For score calculation, respect clip_under_threshold setting\n        threshold_applied_score = 0.0 if info.clip_under_threshold else raw_similarity\n\n    # Return hierarchical structure for consistency\n    return {\n        \"overall\": metrics,\n        \"fields\": {},\n        \"raw_similarity_score\": raw_similarity,\n        \"similarity_score\": raw_similarity,\n        \"threshold_applied_score\": threshold_applied_score,\n        \"weight\": weight\n    }\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#_compare_primitive_list_with_scoresself-gt_list-listany-pred_list-listany-field_name-str-dict","title":"<code>_compare_primitive_list_with_scores(self, gt_list: List[Any], pred_list: List[Any], field_name: str) -&gt; dict</code>","text":"<p>Handles lists of primitive values (strings, numbers) using Hungarian matching.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#critical-design-decision-universal-hierarchical-structure","title":"Critical Design Decision: Universal Hierarchical Structure","text":"<p>This method returns a hierarchical structure <code>{\"overall\": {...}, \"fields\": {...}}</code> even for primitive lists to maintain API consistency across all field types.</p> <p>Rationale: - Consistency: All list fields use the same access pattern: <code>cm[\"fields\"][name][\"overall\"]</code> - Test Compatibility: Multiple test files expect this pattern for both primitive and structured lists - Predictable API: Consumers don't need to check field type before accessing metrics</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#emptynull-handling","title":"Empty/Null Handling","text":"<pre><code># CRITICAL FIX: Handle None values before checking length\nif gt_list is None:\n    gt_list = []\nif pred_list is None:\n    pred_list = []\n\n# Handle empty/null list cases first - FIXED: Empty lists should be TN=1\nif len(gt_list) == 0 and len(pred_list) == 0:\n    # Both empty lists should be TN=1\n    return {\n        \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n        \"fields\": {},  # Empty for primitive lists\n        \"raw_similarity_score\": 1.0,  # Perfect match\n        \"similarity_score\": 1.0,\n        \"threshold_applied_score\": 1.0,\n        \"weight\": weight\n    }\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#hungarian-matching-integration","title":"Hungarian Matching Integration","text":"<pre><code># For primitive lists, use the comparison logic from _compare_unordered_lists\ncomparator = info.comparator\nmatch_result = self._compare_unordered_lists(gt_list, pred_list, comparator, threshold)\n\n# Extract the counts from the match result\ntp = match_result.get(\"tp\", 0)\nfd = match_result.get(\"fd\", 0) \nfa = match_result.get(\"fa\", 0)\nfn = match_result.get(\"fn\", 0)\n\n# Use the overall_score from the match result for raw similarity\nraw_similarity = match_result.get(\"overall_score\", 0.0)\n\n# CRITICAL FIX: For lists, we NEVER clip under threshold - partial matches are important\nthreshold_applied_score = raw_similarity  # Always use raw score for lists\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#_compare_struct_list_with_scoresself-gt_list-liststructuredmodel-pred_list-liststructuredmodel-field_name-str-dict","title":"<code>_compare_struct_list_with_scores(self, gt_list: List['StructuredModel'], pred_list: List['StructuredModel'], field_name: str) -&gt; dict</code>","text":"<p>This is the most complex handler, dealing with lists of structured objects. It implements sophisticated object-level and field-level analysis.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#critical-design-principle-object-level-vs-field-level-separation","title":"Critical Design Principle: Object-Level vs Field-Level Separation","text":"<ul> <li>List-level metrics count OBJECTS, not individual fields</li> <li>Field-level details are kept separate for hierarchical analysis</li> <li>Tests expect <code>TP=3</code> for 3 matched objects, not <code>TP=9</code> for 3 objects \u00d7 3 fields each</li> </ul>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#empty-case-handling-with-match-statements","title":"Empty Case Handling with Match Statements","text":"<pre><code>def _handle_struct_list_empty_cases(self, gt_list: List['StructuredModel'], pred_list: List['StructuredModel'], weight: float) -&gt; dict:\n    # Normalize None to empty lists for consistent handling\n    gt_len = len(gt_list or [])\n    pred_len = len(pred_list or [])\n\n    match (gt_len, pred_len):\n        case (0, 0):\n            # Both empty lists \u2192 True Negative\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                \"fields\": {},\n                \"raw_similarity_score\": 1.0,\n                \"similarity_score\": 1.0,\n                \"threshold_applied_score\": 1.0,\n                \"weight\": weight\n            }\n        case (0, pred_len):\n            # GT empty, pred has items \u2192 False Alarms\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": pred_len, \"fd\": 0, \"fp\": pred_len, \"tn\": 0, \"fn\": 0},\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight\n            }\n        case (gt_len, 0):\n            # GT has items, pred empty \u2192 False Negatives\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": gt_len},\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight\n            }\n        case _:\n            # Both non-empty, continue processing\n            return None\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#object-level-metrics-calculation","title":"Object-Level Metrics Calculation","text":"<pre><code>def _calculate_object_level_metrics(self, gt_list: List['StructuredModel'], pred_list: List['StructuredModel'], match_threshold: float) -&gt; tuple:\n    # Use Hungarian matching for OBJECT-LEVEL counts\n    hungarian_helper = HungarianHelper()\n    matched_pairs = hungarian_helper.get_matched_pairs_with_scores(gt_list, pred_list)\n\n    # Count OBJECTS, not individual fields\n    tp_objects = 0  # Objects with similarity &gt;= match_threshold\n    fd_objects = 0  # Objects with similarity &lt; match_threshold\n    for gt_idx, pred_idx, similarity in matched_pairs:\n        if similarity &gt;= match_threshold:\n            tp_objects += 1\n        else:\n            fd_objects += 1\n\n    # Count unmatched objects\n    matched_gt_indices = {idx for idx, _, _ in matched_pairs}\n    matched_pred_indices = {idx for _, idx, _ in matched_pairs}\n    fn_objects = len(gt_list) - len(matched_gt_indices)  # Unmatched GT objects\n    fa_objects = len(pred_list) - len(matched_pred_indices)  # Unmatched pred objects\n\n    # Build list-level metrics counting OBJECTS (not fields)\n    object_level_metrics = {\n        \"tp\": tp_objects,\n        \"fa\": fa_objects,  \n        \"fd\": fd_objects,\n        \"fp\": fa_objects + fd_objects,  # Total false positives\n        \"tn\": 0,  # No true negatives at object level for non-empty lists\n        \"fn\": fn_objects\n    }\n\n    return object_level_metrics, matched_pairs, matched_gt_indices, matched_pred_indices\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#field-level-details-generation-threshold-gated-recursion","title":"Field-Level Details Generation (Threshold-Gated Recursion)","text":"<p>The system only generates detailed field analysis for object pairs that meet the similarity threshold. This is a key performance optimization.</p> <pre><code># Get field-level details for nested structure (but DON'T aggregate to list level)\n# THRESHOLD-GATED RECURSION: Only generate field details for good matches\nfield_details = {}\nif gt_list and isinstance(gt_list[0], StructuredModel):\n    model_class = gt_list[0].__class__\n\n    # Only create field structure if we have good matches (&gt;= match_threshold)\n    has_good_matches = any(sim &gt;= match_threshold for _, _, sim in matched_pairs)\n    has_unmatched = (len(matched_gt_indices) &lt; len(gt_list)) or (len(matched_pred_indices) &lt; len(pred_list))\n\n    # Only generate field details if we have good matches OR unmatched objects\n    if has_good_matches or has_unmatched:\n        for sub_field_name in model_class.model_fields:\n            if sub_field_name == 'extra_fields':\n                continue\n\n            # Check if this field is a List[StructuredModel] that needs hierarchical treatment\n            field_info = model_class.model_fields.get(sub_field_name)\n            is_hierarchical_field = (field_info and model_class._is_structured_field_type(field_info))\n\n            if is_hierarchical_field:\n                # Handle nested List[StructuredModel] fields with aggregation across matched pairs\n                # ... [complex hierarchical processing logic]\n            else:\n                # Handle primitive fields - aggregate across all matched objects\n                sub_field_metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n\n                # THRESHOLD-GATED: Only process matched pairs above match_threshold\n                for gt_idx, pred_idx, similarity in matched_pairs:\n                    if similarity &gt;= match_threshold and gt_idx &lt; len(gt_list) and pred_idx &lt; len(pred_list):\n                        gt_item = gt_list[gt_idx]\n                        pred_item = pred_list[pred_idx]\n                        gt_sub_value = getattr(gt_item, sub_field_name)\n                        pred_sub_value = getattr(pred_item, sub_field_name)\n\n                        # Regular field - use flat classification\n                        field_classification = gt_item._classify_field_for_confusion_matrix(sub_field_name, pred_sub_value)\n\n                        # Aggregate field metrics across all objects\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            sub_field_metrics[metric] += field_classification.get(metric, 0)\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#hungarian-matching-integration_1","title":"Hungarian Matching Integration","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#the-_compare_unordered_lists-method","title":"The <code>_compare_unordered_lists()</code> Method","text":"<p>This method implements the core Hungarian matching algorithm through the <code>HungarianHelper</code> and <code>ComparisonHelper</code> classes.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#key-algorithm-features","title":"Key Algorithm Features","text":"<ol> <li>Optimal Bipartite Matching: Finds the best possible pairing between lists</li> <li>Similarity-Based: Uses actual similarity scores, not just binary match/no-match</li> <li>Handles Unequal Lengths: Gracefully manages lists of different sizes</li> <li>Threshold-Based Classification: Separates matches from false discoveries</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#implementation-strategy","title":"Implementation Strategy","text":"<pre><code># Use HungarianHelper for Hungarian matching operations\nhungarian_helper = HungarianHelper()\n\n# Use the appropriate comparator based on item types\nif all(isinstance(item, StructuredModel) for item in list1[:1]) and all(isinstance(item, StructuredModel) for item in list2[:1]):\n    # For StructuredModel lists, use match_threshold for object-level classification\n    model_class = list1[0].__class__\n    match_threshold = getattr(model_class, 'match_threshold', 0.7)\n    classification_threshold = match_threshold\n\n    # Use HungarianHelper for StructuredModel matching\n    matched_pairs = hungarian_helper.get_matched_pairs_with_scores(list1, list2)\nelse:\n    # Use the provided comparator for other types\n    from stickler.algorithms.hungarian import HungarianMatcher\n    # Use match_threshold=0.0 to capture ALL matches, not just those above threshold\n    hungarian = HungarianMatcher(comparator, match_threshold=0.0)\n    classification_threshold = threshold\n\n    # Get detailed metrics from HungarianMatcher\n    metrics = hungarian.calculate_metrics(list1, list2)\n    matched_pairs = metrics[\"matched_pairs\"]\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#threshold-based-classification","title":"Threshold-Based Classification","text":"<pre><code># Apply threshold logic to classify matches\ntp = 0  # True positives (score &gt;= threshold)\nfd = 0  # False discoveries (score &lt; threshold, including 0)\n\nfor i, j, score in matched_pairs:\n    # Use ThresholdHelper for consistent threshold checking\n    if ThresholdHelper.is_above_threshold(score, classification_threshold):\n        tp += 1\n    else:\n        # All matches below threshold are False Discoveries, including 0.0 scores\n        fd += 1\n\n# False alarms are unmatched prediction items\nfa = len(list2) - len(matched_pairs)\n\n# False negatives are unmatched ground truth items  \nfn = len(list1) - len(matched_pairs)\n\n# Total false positives include both false discoveries and false alarms\nfp = fd + fa\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#overall-score-calculation","title":"Overall Score Calculation","text":"<pre><code># Calculate overall score considering ALL similarities, not just those above threshold\nif not matched_pairs:\n    overall_score = 0.0\nelse:\n    # Average similarity across all matched pairs (regardless of threshold)\n    total_similarity = sum(score for _, _, score in matched_pairs)\n    avg_similarity = total_similarity / len(matched_pairs)\n\n    # Scale by coverage ratio (matched pairs / max list size)\n    max_items = max(len(list1), len(list2))\n    coverage_ratio = len(matched_pairs) / max_items if max_items &gt; 0 else 1.0\n    overall_score = avg_similarity * coverage_ratio\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#score-aggregation-and-percolation","title":"Score Aggregation and Percolation","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#the-percolation-system","title":"The Percolation System","text":"<p>The system uses a \"percolation\" approach where scores bubble up from leaf fields to parent objects, eventually reaching the top-level overall score.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#score-types","title":"Score Types","text":"<ol> <li>Raw Similarity Score: Direct output from comparators (0.0 to 1.0)</li> <li>Similarity Score: Same as raw score, maintained for consistency</li> <li>Threshold Applied Score: Raw score with threshold clipping applied based on <code>clip_under_threshold</code> setting</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#weight-based-aggregation","title":"Weight-Based Aggregation","text":"<pre><code># Score percolation variables\ntotal_score = 0.0\ntotal_weight = 0.0\nthreshold_matched_fields = set()\n\nfor field_name in self.__class__.model_fields:\n    # ... field processing ...\n\n    # Score percolation - aggregate scores upward\n    if \"similarity_score\" in field_result and \"weight\" in field_result:\n        weight = field_result[\"weight\"]\n        threshold_applied_score = field_result[\"threshold_applied_score\"]\n        total_score += threshold_applied_score * weight\n        total_weight += weight\n\n        # Track threshold-matched fields\n        info = self._get_comparison_info(field_name)\n        if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n            threshold_matched_fields.add(field_name)\n\n# Calculate overall similarity score from percolated scores\nif total_weight &gt; 0:\n    result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#all_fields_matched-determination","title":"<code>all_fields_matched</code> Determination","text":"<pre><code># Determine all_fields_matched\nmodel_fields_for_comparison = set(self.__class__.model_fields.keys()) - {'extra_fields'}\nresult[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(model_fields_for_comparison)\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#aggregation-helper-_aggregate_to_overall","title":"Aggregation Helper: <code>_aggregate_to_overall()</code>","text":"<pre><code>def _aggregate_to_overall(self, field_result: dict, overall: dict) -&gt; None:\n    \"\"\"Simple aggregation to overall metrics.\"\"\"\n    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n        if isinstance(field_result, dict):\n            if metric in field_result:\n                overall[metric] += field_result[metric]\n            elif \"overall\" in field_result and metric in field_result[\"overall\"]:\n                overall[metric] += field_result[\"overall\"][metric]\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#threshold-gated-recursion","title":"Threshold-Gated Recursion","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#the-optimization-strategy","title":"The Optimization Strategy","text":"<p>For performance reasons, the system only performs detailed recursive analysis on object pairs that meet a minimum similarity threshold. Poor matches are treated as atomic failures.</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#implementation-in-list-processing","title":"Implementation in List Processing","text":"<pre><code># THRESHOLD-GATED RECURSION: Only perform recursive field analysis for object pairs\n# with similarity &gt;= StructuredModel.match_threshold. Poor matches and unmatched \n# items are treated as atomic units.\n\nmatch_threshold = getattr(model_class, 'match_threshold', 0.7)\n\n# For each field in the nested model\nfor field_name in model_class.model_fields:\n    # ... initialization ...\n\n    # THRESHOLD-GATED RECURSION: Only process pairs that meet the match_threshold\n    for gt_idx, pred_idx, similarity_score in matched_pairs_with_scores:\n        if gt_idx &lt; len(gt_list) and pred_idx &lt; len(pred_list):\n            gt_item = gt_list[gt_idx]\n            pred_item = pred_list[pred_idx]\n\n            # Handle floating point precision issues\n            is_above_threshold = similarity_score &gt;= match_threshold or abs(similarity_score - match_threshold) &lt; 1e-10\n\n            # Only perform recursive field analysis if similarity meets threshold\n            if is_above_threshold:\n                # ... perform detailed field analysis ...\n            else:\n                # Skip recursive analysis for pairs below threshold\n                # These will be handled as FD at the object level\n                pass\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#benefits","title":"Benefits","text":"<ol> <li>Performance: Avoids expensive recursion for obviously poor matches</li> <li>Focus: Concentrates detailed analysis on promising matches</li> <li>Scalability: Handles large lists more efficiently</li> <li>Precision: Maintains accuracy by still counting object-level metrics</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#single-traversal-architecture","title":"Single Traversal Architecture","text":"<p>The biggest performance gain comes from the single-traversal design:</p> <p>Before (Multiple Passes): 1. First pass: Calculate similarity scores 2. Second pass: Generate confusion matrix 3. Third pass: Collect non-matches 4. Result: 3\u00d7 traversal cost</p> <p>After (Single Pass): 1. One pass: Calculate scores AND confusion matrix AND collect non-matches 2. Result: 1\u00d7 traversal cost with identical functionality</p>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code># Add optional features using already-computed recursive result\nif include_confusion_matrix:\n    confusion_matrix = recursive_result\n\n    # Add derived metrics if requested\n    if add_derived_metrics:\n        confusion_matrix = self._add_derived_metrics_to_result(confusion_matrix)\n\n    result[\"confusion_matrix\"] = confusion_matrix\n\n# Add optional non-match documentation\nif document_non_matches:\n    non_matches = recursive_result.get(\"non_matches\", [])\n    if not non_matches:  # Fallback to legacy method if needed\n        non_matches = self._collect_non_matches(other)\n        non_matches = [nm.model_dump() for nm in non_matches]\n    result[\"non_matches\"] = non_matches\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#memory-efficiency","title":"Memory Efficiency","text":"<ol> <li>Hierarchical Results: Results maintain object structure without flattening</li> <li>Streaming Processing: Process fields one at a time rather than loading all into memory</li> <li>Efficient Data Structures: Use sets for tracking rather than lists where possible</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#hungarian-algorithm-optimization","title":"Hungarian Algorithm Optimization","text":"<p>The Hungarian algorithm runs in O(n\u00b3) time, which is optimal for the assignment problem. The implementation optimizations include:</p> <ol> <li>Early Termination: Stop when optimal assignment is found</li> <li>Sparse Matrix Handling: Efficiently handle cases with many zero similarities  </li> <li>Threshold Pre-filtering: Use match_threshold=0.0 to capture all potential matches</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#debugging-and-troubleshooting","title":"Debugging and Troubleshooting","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"Guides/StructuredModel_Advanced_Functionality/#1-incorrect-similarity-scores","title":"1. Incorrect Similarity Scores","text":"<ul> <li>Symptom: Scores don't match expectations</li> <li>Check: Field-level comparator configuration</li> <li>Debug: Add logging to <code>_compare_primitive_with_scores()</code></li> </ul>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#2-confusion-matrix-inconsistencies","title":"2. Confusion Matrix Inconsistencies","text":"<ul> <li>Symptom: TP + FP \u2260 expected totals</li> <li>Check: Object vs field-level counting in list handlers</li> <li>Debug: Verify <code>_calculate_object_level_metrics()</code> logic</li> </ul>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#3-performance-issues","title":"3. Performance Issues","text":"<ul> <li>Symptom: Slow comparison on large objects</li> <li>Check: Threshold-gated recursion settings</li> <li>Debug: Profile <code>compare_recursive()</code> with timing</li> </ul>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#4-memory-usage","title":"4. Memory Usage","text":"<ul> <li>Symptom: High memory consumption</li> <li>Check: Result structure depth and breadth</li> <li>Debug: Monitor object creation in recursive calls</li> </ul>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#debugging-helper-methods","title":"Debugging Helper Methods","text":"<pre><code>def _debug_field_comparison(self, field_name: str, gt_val: Any, pred_val: Any):\n    \"\"\"Add this method for debugging field comparisons.\"\"\"\n    info = self._get_comparison_info(field_name)\n    print(f\"Field: {field_name}\")\n    print(f\"  GT Value: {gt_val}\")\n    print(f\"  Pred Value: {pred_val}\")\n    print(f\"  Comparator: {info.comparator.__class__.__name__}\")\n    print(f\"  Threshold: {info.threshold}\")\n    print(f\"  Weight: {info.weight}\")\n\n    # Add to dispatch method for detailed logging\n    result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n    print(f\"  Result: {result}\")\n    return result\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#testing-complex-scenarios","title":"Testing Complex Scenarios","text":"<p>For testing the monster functions, focus on:</p> <ol> <li>Edge Cases: Empty lists, None values, type mismatches</li> <li>Scale Testing: Large nested structures, deep recursion</li> <li>Performance Testing: Time and memory usage under load</li> <li>Correctness Testing: Known ground truth comparisons</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#profiling-guidelines","title":"Profiling Guidelines","text":"<pre><code>import cProfile\nimport pstats\n\ndef profile_comparison(model1, model2):\n    \"\"\"Profile a comparison operation.\"\"\"\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Perform the comparison\n    result = model1.compare_with(model2, include_confusion_matrix=True)\n\n    profiler.disable()\n\n    # Generate stats\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    stats.print_stats(20)  # Top 20 functions\n\n    return result\n</code></pre>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#key-performance-metrics-to-monitor","title":"Key Performance Metrics to Monitor","text":"<ol> <li>Function Call Counts: Track calls to recursive methods</li> <li>Memory Allocation: Monitor object creation in loops</li> <li>Hungarian Algorithm Performance: O(n\u00b3) scaling behavior</li> <li>Threshold Gate Effectiveness: Ratio of processed vs skipped recursions</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#code-maintenance-guidelines","title":"Code Maintenance Guidelines","text":"<p>When modifying the monster functions, follow these principles:</p> <ol> <li>Preserve Single Traversal: Ensure all optimizations maintain one-pass processing</li> <li>Maintain API Consistency: Keep hierarchical result structures intact</li> <li>Document Design Decisions: Explain any performance vs accuracy trade-offs</li> <li>Test Edge Cases: Verify behavior with empty/null/mismatched data</li> <li>Profile Changes: Measure performance impact of modifications</li> </ol>"},{"location":"Guides/StructuredModel_Advanced_Functionality/#conclusion","title":"Conclusion","text":"<p>The StructuredModel comparison system represents a sophisticated balance of performance, accuracy, and maintainability. The \"monster functions\" implement complex algorithms while maintaining clean, testable interfaces.</p> <p>Key takeaways for developers:</p> <ul> <li>Single Traversal: The core optimization that makes everything else possible</li> <li>Type-Based Dispatch: Clean separation of concerns for different data types</li> <li>Hungarian Matching: Optimal list comparison with O(n\u00b3) complexity</li> <li>Threshold Gating: Performance optimization for deep recursion scenarios</li> <li>Hierarchical Results: Maintains interpretable structure at all levels</li> </ul> <p>Understanding these principles will enable effective debugging, optimization, and extension of the comparison system.</p>"},{"location":"Guides/StructuredModel_Dynamic_Creation/","title":"StructuredModel Dynamic Creation from JSON","text":"<p>This document describes how to create StructuredModel classes dynamically from JSON configuration using the <code>model_from_json()</code> classmethod. This enables configuration-driven model creation with full comparison capabilities.</p>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#overview","title":"Overview","text":"<p>The <code>StructuredModel.model_from_json()</code> method allows you to:</p> <ul> <li>Create StructuredModel classes from JSON configuration</li> <li>Define nested StructuredModel hierarchies</li> <li>Configure custom comparators and thresholds</li> <li>Support lists of StructuredModels with Hungarian matching</li> <li>Enable configuration-driven model creation for flexible applications</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#basic-usage","title":"Basic Usage","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#simple-model-creation","title":"Simple Model Creation","text":"<pre><code>from stickler.structured_object_evaluator.models.structured_model import StructuredModel\n\n# Define model configuration\nperson_config = {\n    \"model_name\": \"Person\",\n    \"fields\": {\n        \"name\": {\n            \"type\": \"str\",\n            \"comparator\": \"LevenshteinComparator\",\n            \"threshold\": 0.8,\n            \"weight\": 1.0,\n            \"required\": True\n        },\n        \"age\": {\n            \"type\": \"int\",\n            \"comparator\": \"NumericComparator\",\n            \"threshold\": 0.9,\n            \"weight\": 0.5,\n            \"required\": True\n        },\n        \"email\": {\n            \"type\": \"str\",\n            \"comparator\": \"ExactComparator\",\n            \"threshold\": 1.0,\n            \"weight\": 1.5,\n            \"required\": False,\n            \"default\": None\n        }\n    }\n}\n\n# Create the model class\nPerson = StructuredModel.model_from_json(person_config)\n\n# Use the model\nperson1 = Person(name=\"John Smith\", age=30, email=\"john@example.com\")\nperson2 = Person(name=\"Jon Smith\", age=31, email=\"john@example.com\")\n\nresult = person1.compare_with(person2)\nprint(f\"Similarity: {result['overall_score']:.3f}\")\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#configuration-schema","title":"Configuration Schema","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#top-level-configuration","title":"Top-Level Configuration","text":"<pre><code>{\n    \"model_name\": \"string\",           // Required: Name of the generated class\n    \"match_threshold\": 0.7,           // Optional: Default threshold for list matching\n    \"fields\": {                       // Required: Field definitions\n        \"field_name\": { ... }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#field-configuration","title":"Field Configuration","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#primitive-fields","title":"Primitive Fields","text":"<pre><code>{\n    \"type\": \"str|int|float|bool|list|dict\",  // Required: Field type\n    \"comparator\": \"ComparatorName\",          // Required: Comparator class name\n    \"comparator_config\": { ... },            // Optional: Comparator configuration\n    \"threshold\": 0.8,                        // Optional: Comparison threshold (0.0-1.0)\n    \"weight\": 1.0,                          // Optional: Field weight (default: 1.0)\n    \"required\": true,                        // Optional: Whether field is required\n    \"default\": null,                         // Optional: Default value\n    \"aggregate\": false,                      // Optional: Enable aggregation\n    \"clip_under_threshold\": true,            // Optional: Clip scores under threshold\n    \"alias\": \"alternative_name\",             // Optional: Field alias\n    \"description\": \"Field description\",      // Optional: Field description\n    \"examples\": [\"example1\", \"example2\"]     // Optional: Example values\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#nested-structuredmodel-fields","title":"Nested StructuredModel Fields","text":"<pre><code>{\n    \"type\": \"structured_model\",              // Single nested model\n    \"threshold\": 0.7,                        // Optional: Nested model threshold\n    \"weight\": 1.0,                          // Optional: Field weight\n    \"fields\": {                             // Required: Nested field definitions\n        \"nested_field\": { ... }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#list-of-structuredmodels","title":"List of StructuredModels","text":"<pre><code>{\n    \"type\": \"list_structured_model\",         // List of nested models\n    \"weight\": 1.0,                          // Optional: Field weight\n    \"match_threshold\": 0.7,                 // Optional: Hungarian matching threshold\n    \"fields\": {                             // Required: Element field definitions\n        \"element_field\": { ... }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#optional-structuredmodel-fields","title":"Optional StructuredModel Fields","text":"<pre><code>{\n    \"type\": \"optional_structured_model\",     // Optional nested model\n    \"threshold\": 0.7,                        // Optional: Nested model threshold\n    \"weight\": 1.0,                          // Optional: Field weight\n    \"fields\": {                             // Required: Nested field definitions\n        \"nested_field\": { ... }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#supported-types","title":"Supported Types","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#primitive-types","title":"Primitive Types","text":"<ul> <li><code>str</code>: String values</li> <li><code>int</code>: Integer values  </li> <li><code>float</code>: Floating-point values</li> <li><code>bool</code>: Boolean values</li> <li><code>list</code>: List of values</li> <li><code>dict</code>: Dictionary/object values</li> <li><code>tuple</code>: Tuple values</li> <li><code>set</code>: Set values</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#generic-types","title":"Generic Types","text":"<ul> <li><code>List</code>: Typed list (equivalent to <code>list</code>)</li> <li><code>Dict</code>: Typed dictionary (equivalent to <code>dict</code>)</li> <li><code>Tuple</code>: Typed tuple (equivalent to <code>tuple</code>)</li> <li><code>Set</code>: Typed set (equivalent to <code>set</code>)</li> <li><code>Optional</code>: Optional type wrapper</li> <li><code>Union</code>: Union type</li> <li><code>Any</code>: Any type</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#structuredmodel-types","title":"StructuredModel Types","text":"<ul> <li><code>structured_model</code>: Single nested StructuredModel</li> <li><code>list_structured_model</code>: List of StructuredModels</li> <li><code>optional_structured_model</code>: Optional StructuredModel</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#available-comparators","title":"Available Comparators","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#string-comparators","title":"String Comparators","text":"<ul> <li><code>ExactComparator</code>: Exact string matching</li> <li><code>LevenshteinComparator</code>: Edit distance-based comparison</li> <li><code>FuzzyComparator</code>: Fuzzy string matching</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#numeric-comparators","title":"Numeric Comparators","text":"<ul> <li><code>NumericComparator</code>: Numeric value comparison with tolerance</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#structured-comparators","title":"Structured Comparators","text":"<ul> <li><code>StructuredComparator</code>: For nested object comparison</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#configuration-examples","title":"Configuration Examples","text":"<pre><code>{\n    \"comparator\": \"LevenshteinComparator\",\n    \"comparator_config\": {\n        \"case_sensitive\": false\n    }\n}\n</code></pre> <pre><code>{\n    \"comparator\": \"NumericComparator\", \n    \"comparator_config\": {\n        \"tolerance\": 0.05  // 5% tolerance\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#nested-model-examples","title":"Nested Model Examples","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#single-nested-model","title":"Single Nested Model","text":"<pre><code>{\n    \"model_name\": \"Company\",\n    \"fields\": {\n        \"name\": {\n            \"type\": \"str\",\n            \"comparator\": \"LevenshteinComparator\",\n            \"threshold\": 0.8,\n            \"weight\": 2.0\n        },\n        \"ceo\": {\n            \"type\": \"structured_model\",\n            \"threshold\": 0.7,\n            \"weight\": 1.5,\n            \"fields\": {\n                \"name\": {\n                    \"type\": \"str\",\n                    \"comparator\": \"LevenshteinComparator\",\n                    \"threshold\": 0.8,\n                    \"weight\": 1.0\n                },\n                \"salary\": {\n                    \"type\": \"float\",\n                    \"comparator\": \"NumericComparator\",\n                    \"threshold\": 0.9,\n                    \"weight\": 0.8\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#list-of-nested-models","title":"List of Nested Models","text":"<pre><code>{\n    \"model_name\": \"Company\",\n    \"fields\": {\n        \"name\": {\n            \"type\": \"str\",\n            \"comparator\": \"LevenshteinComparator\",\n            \"threshold\": 0.8,\n            \"weight\": 2.0\n        },\n        \"employees\": {\n            \"type\": \"list_structured_model\",\n            \"weight\": 1.0,\n            \"match_threshold\": 0.7,\n            \"fields\": {\n                \"name\": {\n                    \"type\": \"str\",\n                    \"comparator\": \"LevenshteinComparator\",\n                    \"threshold\": 0.8,\n                    \"weight\": 1.0\n                },\n                \"department\": {\n                    \"type\": \"str\",\n                    \"comparator\": \"ExactComparator\",\n                    \"threshold\": 1.0,\n                    \"weight\": 0.5\n                },\n                \"salary\": {\n                    \"type\": \"float\",\n                    \"comparator\": \"NumericComparator\",\n                    \"threshold\": 0.95,\n                    \"weight\": 0.7\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#loading-from-json-files","title":"Loading from JSON Files","text":"<pre><code>import json\nfrom stickler.structured_object_evaluator.models.structured_model import StructuredModel\n\n# Load configuration from file\nwith open('model_config.json', 'r') as f:\n    config = json.load(f)\n\n# Create model class\nMyModel = StructuredModel.model_from_json(config)\n\n# Use the model\ninstance1 = MyModel(**data1)\ninstance2 = MyModel(**data2)\nresult = instance1.compare_with(instance2)\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#advanced-features","title":"Advanced Features","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#field-weights-and-thresholds","title":"Field Weights and Thresholds","text":"<pre><code>{\n    \"name\": {\n        \"type\": \"str\",\n        \"comparator\": \"LevenshteinComparator\",\n        \"threshold\": 0.8,    // Minimum similarity for match\n        \"weight\": 2.0        // 2x importance in overall score\n    },\n    \"optional_field\": {\n        \"type\": \"str\", \n        \"comparator\": \"ExactComparator\",\n        \"threshold\": 1.0,\n        \"weight\": 0.5,       // Half importance\n        \"required\": false,\n        \"default\": null\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#aggregation-support","title":"Aggregation Support","text":"<pre><code>{\n    \"score\": {\n        \"type\": \"float\",\n        \"comparator\": \"NumericComparator\",\n        \"threshold\": 0.9,\n        \"weight\": 1.0,\n        \"aggregate\": true    // Enable aggregation for this field\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#threshold-clipping","title":"Threshold Clipping","text":"<pre><code>{\n    \"critical_field\": {\n        \"type\": \"str\",\n        \"comparator\": \"ExactComparator\", \n        \"threshold\": 1.0,\n        \"weight\": 3.0,\n        \"clip_under_threshold\": true  // Set score to 0 if under threshold\n    }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#hungarian-matching-for-lists","title":"Hungarian Matching for Lists","text":"<p>When using <code>list_structured_model</code>, the system automatically applies Hungarian matching to find the optimal pairing between list elements:</p> <pre><code># Lists are compared using Hungarian matching\ncompany1 = Company(\n    name=\"TechCorp\",\n    employees=[\n        {\"name\": \"Alice\", \"department\": \"Engineering\"},\n        {\"name\": \"Bob\", \"department\": \"Marketing\"}\n    ]\n)\n\ncompany2 = Company(\n    name=\"TechCorp\", \n    employees=[\n        {\"name\": \"Bob\", \"department\": \"Marketing\"},    # Reordered\n        {\"name\": \"Alice\", \"department\": \"Engineering\"} # Reordered\n    ]\n)\n\n# Hungarian matching finds optimal pairing despite reordering\nresult = company1.compare_with(company2)\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#error-handling","title":"Error Handling","text":"<p>The system provides detailed error messages for configuration issues:</p> <pre><code>try:\n    Model = StructuredModel.model_from_json(config)\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n    # Example: \"Invalid type for field 'age': Unknown type: 'integer'\"\n    # Example: \"Field 'name' missing required 'comparator' parameter\"\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#best-practices","title":"Best Practices","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#1-field-naming","title":"1. Field Naming","text":"<ul> <li>Use descriptive field names</li> <li>Follow consistent naming conventions</li> <li>Avoid reserved Python keywords</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#2-threshold-selection","title":"2. Threshold Selection","text":"<ul> <li>Start with default thresholds (0.7-0.8)</li> <li>Adjust based on data characteristics</li> <li>Use higher thresholds for critical fields</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#3-weight-assignment","title":"3. Weight Assignment","text":"<ul> <li>Assign higher weights to more important fields</li> <li>Consider the relative importance in your domain</li> <li>Test with representative data</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#4-nested-model-design","title":"4. Nested Model Design","text":"<ul> <li>Keep nesting levels reasonable (2-3 levels max)</li> <li>Group related fields into nested models</li> <li>Use meaningful names for nested model classes</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#5-list-matching","title":"5. List Matching","text":"<ul> <li>Set appropriate <code>match_threshold</code> for list elements</li> <li>Consider the expected similarity of list items</li> <li>Test with various list sizes and orderings</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#performance-considerations","title":"Performance Considerations","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#model-creation","title":"Model Creation","text":"<ul> <li>Model classes are created once and can be reused</li> <li>Cache created model classes for better performance</li> <li>Avoid recreating models unnecessarily</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#comparison-performance","title":"Comparison Performance","text":"<ul> <li>Nested models add computational overhead</li> <li>List comparisons use O(n\u00b3) Hungarian algorithm</li> <li>Consider field weights to optimize important comparisons</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#memory-usage","title":"Memory Usage","text":"<ul> <li>Dynamic models have similar memory footprint to static models</li> <li>Nested models create additional object instances</li> <li>Large lists of nested models can consume significant memory</li> </ul>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#integration-examples","title":"Integration Examples","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#configuration-driven-applications","title":"Configuration-Driven Applications","text":"<pre><code>class ModelFactory:\n    def __init__(self, config_dir):\n        self.models = {}\n        self.load_models(config_dir)\n\n    def load_models(self, config_dir):\n        for config_file in Path(config_dir).glob(\"*.json\"):\n            with open(config_file) as f:\n                config = json.load(f)\n            model_name = config[\"model_name\"]\n            self.models[model_name] = StructuredModel.model_from_json(config)\n\n    def get_model(self, name):\n        return self.models[name]\n\n# Usage\nfactory = ModelFactory(\"model_configs/\")\nPersonModel = factory.get_model(\"Person\")\nCompanyModel = factory.get_model(\"Company\")\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#api-integration","title":"API Integration","text":"<pre><code>from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/compare', methods=['POST'])\ndef compare_objects():\n    data = request.json\n    config = data['model_config']\n    obj1_data = data['object1']\n    obj2_data = data['object2']\n\n    # Create model dynamically\n    Model = StructuredModel.model_from_json(config)\n\n    # Create instances and compare\n    obj1 = Model(**obj1_data)\n    obj2 = Model(**obj2_data)\n    result = obj1.compare_with(obj2)\n\n    return jsonify(result)\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Guides/StructuredModel_Dynamic_Creation/#common-issues","title":"Common Issues","text":"<ol> <li>\"Unknown type\" errors: Check that the type string is in the supported types list</li> <li>\"Missing comparator\" errors: Ensure primitive fields have comparator specified</li> <li>\"Invalid threshold\" errors: Thresholds must be between 0.0 and 1.0</li> <li>Nested model validation errors: Check that nested field configurations are valid</li> </ol>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Start with simple configurations and add complexity gradually</li> <li>Use the validation methods to check configurations before creating models</li> <li>Test with small datasets before scaling up</li> <li>Check the generated model's <code>__annotations__</code> to verify field types</li> </ol>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#validation","title":"Validation","text":"<pre><code>from stickler.structured_object_evaluator.models.field_converter import validate_fields_config\n\n# Validate configuration before creating model\ntry:\n    validate_fields_config(config['fields'])\n    print(\"Configuration is valid\")\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"Guides/StructuredModel_Dynamic_Creation/#see-also","title":"See Also","text":"<ul> <li>StructuredModel Advanced Functionality</li> <li>Comparators Documentation</li> <li>Examples</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/","title":"StructuredModel <code>compare_with</code> Method: A Layman's Guide","text":""},{"location":"Guides/StructuredModel_compare_with_README/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What It Does</li> <li>Why It Matters</li> <li>How It Works</li> <li>Flow Chart</li> <li>Key Concepts</li> <li>Examples</li> <li>Architecture Overview</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#what-it-does","title":"What It Does","text":"<p>The <code>compare_with</code> method is like having a super-smart assistant that can compare two complex documents (like invoices, contracts, or product catalogs) and tell you:</p> <ul> <li>How similar they are (as a percentage score)</li> <li>Which specific parts match or don't match (field-by-field analysis)</li> <li>Detailed statistics about the comparison (like how many items were correct, incorrect, or missing)</li> </ul> <p>Think of it like comparing two shopping receipts - but instead of just looking at totals, it can compare every line item, every date, every store detail, and even handle cases where items are in different orders or some information is missing.</p>"},{"location":"Guides/StructuredModel_compare_with_README/#why-it-matters","title":"Why It Matters","text":"<p>This system is crucial for:</p> <ul> <li>Document Processing: Automatically checking if extracted data matches expected formats</li> <li>Quality Assurance: Validating that AI systems correctly understand documents</li> <li>Data Migration: Ensuring data transfers preserve all important information</li> <li>Compliance: Proving that automated systems meet accuracy requirements</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#how-it-works","title":"How It Works","text":""},{"location":"Guides/StructuredModel_compare_with_README/#the-big-picture","title":"The Big Picture","text":"<ol> <li>Start: You give it two structured objects to compare (like two invoices)</li> <li>Field-by-Field Analysis: It looks at every piece of information in both objects</li> <li>Smart Matching: For lists of items, it figures out which items should be compared to each other</li> <li>Scoring: It calculates how similar each part is and combines them into an overall score</li> <li>Detailed Report: It gives you both a simple score and detailed breakdown</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#the-process-step-by-step","title":"The Process Step-by-Step","text":"<ol> <li>Preparation: The method receives two objects and comparison options</li> <li>Recursive Traversal: It walks through every field in both objects simultaneously</li> <li>Type-Specific Handling: Different types of data get different comparison treatments:</li> <li>Simple text/numbers: Direct comparison</li> <li>Lists: Smart matching to pair up similar items</li> <li>Nested objects: Recursive comparison of sub-fields</li> <li>Score Calculation: Each comparison gets a similarity score (0.0 to 1.0)</li> <li>Aggregation: All scores are combined using weighted averages</li> <li>Result Assembly: Final results include scores, statistics, and detailed breakdowns</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#flow-chart","title":"Flow Chart","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   compare_with()        \u2502 \u2190 Entry Point\n\u2502   (Public Interface)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   compare_recursive()   \u2502 \u2190 Core Logic Engine\n\u2502   (Single Traversal)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   For Each Field:       \u2502\n\u2502   _dispatch_field_      \u2502 \u2190 Field Processing Loop\n\u2502   _comparison()         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Field Type Check      \u2502 \u2190 Decision Point\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502     \u2502     \u2502\n      \u25bc     \u25bc     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Primitive\u2502 \u2502  List   \u2502 \u2502   Nested    \u2502\n\u2502  Field  \u2502 \u2502  Field  \u2502 \u2502   Object    \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502           \u2502             \u2502\n     \u2502           \u25bc             \u2502\n     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n     \u2502    \u2502  Hungarian  \u2502      \u2502 \u2190 Smart List Matching\n     \u2502    \u2502  Matching   \u2502      \u2502\n     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n     \u2502           \u2502             \u2502\n     \u25bc           \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Score Calculation &amp;        \u2502 \u2190 Similarity Assessment\n\u2502     Threshold Application      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Result Aggregation &amp;        \u2502 \u2190 Combine All Results\n\u2502    Score Percolation           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Format Final Result        \u2502 \u2190 Output Generation\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#pseudocode-logic","title":"Pseudocode Logic","text":""},{"location":"Guides/StructuredModel_compare_with_README/#core-compare_with-method","title":"Core <code>compare_with</code> Method","text":"<pre><code>Algorithm: compare_with(other_model, options)\nInput: other_model (StructuredModel), options (dict)\n\n1. CALL recursive_result = compare_recursive(other_model)\n2. EXTRACT field_scores from recursive_result\n3. EXTRACT overall_score from recursive_result\n4. EXTRACT all_fields_matched from recursive_result\n\n5. CREATE result = {\n     field_scores: field_scores,\n     overall_score: overall_score,\n     all_fields_matched: all_fields_matched\n   }\n\n6. IF options.include_confusion_matrix THEN\n7.     ADD recursive_result to result.confusion_matrix\n8. ENDIF\n\n9. IF options.document_non_matches THEN\n10.    ADD non_matches to result.non_matches\n11. ENDIF\n\n12. IF options.evaluator_format THEN\n13.    TRANSFORM result for evaluator compatibility\n14. ENDIF\n\n15. RETURN result\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#core-recursive-comparison","title":"Core Recursive Comparison","text":"<pre><code>Algorithm: compare_recursive(other_model)\nInput: other_model (StructuredModel)\n\n1. INITIALIZE total_score = 0, total_weight = 0\n2. INITIALIZE overall_metrics = {tp: 0, fa: 0, fd: 0, fp: 0, tn: 0, fn: 0}\n3. INITIALIZE field_results = {}\n\n4. FOR each field_name IN model_fields DO\n5.     GET gt_value = get_field(field_name)\n6.     GET pred_value = other_model.get_field(field_name)\n7.     \n8.     CALL field_result = dispatch_field_comparison(field_name, gt_value, pred_value)\n9.     \n10.    SET field_results[field_name] = field_result\n11.    AGGREGATE field_result INTO overall_metrics\n12.    \n13.    UPDATE total_score += field_result.score \u00d7 field_result.weight\n14.    UPDATE total_weight += field_result.weight\n15. ENDFOR\n\n16. CALCULATE overall_score = total_score / total_weight\n17. RETURN {overall: overall_metrics, fields: field_results, overall_score: overall_score}\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#field-type-dispatch","title":"Field Type Dispatch","text":"<pre><code>Algorithm: dispatch_field_comparison(field_name, gt_val, pred_val)\nInput: field_name (string), gt_val (any), pred_val (any)\n\n1. GET field_config = get_field_config(field_name)\n2. \n3. IF both_null(gt_val, pred_val) THEN\n4.     RETURN true_negative_result()\n5. ENDIF\n6. \n7. IF one_null(gt_val, pred_val) THEN\n8.     RETURN false_alarm_or_negative_result()\n9. ENDIF\n10. \n11. MATCH field_type:\n12.     CASE primitive: RETURN compare_primitive(gt_val, pred_val, field_config)\n13.     CASE list: RETURN compare_list(gt_val, pred_val, field_config)\n14.     CASE nested_object: RETURN compare_nested(gt_val, pred_val, field_config)\n15. ENDMATCH\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#list-matching-hungarian-algorithm","title":"List Matching (Hungarian Algorithm)","text":"<pre><code>Algorithm: hungarian_list_matching(gt_list, pred_list, config)\nInput: gt_list, pred_list, config\n\n1. IF both_empty(gt_list, pred_list) THEN\n2.     RETURN perfect_match()\n3. ENDIF\n\n4. IF one_empty(gt_list, pred_list) THEN\n5.     RETURN count_unmatched_items()\n6. ENDIF\n\n7. BUILD similarity_matrix FOR all pairs\n8. RUN hungarian_algorithm(similarity_matrix)\n9. GET matched_pairs WITH similarity_scores\n\n10. CLASSIFY matches:\n11.     IF similarity \u2265 threshold THEN tp++\n12.     ELSE fd++\n\n13. COUNT unmatched_items:\n14.     fn = unmatched_gt_items\n15.     fa = unmatched_pred_items\n\n16. CALCULATE overall_score = average_similarity \u00d7 coverage_ratio\n17. RETURN {tp, fd, fa, fn, overall_score, matched_pairs}\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#structured-list-processing","title":"Structured List Processing","text":"<pre><code>Algorithm: compare_struct_list(gt_list, pred_list, field_name)\nInput: gt_list, pred_list, field_name\n\n1. GET match_threshold FROM gt_list[0].__class__\n2. \n3. HANDLE empty_cases:\n4.     IF both_empty THEN RETURN true_negative\n5.     IF one_empty THEN RETURN false_alarms_or_negatives\n6. \n7. RUN hungarian_matching(gt_list, pred_list)\n8. GET matched_pairs WITH object_similarities\n9. \n10. CLASSIFY objects:\n11.     IF object_similarity \u2265 match_threshold THEN tp_objects++\n12.     ELSE fd_objects++\n13. \n14. FOR good_matches_only DO\n15.     FOR each sub_field IN object_fields DO\n16.         COMPARE sub_field_values\n17.         AGGREGATE sub_field_metrics\n18.     ENDFOR\n19. ENDFOR\n20. \n21. RETURN {object_metrics, field_details, overall_score}\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#key-concepts","title":"Key Concepts","text":""},{"location":"Guides/StructuredModel_compare_with_README/#1-field-types","title":"1. Field Types","text":"<ul> <li>Primitive Fields: Simple data like names, dates, numbers</li> <li>List Fields: Collections of items (like transaction lists)</li> <li>Nested Objects: Complex structures containing other fields</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#2-hungarian-matching","title":"2. Hungarian Matching","text":"<p>For lists of items (like products in an order), the system uses a sophisticated algorithm to figure out which items in list A should be compared to which items in list B. This handles cases where: - Items are in different orders - Some items are missing - Lists have different lengths</p> <p>Example:  <pre><code>Ground Truth: [Apple, Banana, Cherry]\nPrediction:   [Banana, Apple, Orange]\n</code></pre> The algorithm pairs: Apple\u2194Apple, Banana\u2194Banana, and notes Cherry is missing while Orange is extra.</p>"},{"location":"Guides/StructuredModel_compare_with_README/#3-thresholds-and-weights","title":"3. Thresholds and Weights","text":"<ul> <li>Threshold: Minimum similarity score to consider two items as \"matching\" (e.g., 0.7 = 70%)</li> <li>Weight: How important each field is in the overall score (e.g., customer name might be weighted higher than order date)</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#4-confusion-matrix-metrics","title":"4. Confusion Matrix Metrics","text":"<p>The system tracks detailed statistics: - True Positives (TP): Correctly identified matches - False Positives (FP): Incorrectly claimed matches - False Negatives (FN): Missed actual matches - True Negatives (TN): Correctly identified non-matches</p>"},{"location":"Guides/StructuredModel_compare_with_README/#5-score-percolation","title":"5. Score Percolation","text":"<p>Scores \"bubble up\" from detailed fields to overall scores: <pre><code>Field Level:    name=0.9, date=1.0, amount=0.8\n\u2193\nObject Level:   Weighted average = 0.9\n\u2193  \nDocument Level: Overall similarity = 0.9\n</code></pre></p>"},{"location":"Guides/StructuredModel_compare_with_README/#examples","title":"Examples","text":""},{"location":"Guides/StructuredModel_compare_with_README/#example-1-simple-invoice-comparison","title":"Example 1: Simple Invoice Comparison","text":"<p>Ground Truth Invoice: <pre><code>{\n  \"invoice_number\": \"INV-001\",\n  \"date\": \"2024-01-15\",\n  \"amount\": 150.00\n}\n</code></pre></p> <p>Predicted Invoice: <pre><code>{\n  \"invoice_number\": \"INV-001\",\n  \"date\": \"2024-01-15\", \n  \"amount\": 155.00\n}\n</code></pre></p> <p>Result: - <code>invoice_number</code>: 1.0 (exact match) - <code>date</code>: 1.0 (exact match) - <code>amount</code>: 0.0 (differs by more than threshold) - Overall Score: 0.67 (2 out of 3 fields match)</p>"},{"location":"Guides/StructuredModel_compare_with_README/#example-2-list-comparison-with-hungarian-matching","title":"Example 2: List Comparison with Hungarian Matching","text":"<p>Ground Truth Products: <pre><code>{\n  \"products\": [\n    {\"name\": \"Widget A\", \"price\": 10.00},\n    {\"name\": \"Widget B\", \"price\": 20.00}\n  ]\n}\n</code></pre></p> <p>Predicted Products: <pre><code>{\n  \"products\": [\n    {\"name\": \"Widget B\", \"price\": 20.00},\n    {\"name\": \"Widget A\", \"price\": 10.50}\n  ]\n}\n</code></pre></p> <p>Hungarian Matching Process: 1. Calculate similarity between all pairs 2. Find optimal assignment: GT[0]\u2194Pred[1], GT[1]\u2194Pred[0] 3. Compare matched pairs:    - Widget A vs Widget A: High similarity (price slightly off)    - Widget B vs Widget B: Perfect match</p>"},{"location":"Guides/StructuredModel_compare_with_README/#example-3-nested-object-comparison","title":"Example 3: Nested Object Comparison","text":"<p>Ground Truth Customer: <pre><code>{\n  \"name\": \"John Doe\",\n  \"address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"Springfield\",\n    \"zip\": \"12345\"\n  }\n}\n</code></pre></p> <p>The system recursively compares: 1. Top-level <code>name</code> field 2. Nested <code>address</code> object:    - <code>address.street</code>    - <code>address.city</code>     - <code>address.zip</code></p> <p>Each nested field contributes to the overall score.</p>"},{"location":"Guides/StructuredModel_compare_with_README/#architecture-overview","title":"Architecture Overview","text":""},{"location":"Guides/StructuredModel_compare_with_README/#core-components","title":"Core Components","text":"<ol> <li>StructuredModel: The main class that defines comparable data structures</li> <li>ComparableField: Configuration for how each field should be compared</li> <li>ComparisonHelper: Utilities for field-level comparisons</li> <li>HungarianHelper: Implements the Hungarian matching algorithm</li> <li>Various Helpers: Specialized utilities for thresholds, metrics, formatting</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#helper-classes","title":"Helper Classes","text":"<ul> <li>MetricsHelper: Calculates precision, recall, F1-score, etc.</li> <li>ThresholdHelper: Handles threshold application and edge cases  </li> <li>NonMatchesHelper: Documents what didn't match for debugging</li> <li>FieldHelper: Utilities for field type detection and null handling</li> <li>ConfigurationHelper: Manages field comparison settings</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#design-principles","title":"Design Principles","text":"<ol> <li> <p>Single Traversal Optimization: The system walks through the object structure only once, gathering both similarity scores and confusion matrix data simultaneously. This is much more efficient than separate passes.</p> </li> <li> <p>Type-Based Dispatch: Different field types get different treatment:</p> </li> <li>Primitives: Direct comparator application with threshold checking</li> <li>Lists: Hungarian matching for optimal pairing</li> <li> <p>Nested Objects: Recursive descent into sub-structures</p> </li> <li> <p>Threshold-Gated Recursion: For complex nested structures, the system only performs detailed analysis on object pairs that meet similarity thresholds. Poor matches are treated as atomic failures.</p> </li> <li> <p>Hierarchical Result Structure: Results maintain the same nested structure as the input objects, making it easy to understand which specific parts matched or failed.</p> </li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#method-flow-breakdown","title":"Method Flow Breakdown","text":""},{"location":"Guides/StructuredModel_compare_with_README/#1-entry-point-compare_with","title":"1. Entry Point: <code>compare_with()</code>","text":"<ul> <li>Purpose: Public API that orchestrates the comparison</li> <li>Parameters: </li> <li><code>other</code>: The object to compare against</li> <li><code>include_confusion_matrix</code>: Whether to include detailed metrics</li> <li><code>document_non_matches</code>: Whether to document failures</li> <li><code>evaluator_format</code>: Whether to format for evaluation tools</li> <li>Returns: Comprehensive comparison results</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#2-core-engine-compare_recursive","title":"2. Core Engine: <code>compare_recursive()</code>","text":"<ul> <li>Purpose: Single-traversal comparison engine</li> <li>Key Innovation: Gathers similarity scores AND confusion matrix data in one pass</li> <li>Output: Hierarchical result structure with metrics at every level</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#3-field-dispatcher-_dispatch_field_comparison","title":"3. Field Dispatcher: <code>_dispatch_field_comparison()</code>","text":"<ul> <li>Purpose: Routes each field to appropriate comparison logic based on type</li> <li>Handles: Null checks, type detection, and routing to specialized handlers</li> <li>Uses Match Statements: Clean, efficient type-based routing</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#4-specialized-handlers","title":"4. Specialized Handlers:","text":"<p><code>_compare_primitive_with_scores()</code>: - Handles simple fields (strings, numbers, dates) - Applies comparators (Levenshtein, exact match, etc.) - Converts similarity scores to binary classification metrics</p> <p><code>_compare_primitive_list_with_scores()</code>: - Handles lists of simple items - Uses Hungarian matching for optimal pairing - Handles unmatched items as false positives/negatives</p> <p><code>_compare_struct_list_with_scores()</code>: - Handles lists of complex objects - Uses object-level Hungarian matching - Supports threshold-gated recursion for field-level details - Maintains separation between object-level and field-level metrics</p>"},{"location":"Guides/StructuredModel_compare_with_README/#5-hungarian-matching","title":"5. Hungarian Matching","text":"<ul> <li>Algorithm: Optimal bipartite matching using the Hungarian algorithm</li> <li>Handles: Different list lengths, optimal pairing, similarity scoring</li> <li>Output: Matched pairs with similarity scores, plus unmatched items</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#6-score-calculation-and-aggregation","title":"6. Score Calculation and Aggregation","text":"<ul> <li>Raw Scores: Direct comparator outputs (0.0 to 1.0)</li> <li>Threshold Application: Convert to binary pass/fail based on thresholds</li> <li>Weight Application: Combine field scores using configured weights</li> <li>Percolation: Bubble up scores from leaf fields to parent objects</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#performance-optimizations","title":"Performance Optimizations","text":"<ol> <li>Single Traversal: Eliminates redundant object walks</li> <li>Lazy Evaluation: Only calculates detailed metrics when requested</li> <li>Threshold Gating: Skips expensive recursion for poor matches</li> <li>Efficient Matching: Hungarian algorithm runs in O(n\u00b3) time</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#error-handling","title":"Error Handling","text":"<ul> <li>Type Mismatches: Graceful handling when field types don't match</li> <li>Missing Fields: Treats as null for comparison purposes</li> <li>Circular References: Prevented through proper object traversal</li> <li>Memory Management: Efficient result structure prevents memory bloat</li> </ul>"},{"location":"Guides/StructuredModel_compare_with_README/#complete-output-structure-reference","title":"Complete Output Structure Reference","text":"<p>The <code>compare_with</code> method returns different structures based on the parameters provided. Here's the complete reference:</p>"},{"location":"Guides/StructuredModel_compare_with_README/#1-basic-output-default-parameters","title":"1. Basic Output (Default Parameters)","text":"<pre><code>result = model1.compare_with(model2)\n</code></pre> <p>Structure: <pre><code>{\n  \"field_scores\": {\n    \"field_name\": 0.85,\n    \"another_field\": 1.0\n  },\n  \"overall_score\": 0.92,\n  \"all_fields_matched\": true\n}\n</code></pre></p> <p>Fields: - <code>field_scores</code>: Dictionary mapping field names to similarity scores (0.0-1.0) - <code>overall_score</code>: Weighted average of all field scores (0.0-1.0) - <code>all_fields_matched</code>: Boolean indicating if all fields met their thresholds</p>"},{"location":"Guides/StructuredModel_compare_with_README/#2-with-confusion-matrix","title":"2. With Confusion Matrix","text":"<pre><code>result = model1.compare_with(model2, include_confusion_matrix=True)\n</code></pre> <p>Structure: <pre><code>{\n  \"field_scores\": {...},\n  \"overall_score\": 0.92,\n  \"all_fields_matched\": true,\n  \"confusion_matrix\": {\n    \"overall\": {\n      \"tp\": 5, \"fa\": 1, \"fd\": 2, \"fp\": 3, \"tn\": 0, \"fn\": 1,\n      \"similarity_score\": 0.92,\n      \"all_fields_matched\": true,\n      \"derived\": {\n        \"cm_precision\": 0.83,\n        \"cm_recall\": 0.91,\n        \"cm_f1\": 0.87,\n        \"cm_accuracy\": 0.85\n      }\n    },\n    \"fields\": {\n      \"field_name\": {\n        \"overall\": {\n          \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0,\n          \"similarity_score\": 1.0,\n          \"all_fields_matched\": true\n        },\n        \"fields\": {},\n        \"aggregate\": {\n          \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0,\n          \"derived\": {\n            \"cm_precision\": 1.0,\n            \"cm_recall\": 1.0,\n            \"cm_f1\": 1.0,\n            \"cm_accuracy\": 1.0\n          }\n        }\n      }\n    },\n    \"non_matches\": [],\n    \"aggregate\": {\n      \"tp\": 5, \"fa\": 1, \"fd\": 2, \"fp\": 3, \"tn\": 0, \"fn\": 1,\n      \"derived\": {\n        \"cm_precision\": 0.83,\n        \"cm_recall\": 0.91,\n        \"cm_f1\": 0.87,\n        \"cm_accuracy\": 0.85\n      }\n    }\n  }\n}\n</code></pre></p> <p>Key Features: - Universal Aggregate Fields: Every node includes an <code>aggregate</code> field that sums all primitive field metrics below that node - Hierarchical Structure: Maintains the same nested structure as your data models - Derived Metrics: Automatic calculation of precision, recall, F1-score, and accuracy - Field-Level Granularity: Access confusion matrix metrics at any level of nesting</p>"},{"location":"Guides/StructuredModel_compare_with_README/#3-with-non-matches-documentation","title":"3. With Non-Matches Documentation","text":"<pre><code>result = model1.compare_with(model2, document_non_matches=True)\n</code></pre> <p>Additional Field: <pre><code>{\n  \"non_matches\": [\n    {\n      \"field_path\": \"contact.phone\",\n      \"non_match_type\": \"false_discovery\",\n      \"ground_truth_value\": \"555-123-4567\",\n      \"prediction_value\": \"555-999-8888\",\n      \"similarity_score\": 0.3,\n      \"details\": {\n        \"reason\": \"below threshold (0.300 &lt; 1.0)\"\n      }\n    }\n  ]\n}\n</code></pre></p> <p>Non-Match Types: - <code>false_discovery</code>: Fields that don't meet similarity threshold - <code>false_alarm</code>: Predicted fields that shouldn't exist - <code>false_negative</code>: Missing fields that should exist</p>"},{"location":"Guides/StructuredModel_compare_with_README/#4-evaluator-format","title":"4. Evaluator Format","text":"<pre><code>result = model1.compare_with(model2, evaluator_format=True)\n</code></pre> <p>Structure (Optimized for evaluation tools): <pre><code>{\n  \"overall\": {\n    \"precision\": 0.83,\n    \"recall\": 0.91,\n    \"f1\": 0.87,\n    \"accuracy\": 0.85,\n    \"anls_score\": 0.89\n  },\n  \"fields\": {\n    \"field_name\": {\n      \"precision\": 1.0,\n      \"recall\": 1.0,\n      \"f1\": 1.0,\n      \"accuracy\": 1.0\n    }\n  },\n  \"confusion_matrix\": {},\n  \"non_matches\": []\n}\n</code></pre></p>"},{"location":"Guides/StructuredModel_compare_with_README/#5-complete-parameter-reference","title":"5. Complete Parameter Reference","text":"<pre><code>result = model1.compare_with(\n    other,                      # Required: Model to compare against\n    include_confusion_matrix=True,   # Include detailed metrics\n    document_non_matches=True,       # Document what didn't match\n    evaluator_format=False,          # Format for evaluation tools\n    recall_with_fd=False,           # Include FD in recall calculation\n    add_derived_metrics=True        # Add precision/recall/F1 metrics\n)\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#universal-aggregate-fields-new-feature","title":"Universal Aggregate Fields (NEW FEATURE)","text":""},{"location":"Guides/StructuredModel_compare_with_README/#what-are-aggregate-fields","title":"What Are Aggregate Fields?","text":"<p>Every node in the confusion matrix now automatically includes an <code>aggregate</code> field that contains the sum of all primitive field confusion matrices below that node. This provides universal field-level granularity without any configuration.</p>"},{"location":"Guides/StructuredModel_compare_with_README/#structure","title":"Structure","text":"<pre><code>{\n  \"confusion_matrix\": {\n    \"aggregate\": {\n      \"tp\": 8, \"fa\": 2, \"fd\": 1, \"fp\": 3, \"tn\": 0, \"fn\": 1,\n      \"derived\": {...}\n    },\n    \"fields\": {\n      \"contact\": {\n        \"aggregate\": {\n          \"tp\": 2, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0,\n          \"derived\": {...}\n        },\n        \"fields\": {\n          \"phone\": {\n            \"aggregate\": {\n              \"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0,\n              \"derived\": {...}\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#benefits","title":"Benefits","text":"<ol> <li>Universal Access: Get aggregate metrics at any level without configuration</li> <li>Hierarchical Analysis: Understand which sections of your data have issues</li> <li>Automatic: Works out of the box for all comparisons</li> <li>Consistent: Every node has the same structure</li> </ol>"},{"location":"Guides/StructuredModel_compare_with_README/#usage-examples","title":"Usage Examples","text":"<pre><code># Get total primitive field metrics across entire comparison\ntotal_tp = result['confusion_matrix']['aggregate']['tp']\n\n# Get metrics for a specific section (e.g., contact information)\ncontact_metrics = result['confusion_matrix']['fields']['contact']['aggregate']\ncontact_f1 = contact_metrics['derived']['cm_f1']\n\n# Get metrics for deeply nested fields\naddress_metrics = result['confusion_matrix']['fields']['customer']['fields']['address']['aggregate']\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#using-the-method","title":"Using the Method","text":""},{"location":"Guides/StructuredModel_compare_with_README/#basic-usage","title":"Basic Usage","text":"<pre><code># Simple comparison\nresult = model1.compare_with(model2)\nprint(f\"Overall similarity: {result['overall_score']:.2%}\")\n\n# With detailed metrics\nresult = model1.compare_with(model2, include_confusion_matrix=True)\nconfusion_matrix = result['confusion_matrix']\nprint(f\"True Positives: {confusion_matrix['overall']['tp']}\")\n\n# Access universal aggregate fields\ntotal_aggregate = confusion_matrix['aggregate']\nprint(f\"Total primitive TP: {total_aggregate['tp']}\")\nprint(f\"Overall F1: {total_aggregate['derived']['cm_f1']:.3f}\")\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#advanced-options","title":"Advanced Options","text":"<pre><code># Complete analysis with non-match documentation\nresult = model1.compare_with(\n    model2,\n    include_confusion_matrix=True,\n    document_non_matches=True,\n    evaluator_format=False\n)\n\n# Access field-level scores\nfield_scores = result['field_scores']\nfor field, score in field_scores.items():\n    print(f\"{field}: {score:.2%}\")\n\n# Access hierarchical aggregate metrics\ncm = result['confusion_matrix']\nfor field_name, field_data in cm['fields'].items():\n    if 'aggregate' in field_data:\n        agg = field_data['aggregate']\n        print(f\"{field_name} section F1: {agg['derived']['cm_f1']:.3f}\")\n\n# Access non-matches for debugging\nnon_matches = result.get('non_matches', [])\nfor nm in non_matches:\n    print(f\"Field {nm['field_path']} failed: {nm['non_match_type']}\")\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#aggregate-field-analysis","title":"Aggregate Field Analysis","text":"<pre><code># Analyze performance by data section\nresult = model1.compare_with(model2, include_confusion_matrix=True)\ncm = result['confusion_matrix']\n\n# Top-level summary\nprint(\"=== OVERALL PERFORMANCE ===\")\ntotal_agg = cm['aggregate']\nprint(f\"Total Precision: {total_agg['derived']['cm_precision']:.3f}\")\nprint(f\"Total Recall: {total_agg['derived']['cm_recall']:.3f}\")\nprint(f\"Total F1: {total_agg['derived']['cm_f1']:.3f}\")\n\n# Section-by-section analysis\nprint(\"\\n=== SECTION PERFORMANCE ===\")\nfor section_name, section_data in cm['fields'].items():\n    if 'aggregate' in section_data:\n        agg = section_data['aggregate']\n        f1 = agg['derived']['cm_f1']\n        tp = agg['tp']\n        total_errors = agg['fd'] + agg['fa'] + agg['fn']\n        print(f\"{section_name}: F1={f1:.3f}, TP={tp}, Errors={total_errors}\")\n</code></pre>"},{"location":"Guides/StructuredModel_compare_with_README/#conclusion","title":"Conclusion","text":"<p>The <code>compare_with</code> method represents a sophisticated document comparison system that balances accuracy, performance, and usability. It provides both simple similarity scores for basic use cases and detailed analytical data for advanced evaluation scenarios.</p> <p>The method's strength lies in its ability to handle complex, nested data structures while maintaining interpretable results that can guide both automated systems and human reviewers in understanding how well document processing systems are performing.</p>"},{"location":"Guides/Universal_Aggregate_Field_Feature/","title":"Universal Aggregate Field Feature","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#overview","title":"Overview","text":"<p>The Universal Aggregate Field feature provides automatic aggregation of confusion matrix metrics at every node in the comparison result tree. This feature replaces the previous field-level <code>aggregate=True</code> parameter with a universal approach that provides field-level granularity across all structured comparisons.</p>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#key-features","title":"Key Features","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#1-universal-coverage","title":"1. Universal Coverage","text":"<ul> <li>Automatic: Every node in the comparison result tree now includes an <code>aggregate</code> field</li> <li>No Configuration Required: Works automatically without any field-level configuration</li> <li>Consistent Structure: Provides uniform access pattern across all comparison results</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#2-correct-placement","title":"2. Correct Placement","text":"<ul> <li>Sibling Relationship: The <code>aggregate</code> field appears as a sibling of <code>overall</code> and <code>fields</code>, not nested within <code>overall</code></li> <li>Hierarchical Consistency: Available at every level of the tree structure</li> <li>Easy Access: Simple access pattern: <code>result['confusion_matrix']['aggregate']</code> or <code>result['confusion_matrix']['fields']['contact']['aggregate']</code></li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#3-comprehensive-metrics","title":"3. Comprehensive Metrics","text":"<ul> <li>Primitive Field Summation: Aggregates all primitive field confusion matrices below each node</li> <li>Derived Metrics Included: Each aggregate includes precision, recall, F1, and accuracy calculations</li> <li>Hierarchical Rollup: Parent nodes sum metrics from all child primitive fields</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#usage-examples","title":"Usage Examples","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#basic-usage","title":"Basic Usage","text":"<pre><code>from src.stickler.structured_object_evaluator.models.structured_model import StructuredModel\nfrom src.stickler.structured_object_evaluator.models.comparable_field import ComparableField\nfrom src.stickler.comparators.exact import ExactComparator\n\nclass Contact(StructuredModel):\n    phone: str = ComparableField(comparator=ExactComparator(), threshold=1.0)\n    email: str = ComparableField(comparator=ExactComparator(), threshold=1.0)\n\nclass Person(StructuredModel):\n    name: str = ComparableField(comparator=ExactComparator(), threshold=1.0)\n    contact: Contact = ComparableField(comparator=ExactComparator(), threshold=1.0)\n\n# Create test instances\ngt = Person(name=\"John\", contact=Contact(phone=\"123\", email=\"john@test.com\"))\npred = Person(name=\"John\", contact=Contact(phone=\"456\", email=\"john@test.com\"))\n\n# Compare with confusion matrix\nresult = gt.compare_with(pred, include_confusion_matrix=True)\ncm = result['confusion_matrix']\n\n# Access aggregate metrics at different levels\nprint(\"Top-level aggregate:\", cm['aggregate'])\nprint(\"Contact aggregate:\", cm['fields']['contact']['aggregate'])\nprint(\"Phone aggregate:\", cm['fields']['contact']['fields']['phone']['aggregate'])\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#output-structure","title":"Output Structure","text":"<pre><code>{\n  \"confusion_matrix\": {\n    \"overall\": {\n      \"tp\": 1, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0,\n      \"similarity_score\": 0.5,\n      \"all_fields_matched\": false,\n      \"derived\": { \"cm_precision\": 0.5, \"cm_recall\": 1.0, \"cm_f1\": 0.67, \"cm_accuracy\": 0.5 }\n    },\n    \"aggregate\": {\n      \"tp\": 2, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0,\n      \"derived\": { \"cm_precision\": 0.67, \"cm_recall\": 1.0, \"cm_f1\": 0.8, \"cm_accuracy\": 0.67 }\n    },\n    \"fields\": {\n      \"name\": {\n        \"overall\": { \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0 },\n        \"aggregate\": { \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0 },\n        \"fields\": {}\n      },\n      \"contact\": {\n        \"overall\": { \"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0 },\n        \"aggregate\": { \"tp\": 1, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0 },\n        \"fields\": {\n          \"phone\": {\n            \"overall\": { \"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0 },\n            \"aggregate\": { \"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0 }\n          },\n          \"email\": {\n            \"overall\": { \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0 },\n            \"aggregate\": { \"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0 }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#migration-from-legacy-aggregatetrue","title":"Migration from Legacy <code>aggregate=True</code>","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#deprecated-parameter","title":"Deprecated Parameter","text":"<p>The <code>aggregate=True</code> parameter in <code>ComparableField</code> is now deprecated and triggers a warning:</p> <pre><code># DEPRECATED - triggers warning\nfield = ComparableField(aggregate=True)\n\n# NEW - automatic universal aggregation\nfield = ComparableField()  # aggregate field automatically available\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#migration-steps","title":"Migration Steps","text":"<ol> <li>Remove <code>aggregate=True</code>: Remove all <code>aggregate=True</code> parameters from <code>ComparableField</code> definitions</li> <li>Update Access Patterns: Change from field-specific aggregate access to universal aggregate access</li> <li>Test Compatibility: Verify that existing code works with the new universal aggregate structure</li> </ol>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#before-legacy","title":"Before (Legacy)","text":"<pre><code>class MyModel(StructuredModel):\n    contact: Contact = ComparableField(aggregate=True)  # Only this field had aggregation\n\n# Access was field-specific\nif 'aggregate' in result['confusion_matrix']['fields']['contact']:\n    contact_aggregate = result['confusion_matrix']['fields']['contact']['aggregate']\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#after-universal","title":"After (Universal)","text":"<pre><code>class MyModel(StructuredModel):\n    contact: Contact = ComparableField()  # All fields automatically have aggregation\n\n# Access is universal and consistent\ncontact_aggregate = result['confusion_matrix']['fields']['contact']['aggregate']\ntop_aggregate = result['confusion_matrix']['aggregate']\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#technical-implementation","title":"Technical Implementation","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#calculation-logic","title":"Calculation Logic","text":"<ol> <li>Leaf Nodes: For primitive fields, <code>aggregate</code> equals <code>overall</code> metrics</li> <li>Parent Nodes: Sum all <code>aggregate</code> metrics from child fields</li> <li>Hierarchical Rollup: Each level aggregates metrics from all primitive fields below it</li> </ol>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Single Traversal: Aggregate calculation happens during the main comparison traversal</li> <li>Efficient Summation: Simple integer addition for each metric type</li> <li>Memory Overhead: Minimal additional memory for aggregate dictionaries</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#backward-compatibility","title":"Backward Compatibility","text":"<ul> <li>Existing Code: All existing code continues to work unchanged</li> <li>Deprecation Warning: Legacy <code>aggregate=True</code> usage shows deprecation warning</li> <li>Gradual Migration: Teams can migrate at their own pace</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#benefits","title":"Benefits","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#1-universal-field-level-granularity","title":"1. Universal Field-Level Granularity","text":"<ul> <li>Complete Coverage: Every field comparison now provides aggregate metrics</li> <li>Consistent Interface: Same access pattern regardless of field type or nesting level</li> <li>No Configuration: Works automatically without any setup</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#2-enhanced-analysis-capabilities","title":"2. Enhanced Analysis Capabilities","text":"<ul> <li>Drill-Down Analysis: Easily identify which parts of nested structures contribute to overall metrics</li> <li>Comparative Analysis: Compare aggregate metrics across different levels of the hierarchy</li> <li>Debugging Support: Quickly identify problematic areas in complex nested structures</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#3-improved-developer-experience","title":"3. Improved Developer Experience","text":"<ul> <li>Predictable Structure: Always know that <code>aggregate</code> field will be available</li> <li>Simplified Code: No need to check for aggregate field existence</li> <li>Better Tooling: IDE autocomplete and type checking work consistently</li> </ul>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#use-cases","title":"Use Cases","text":""},{"location":"Guides/Universal_Aggregate_Field_Feature/#1-complex-nested-structure-analysis","title":"1. Complex Nested Structure Analysis","text":"<pre><code># Analyze which parts of a complex invoice structure have issues\ninvoice_result = gt_invoice.compare_with(pred_invoice, include_confusion_matrix=True)\n\n# Check overall document accuracy\nprint(\"Document aggregate:\", invoice_result['confusion_matrix']['aggregate'])\n\n# Check line items accuracy\nprint(\"Line items aggregate:\", invoice_result['confusion_matrix']['fields']['line_items']['aggregate'])\n\n# Check individual product accuracy within line items\nfor i, item in enumerate(invoice_result['confusion_matrix']['fields']['line_items']['fields']):\n    print(f\"Item {i} aggregate:\", item['aggregate'])\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#2-model-performance-monitoring","title":"2. Model Performance Monitoring","text":"<pre><code># Monitor different aspects of model performance\nresults = []\nfor test_case in test_cases:\n    result = gt.compare_with(pred, include_confusion_matrix=True)\n    results.append({\n        'overall': result['confusion_matrix']['aggregate'],\n        'contact_info': result['confusion_matrix']['fields']['contact']['aggregate'],\n        'personal_info': result['confusion_matrix']['fields']['name']['aggregate']\n    })\n\n# Analyze trends across different field types\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#3-hierarchical-reporting","title":"3. Hierarchical Reporting","text":"<pre><code>def print_hierarchical_metrics(node, path=\"\"):\n    \"\"\"Print aggregate metrics for all levels of a comparison result\"\"\"\n    if 'aggregate' in node:\n        metrics = node['aggregate']\n        precision = metrics.get('derived', {}).get('cm_precision', 0)\n        recall = metrics.get('derived', {}).get('cm_recall', 0)\n        f1 = metrics.get('derived', {}).get('cm_f1', 0)\n\n        print(f\"{path}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n\n    if 'fields' in node:\n        for field_name, field_data in node['fields'].items():\n            new_path = f\"{path}.{field_name}\" if path else field_name\n            print_hierarchical_metrics(field_data, new_path)\n\n# Usage\nresult = gt.compare_with(pred, include_confusion_matrix=True)\nprint_hierarchical_metrics(result['confusion_matrix'])\n</code></pre>"},{"location":"Guides/Universal_Aggregate_Field_Feature/#conclusion","title":"Conclusion","text":"<p>The Universal Aggregate Field feature provides comprehensive field-level granularity for all structured model comparisons. By automatically including aggregate metrics at every node, it enables powerful analysis capabilities while maintaining backward compatibility and providing a consistent, predictable interface for developers.</p>"},{"location":"SDK-Docs/","title":"SDK Documentation","text":"<p>Complete API reference for the Stickler library.</p>"},{"location":"SDK-Docs/#sections","title":"Sections","text":"<ul> <li>API Reference - Core classes and functions</li> <li>Comparators - Comparison strategies</li> <li>Evaluator - Evaluation engines</li> <li>Models - Data models and fields</li> <li>Utils - Utility functions</li> </ul>"},{"location":"SDK-Docs/api/","title":"API Reference","text":""},{"location":"SDK-Docs/api/#stickler","title":"<code>stickler</code>","text":"<p>stickler: Structured object comparison and evaluation library.</p> <p>This library provides tools for comparing complex structured objects with configurable comparison strategies and detailed evaluation metrics.</p>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel","title":"<code>stickler.structured_object_evaluator.models.structured_model.StructuredModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for models with structured comparison capabilities.</p> <p>This class extends Pydantic's BaseModel with the ability to compare instances using configurable comparison metrics for each field. It supports: - Field-level comparison configuration - Nested model comparison - Integration with ANLS* comparators - JSON schema generation with comparison metadata - Unordered list comparison using Hungarian matching - Retention of extra fields not defined in the model</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>class StructuredModel(BaseModel):\n    \"\"\"Base class for models with structured comparison capabilities.\n\n    This class extends Pydantic's BaseModel with the ability to compare\n    instances using configurable comparison metrics for each field.\n    It supports:\n    - Field-level comparison configuration\n    - Nested model comparison\n    - Integration with ANLS* comparators\n    - JSON schema generation with comparison metadata\n    - Unordered list comparison using Hungarian matching\n    - Retention of extra fields not defined in the model\n    \"\"\"\n\n    # Default match threshold - can be overridden in subclasses\n    match_threshold: ClassVar[float] = 0.7\n\n    extra_fields: Dict[str, Any] = Field(default_factory=dict, exclude=True)\n\n    model_config = {\n        \"arbitrary_types_allowed\": True,\n        \"extra\": \"allow\",  # Allow extra fields to be stored in extra_fields\n    }\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Validate field configurations when a StructuredModel subclass is defined.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Validate field configurations using class annotations since model_fields isn't populated yet\n        if hasattr(cls, \"__annotations__\"):\n            for field_name, field_type in cls.__annotations__.items():\n                if field_name == \"extra_fields\":\n                    continue\n\n                # Get the field default value if it exists\n                field_default = getattr(cls, field_name, None)\n\n                # Since ComparableField is now always a function that returns a Field,\n                # we need to check if field_default has comparison metadata\n                if hasattr(field_default, \"json_schema_extra\") and callable(\n                    field_default.json_schema_extra\n                ):\n                    # Check for comparison metadata\n                    temp_schema = {}\n                    field_default.json_schema_extra(temp_schema)\n                    if \"x-comparison\" in temp_schema:\n                        # This field was created with ComparableField function - validate constraints\n                        if cls._is_list_of_structured_model_type(field_type):\n                            comparison_config = temp_schema[\"x-comparison\"]\n\n                            # Threshold validation - only flag if explicitly set to non-default value\n                            threshold = comparison_config.get(\"threshold\", 0.5)\n                            if threshold != 0.5:  # Default threshold value\n                                raise ValueError(\n                                    f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                    f\"'threshold' parameter in ComparableField. Hungarian matching uses each \"\n                                    f\"StructuredModel's 'match_threshold' class attribute instead. \"\n                                    f\"Set 'match_threshold = {threshold}' on the list element class.\"\n                                )\n\n                            # Comparator validation - only flag if explicitly set to non-default type\n                            comparator_type = comparison_config.get(\n                                \"comparator_type\", \"LevenshteinComparator\"\n                            )\n                            if (\n                                comparator_type != \"LevenshteinComparator\"\n                            ):  # Default comparator type\n                                raise ValueError(\n                                    f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                    f\"'comparator' parameter in ComparableField. Object comparison uses each \"\n                                    f\"StructuredModel's individual field comparators instead.\"\n                                )\n\n    @classmethod\n    def _is_list_of_structured_model_type(cls, field_type) -&gt; bool:\n        \"\"\"Check if a field type annotation represents List[StructuredModel].\n\n        Args:\n            field_type: The field type annotation\n\n        Returns:\n            True if the field is a List[StructuredModel] type\n        \"\"\"\n        # Handle direct imports and typing constructs\n        origin = get_origin(field_type)\n        if origin is list or origin is List:\n            args = get_args(field_type)\n            if args:\n                element_type = args[0]\n                # Check if element type is a StructuredModel subclass\n                try:\n                    return inspect.isclass(element_type) and issubclass(\n                        element_type, StructuredModel\n                    )\n                except (TypeError, AttributeError):\n                    return False\n\n        # Handle Union types (like Optional[List[StructuredModel]])\n        elif origin is Union:\n            args = get_args(field_type)\n            for arg in args:\n                if cls._is_list_of_structured_model_type(arg):\n                    return True\n\n        return False\n\n    @classmethod\n    def from_json(cls, json_data: Dict[str, Any]) -&gt; \"StructuredModel\":\n        \"\"\"Create a StructuredModel instance from JSON data.\n\n        This method handles missing fields gracefully and stores extra fields\n        in the extra_fields attribute.\n\n        Args:\n            json_data: Dictionary containing the JSON data\n\n        Returns:\n            StructuredModel instance created from the JSON data\n        \"\"\"\n        return ConfigurationHelper.from_json(cls, json_data)\n\n    @classmethod\n    def model_from_json(cls, config: Dict[str, Any]) -&gt; Type[\"StructuredModel\"]:\n        \"\"\"Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().\n\n        This method leverages Pydantic's native dynamic model creation capabilities to ensure\n        full compatibility with all Pydantic features while adding structured comparison\n        functionality through inherited StructuredModel methods.\n\n        The generated model inherits all StructuredModel capabilities:\n        - compare_with() method for detailed comparisons\n        - Field-level comparison configuration\n        - Hungarian algorithm for list matching\n        - Confusion matrix generation\n        - JSON schema with comparison metadata\n\n        Args:\n            config: JSON configuration with fields, comparators, and model settings.\n                   Required keys:\n                   - fields: Dict mapping field names to field configurations\n                   Optional keys:\n                   - model_name: Name for the generated class (default: \"DynamicModel\")\n                   - match_threshold: Overall matching threshold (default: 0.7)\n\n                   Field configuration format:\n                   {\n                       \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required\n                       \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional\n                       \"threshold\": 0.8,  # Optional, default 0.5\n                       \"weight\": 2.0,     # Optional, default 1.0\n                       \"required\": true,  # Optional, default false\n                       \"default\": \"value\", # Optional\n                       \"description\": \"Field description\",  # Optional\n                       \"alias\": \"field_alias\",  # Optional\n                       \"examples\": [\"example1\", \"example2\"]  # Optional\n                   }\n\n        Returns:\n            A fully functional StructuredModel subclass created with create_model()\n\n        Raises:\n            ValueError: If configuration is invalid or contains unsupported types/comparators\n            KeyError: If required configuration keys are missing\n\n        Examples:\n            &gt;&gt;&gt; config = {\n            ...     \"model_name\": \"Product\",\n            ...     \"match_threshold\": 0.8,\n            ...     \"fields\": {\n            ...         \"name\": {\n            ...             \"type\": \"str\",\n            ...             \"comparator\": \"LevenshteinComparator\",\n            ...             \"threshold\": 0.8,\n            ...             \"weight\": 2.0,\n            ...             \"required\": True\n            ...         },\n            ...         \"price\": {\n            ...             \"type\": \"float\",\n            ...             \"comparator\": \"NumericComparator\",\n            ...             \"default\": 0.0\n            ...         }\n            ...     }\n            ... }\n            &gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n            &gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\n            True\n            &gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n            &gt;&gt;&gt; product.name\n            'Widget'\n            &gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n            &gt;&gt;&gt; result[\"overall_score\"]\n            1.0\n        \"\"\"\n        from pydantic import create_model\n        from .field_converter import convert_fields_config, validate_fields_config\n\n        # Validate configuration structure\n        if not isinstance(config, dict):\n            raise ValueError(\"Configuration must be a dictionary\")\n\n        if \"fields\" not in config:\n            raise ValueError(\"Configuration must contain 'fields' key\")\n\n        fields_config = config[\"fields\"]\n        if not isinstance(fields_config, dict) or len(fields_config) == 0:\n            raise ValueError(\"'fields' must be a non-empty dictionary\")\n\n        # Validate all field configurations before proceeding (including nested schema validation)\n        try:\n            from .field_converter import get_global_converter\n\n            converter = get_global_converter()\n\n            # First validate basic field configurations\n            validate_fields_config(fields_config)\n\n            # Then validate nested schema rules\n            for field_name, field_config in fields_config.items():\n                converter.validate_nested_field_schema(field_name, field_config)\n\n        except ValueError as e:\n            raise ValueError(f\"Invalid field configuration: {e}\")\n\n        # Extract model configuration\n        model_name = config.get(\"model_name\", \"DynamicModel\")\n        match_threshold = config.get(\"match_threshold\", 0.7)\n\n        # Validate model name\n        if not isinstance(model_name, str) or not model_name.isidentifier():\n            raise ValueError(\n                f\"model_name must be a valid Python identifier, got: {model_name}\"\n            )\n\n        # Validate match threshold\n        if not isinstance(match_threshold, (int, float)) or not (\n            0.0 &lt;= match_threshold &lt;= 1.0\n        ):\n            raise ValueError(\n                f\"match_threshold must be a number between 0.0 and 1.0, got: {match_threshold}\"\n            )\n\n        # Convert field configurations to Pydantic field definitions\n        try:\n            field_definitions = convert_fields_config(fields_config)\n        except ValueError as e:\n            raise ValueError(f\"Error converting field configurations: {e}\")\n\n        # Create the dynamic model extending StructuredModel\n        try:\n            DynamicClass = create_model(\n                model_name,\n                __base__=cls,  # Extend StructuredModel\n                **field_definitions,\n            )\n        except Exception as e:\n            raise ValueError(f\"Error creating dynamic model: {e}\")\n\n        # Set class-level attributes\n        DynamicClass.match_threshold = match_threshold\n\n        # Add configuration metadata for debugging/introspection\n        DynamicClass._model_config = config\n\n        return DynamicClass\n\n    @classmethod\n    def _is_structured_field_type(cls, field_info) -&gt; bool:\n        \"\"\"Check if a field represents a structured type that needs special handling.\n\n        Args:\n            field_info: Pydantic field info object\n\n        Returns:\n            True if the field is a List[StructuredModel] or StructuredModel type\n        \"\"\"\n        return ConfigurationHelper.is_structured_field_type(field_info)\n\n    @classmethod\n    def _get_comparison_info(cls, field_name: str) -&gt; ComparableField:\n        \"\"\"Extract comparison info from a field.\n\n        Args:\n            field_name: Name of the field to get comparison info for\n\n        Returns:\n            ComparableField object with comparison configuration\n        \"\"\"\n        return ConfigurationHelper.get_comparison_info(cls, field_name)\n\n    # Remove legacy ComparableField handling since ComparableField is now always a function\n    # that returns proper Pydantic Fields\n    pass\n\n    # No special __init__ needed since ComparableField is now always a function\n    # that returns proper Pydantic Fields\n    pass\n\n    @classmethod\n    def _is_aggregate_field(cls, field_name: str) -&gt; bool:\n        \"\"\"Check if field is marked for confusion matrix aggregation.\n\n        Args:\n            field_name: Name of the field to check\n\n        Returns:\n            True if the field is marked for aggregation, False otherwise\n        \"\"\"\n        return ConfigurationHelper.is_aggregate_field(cls, field_name)\n\n    def _is_truly_null(self, val: Any) -&gt; bool:\n        \"\"\"Check if a value is truly null (None).\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None, False otherwise\n        \"\"\"\n        return val is None\n\n    def _should_use_hierarchical_structure(self, val: Any, field_name: str) -&gt; bool:\n        \"\"\"Check if a list value should maintain hierarchical structure.\n\n        For lists, we need to check if they should maintain hierarchical structure\n        based on their field type configuration.\n\n        Args:\n            val: Value to check (typically a list)\n            field_name: Name of the field being evaluated\n\n        Returns:\n            True if the value should use hierarchical structure, False otherwise\n        \"\"\"\n        if isinstance(val, list):\n            # Check if this field is configured as List[StructuredModel]\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                return True\n        return False\n\n    def _is_effectively_null_for_lists(self, val: Any) -&gt; bool:\n        \"\"\"Check if a list value is effectively null (None or empty list).\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None or an empty list, False otherwise\n        \"\"\"\n        return val is None or (isinstance(val, list) and len(val) == 0)\n\n    def _is_effectively_null_for_primitives(self, val: Any) -&gt; bool:\n        \"\"\"Check if a primitive value is effectively null.\n\n        Treats empty strings and None as equivalent for string fields.\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None or an empty string, False otherwise\n        \"\"\"\n        return val is None or (isinstance(val, str) and val == \"\")\n\n    def _is_list_field(self, field_name: str) -&gt; bool:\n        \"\"\"Check if a field is ANY list type.\n\n        Args:\n            field_name: Name of the field to check\n\n        Returns:\n            True if the field is a list type (List[str], List[StructuredModel], etc.)\n        \"\"\"\n        field_info = self.__class__.model_fields.get(field_name)\n        if not field_info:\n            return False\n\n        field_type = field_info.annotation\n        # Handle Optional types and direct List types\n        if hasattr(field_type, \"__origin__\"):\n            origin = field_type.__origin__\n            if origin is list or origin is List:\n                return True\n            elif origin is Union:  # Optional[List[...]] case\n                args = field_type.__args__\n                for arg in args:\n                    if hasattr(arg, \"__origin__\") and (\n                        arg.__origin__ is list or arg.__origin__ is List\n                    ):\n                        return True\n        return False\n\n    def _handle_list_field_dispatch(\n        self, gt_val: Any, pred_val: Any, weight: float\n    ) -&gt; dict:\n        \"\"\"Handle list field comparison using match statements.\n\n        Args:\n            gt_val: Ground truth list value\n            pred_val: Predicted list value\n            weight: Field weight for scoring\n\n        Returns:\n            Comparison result dictionary\n        \"\"\"\n        gt_effectively_null = self._is_effectively_null_for_lists(gt_val)\n        pred_effectively_null = self._is_effectively_null_for_lists(pred_val)\n\n        match (gt_effectively_null, pred_effectively_null):\n            case (True, True):\n                # Both None or empty lists \u2192 True Negative\n                return {\n                    \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                    \"fields\": {},\n                    \"raw_similarity_score\": 1.0,\n                    \"similarity_score\": 1.0,\n                    \"threshold_applied_score\": 1.0,\n                    \"weight\": weight,\n                }\n            case (True, False):\n                # GT=None/empty, Pred=populated list \u2192 False Alarm\n                pred_list = pred_val if isinstance(pred_val, list) else []\n                fa_count = (\n                    len(pred_list) if pred_list else 1\n                )  # At least 1 FA for the field itself\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": fa_count,\n                        \"fd\": 0,\n                        \"fp\": fa_count,\n                        \"tn\": 0,\n                        \"fn\": 0,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case (False, True):\n                # GT=populated list, Pred=None/empty \u2192 False Negative\n                gt_list = gt_val if isinstance(gt_val, list) else []\n                fn_count = (\n                    len(gt_list) if gt_list else 1\n                )  # At least 1 FN for the field itself\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": 0,\n                        \"fd\": 0,\n                        \"fp\": 0,\n                        \"tn\": 0,\n                        \"fn\": fn_count,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case _:\n                # Both non-null and non-empty, return None to continue processing\n                return None\n\n    def _create_true_negative_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a true negative result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            True negative result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n            \"fields\": {},\n            \"raw_similarity_score\": 1.0,\n            \"similarity_score\": 1.0,\n            \"threshold_applied_score\": 1.0,\n            \"weight\": weight,\n        }\n\n    def _create_false_alarm_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a false alarm result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            False alarm result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 1, \"fd\": 0, \"fp\": 1, \"tn\": 0, \"fn\": 0},\n            \"fields\": {},\n            \"raw_similarity_score\": 0.0,\n            \"similarity_score\": 0.0,\n            \"threshold_applied_score\": 0.0,\n            \"weight\": weight,\n        }\n\n    def _create_false_negative_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a false negative result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            False negative result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 1},\n            \"fields\": {},\n            \"raw_similarity_score\": 0.0,\n            \"similarity_score\": 0.0,\n            \"threshold_applied_score\": 0.0,\n            \"weight\": weight,\n        }\n\n    def _handle_struct_list_empty_cases(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        weight: float,\n    ) -&gt; dict:\n        \"\"\"Handle empty list cases with beautiful match statements.\n\n        Args:\n            gt_list: Ground truth list (may be None)\n            pred_list: Predicted list (may be None)\n            weight: Field weight for scoring\n\n        Returns:\n            Result dictionary if early exit needed, None if should continue processing\n        \"\"\"\n        # Normalize None to empty lists for consistent handling\n        gt_len = len(gt_list or [])\n        pred_len = len(pred_list or [])\n\n        match (gt_len, pred_len):\n            case (0, 0):\n                # Both empty lists \u2192 True Negative\n                return {\n                    \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                    \"fields\": {},\n                    \"raw_similarity_score\": 1.0,\n                    \"similarity_score\": 1.0,\n                    \"threshold_applied_score\": 1.0,\n                    \"weight\": weight,\n                }\n            case (0, pred_len):\n                # GT empty, pred has items \u2192 False Alarms\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": pred_len,\n                        \"fd\": 0,\n                        \"fp\": pred_len,\n                        \"tn\": 0,\n                        \"fn\": 0,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case (gt_len, 0):\n                # GT has items, pred empty \u2192 False Negatives\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": 0,\n                        \"fd\": 0,\n                        \"fp\": 0,\n                        \"tn\": 0,\n                        \"fn\": gt_len,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case _:\n                # Both non-empty, continue processing\n                return None\n\n    def _calculate_object_level_metrics(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        match_threshold: float,\n    ) -&gt; tuple:\n        \"\"\"Calculate object-level metrics using Hungarian matching.\n\n        Args:\n            gt_list: Ground truth list\n            pred_list: Predicted list\n            match_threshold: Threshold for considering objects as matches\n\n        Returns:\n            Tuple of (object_metrics_dict, matched_pairs, matched_gt_indices, matched_pred_indices)\n        \"\"\"\n        # Use Hungarian matching for OBJECT-LEVEL counts - OPTIMIZED: Single call gets all info\n        hungarian_helper = HungarianHelper()\n        hungarian_info = hungarian_helper.get_complete_matching_info(gt_list, pred_list)\n        matched_pairs = hungarian_info[\"matched_pairs\"]\n\n        # Count OBJECTS, not individual fields\n        tp_objects = 0  # Objects with similarity &gt;= match_threshold\n        fd_objects = 0  # Objects with similarity &lt; match_threshold\n        for gt_idx, pred_idx, similarity in matched_pairs:\n            if similarity &gt;= match_threshold:\n                tp_objects += 1\n            else:\n                fd_objects += 1\n\n        # Count unmatched objects\n        matched_gt_indices = {idx for idx, _, _ in matched_pairs}\n        matched_pred_indices = {idx for _, idx, _ in matched_pairs}\n        fn_objects = len(gt_list) - len(matched_gt_indices)  # Unmatched GT objects\n        fa_objects = len(pred_list) - len(\n            matched_pred_indices\n        )  # Unmatched pred objects\n\n        # Build list-level metrics counting OBJECTS (not fields)\n        object_level_metrics = {\n            \"tp\": tp_objects,\n            \"fa\": fa_objects,\n            \"fd\": fd_objects,\n            \"fp\": fa_objects + fd_objects,  # Total false positives\n            \"tn\": 0,  # No true negatives at object level for non-empty lists\n            \"fn\": fn_objects,\n        }\n\n        return (\n            object_level_metrics,\n            matched_pairs,\n            matched_gt_indices,\n            matched_pred_indices,\n        )\n\n    def _calculate_struct_list_similarity(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        info: \"ComparableField\",\n    ) -&gt; float:\n        \"\"\"Calculate raw similarity score for structured list.\n\n        Args:\n            gt_list: Ground truth list\n            pred_list: Predicted list\n            info: Field comparison info\n\n        Returns:\n            Raw similarity score between 0.0 and 1.0\n        \"\"\"\n        if len(pred_list) &gt; 0:\n            match_result = self._compare_unordered_lists(\n                gt_list, pred_list, info.comparator, info.threshold\n            )\n            return match_result.get(\"overall_score\", 0.0)\n        else:\n            return 0.0\n\n    # Necessary/sufficient field methods removed - no longer used\n\n    def _compare_unordered_lists(\n        self,\n        list1: List[Any],\n        list2: List[Any],\n        comparator: BaseComparator,\n        threshold: float,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compare two lists as unordered collections using Hungarian matching.\n\n        Args:\n            list1: First list\n            list2: Second list\n            comparator: Comparator to use for item comparison\n            threshold: Minimum score to consider a match\n\n        Returns:\n            Dictionary with confusion matrix metrics including:\n            - tp: True positives (matches &gt;= threshold)\n            - fd: False discoveries (matches &lt; threshold)\n            - fa: False alarms (unmatched prediction items)\n            - fn: False negatives (unmatched ground truth items)\n            - fp: Total false positives (fd + fa)\n            - overall_score: Similarity score for backward compatibility\n        \"\"\"\n        return ComparisonHelper.compare_unordered_lists(\n            list1, list2, comparator, threshold\n        )\n\n    def compare_field(self, field_name: str, other_value: Any) -&gt; float:\n        \"\"\"Compare a single field with a value using the configured comparator.\n\n        Args:\n            field_name: Name of the field to compare\n            other_value: Value to compare with\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        # Get our field value\n        my_value = getattr(self, field_name)\n\n        # If both values are StructuredModel instances, use recursive compare_with\n        if isinstance(my_value, StructuredModel) and isinstance(\n            other_value, StructuredModel\n        ):\n            # Use compare_with for rich comparison\n            comparison_result = my_value.compare_with(\n                other_value,\n                include_confusion_matrix=False,\n                document_non_matches=False,\n                evaluator_format=False,\n                recall_with_fd=False,\n            )\n            # Apply field-level threshold if configured\n            info = self._get_comparison_info(field_name)\n            raw_score = comparison_result[\"overall_score\"]\n            return (\n                raw_score\n                if raw_score &gt;= info.threshold or not info.clip_under_threshold\n                else 0.0\n            )\n\n        # CRITICAL FIX: For lists, don't clip under threshold for partial matches\n        if isinstance(my_value, list) and isinstance(other_value, list):\n            # Get field info\n            info = self._get_comparison_info(field_name)\n\n            # Use the raw comparison result without threshold clipping for lists\n            result = ComparisonHelper.compare_unordered_lists(\n                my_value, other_value, info.comparator, info.threshold\n            )\n\n            # Return the overall score directly (don't clip based on threshold for lists)\n            return result[\"overall_score\"]\n\n        # For other fields, use existing logic\n        return ComparisonHelper.compare_field_with_threshold(\n            self, field_name, other_value\n        )\n\n    def compare_field_raw(self, field_name: str, other_value: Any) -&gt; float:\n        \"\"\"Compare a single field with a value WITHOUT applying thresholds.\n\n        This version is used by the compare method to get raw similarity scores.\n\n        Args:\n            field_name: Name of the field to compare\n            other_value: Value to compare with\n\n        Returns:\n            Raw similarity score between 0.0 and 1.0 without threshold filtering\n        \"\"\"\n        # Get our field value\n        my_value = getattr(self, field_name)\n\n        # If both values are StructuredModel instances, use recursive compare_with\n        if isinstance(my_value, StructuredModel) and isinstance(\n            other_value, StructuredModel\n        ):\n            # Use compare_with for rich comparison, but extract the raw score\n            comparison_result = my_value.compare_with(\n                other_value,\n                include_confusion_matrix=False,\n                document_non_matches=False,\n                evaluator_format=False,\n                recall_with_fd=False,\n            )\n            return comparison_result[\"overall_score\"]\n\n        # For non-StructuredModel fields, use existing logic\n        return ComparisonHelper.compare_field_raw(self, field_name, other_value)\n\n    def compare_recursive(self, other: \"StructuredModel\") -&gt; dict:\n        \"\"\"The ONE clean recursive function that handles everything.\n\n        Enhanced to capture BOTH confusion matrix metrics AND similarity scores\n        in a single traversal to eliminate double traversal inefficiency.\n\n        Args:\n            other: Another instance of the same model to compare with\n\n        Returns:\n            Dictionary with clean hierarchical structure:\n            - overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched\n            - fields: Recursive structure for each field with scores\n            - non_matches: List of non-matching items\n        \"\"\"\n        result = {\n            \"overall\": {\n                \"tp\": 0,\n                \"fa\": 0,\n                \"fd\": 0,\n                \"fp\": 0,\n                \"tn\": 0,\n                \"fn\": 0,\n                \"similarity_score\": 0.0,\n                \"all_fields_matched\": False,\n            },\n            \"fields\": {},\n            \"non_matches\": [],\n        }\n\n        # Score percolation variables\n        total_score = 0.0\n        total_weight = 0.0\n        threshold_matched_fields = set()\n\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            gt_val = getattr(self, field_name)\n            pred_val = getattr(other, field_name, None)\n\n            # Enhanced dispatch returns both metrics AND scores\n            field_result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n\n            result[\"fields\"][field_name] = field_result\n\n            # Simple aggregation to overall metrics\n            self._aggregate_to_overall(field_result, result[\"overall\"])\n\n            # Score percolation - aggregate scores upward\n            if \"similarity_score\" in field_result and \"weight\" in field_result:\n                weight = field_result[\"weight\"]\n                threshold_applied_score = field_result[\"threshold_applied_score\"]\n                total_score += threshold_applied_score * weight\n                total_weight += weight\n\n                # Track threshold-matched fields\n                info = self._get_comparison_info(field_name)\n                if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n                    threshold_matched_fields.add(field_name)\n\n        # CRITICAL FIX: Handle hallucinated fields (extra fields) as False Alarms\n        extra_fields_fa = self._count_extra_fields_as_false_alarms(other)\n        result[\"overall\"][\"fa\"] += extra_fields_fa\n        result[\"overall\"][\"fp\"] += extra_fields_fa\n\n        # Calculate overall similarity score from percolated scores\n        if total_weight &gt; 0:\n            result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n\n        # Determine all_fields_matched\n        model_fields_for_comparison = set(self.__class__.model_fields.keys()) - {\n            \"extra_fields\"\n        }\n        result[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(\n            model_fields_for_comparison\n        )\n\n        return result\n\n    def _dispatch_field_comparison(\n        self, field_name: str, gt_val: Any, pred_val: Any\n    ) -&gt; dict:\n        \"\"\"Enhanced case-based dispatch using match statements for clean logic flow.\"\"\"\n\n        # Get field configuration for scoring\n        info = self._get_comparison_info(field_name)\n        weight = info.weight\n        threshold = info.threshold\n\n        # Check if this field is ANY list type (including Optional[List[str]], Optional[List[StructuredModel]], etc.)\n        is_list_field = self._is_list_field(field_name)\n\n        # Get null states and hierarchical needs\n        gt_is_null = self._is_truly_null(gt_val)\n        pred_is_null = self._is_truly_null(pred_val)\n        gt_needs_hierarchy = self._should_use_hierarchical_structure(gt_val, field_name)\n        pred_needs_hierarchy = self._should_use_hierarchical_structure(\n            pred_val, field_name\n        )\n\n        # Handle list fields with match statements\n        if is_list_field:\n            list_result = self._handle_list_field_dispatch(gt_val, pred_val, weight)\n            if list_result is not None:\n                return list_result\n            # If None returned, continue to regular type-based dispatch\n\n        # Handle non-hierarchical primitive null cases with match statements\n        if not (gt_needs_hierarchy or pred_needs_hierarchy):\n            gt_effectively_null_prim = self._is_effectively_null_for_primitives(gt_val)\n            pred_effectively_null_prim = self._is_effectively_null_for_primitives(\n                pred_val\n            )\n\n            match (gt_effectively_null_prim, pred_effectively_null_prim):\n                case (True, True):\n                    return self._create_true_negative_result(weight)\n                case (True, False):\n                    return self._create_false_alarm_result(weight)\n                case (False, True):\n                    return self._create_false_negative_result(weight)\n                case _:\n                    # Both non-null, continue to type-based dispatch\n                    pass\n\n        # Type-based dispatch\n        if isinstance(gt_val, (str, int, float)) and isinstance(\n            pred_val, (str, int, float)\n        ):\n            return self._compare_primitive_with_scores(gt_val, pred_val, field_name)\n        elif isinstance(gt_val, list) and isinstance(pred_val, list):\n            # Check if this should be structured list\n            if gt_val and isinstance(gt_val[0], StructuredModel):\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(gt_val, list) and len(gt_val) == 0:\n            # Handle empty GT list - check if it should be structured\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                # Empty structured list - should still return hierarchical structure\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(pred_val, list) and len(pred_val) == 0:\n            # Handle empty pred list - check if it should be structured\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                # Empty structured list - should still return hierarchical structure\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(gt_val, StructuredModel) and isinstance(\n            pred_val, StructuredModel\n        ):\n            # CRITICAL FIX: For StructuredModel fields, object-level metrics should be based on\n            # object similarity, not rollup of nested field metrics\n\n            # Get object-level similarity score\n            raw_score = gt_val.compare(pred_val)  # Overall object similarity\n\n            # Apply object-level binary classification based on threshold\n            if raw_score &gt;= threshold:\n                # Object matches threshold -&gt; True Positive\n                object_metrics = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n                threshold_applied_score = raw_score\n            else:\n                # Object below threshold -&gt; False Discovery\n                object_metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n                threshold_applied_score = (\n                    0.0 if info.clip_under_threshold else raw_score\n                )\n\n            # Still generate nested field details for debugging, but don't roll them up\n            nested_details = gt_val.compare_recursive(pred_val)[\"fields\"]\n\n            # Return structure with object-level metrics and nested field details kept separate\n            return {\n                \"overall\": {\n                    **object_metrics,\n                    \"similarity_score\": raw_score,\n                    \"all_fields_matched\": raw_score &gt;= threshold,\n                },\n                \"fields\": nested_details,  # Nested details available for debugging\n                \"raw_similarity_score\": raw_score,\n                \"similarity_score\": raw_score,\n                \"threshold_applied_score\": threshold_applied_score,\n                \"weight\": weight,\n                \"non_matches\": [],  # Add empty non_matches for consistency\n            }\n        else:\n            # Mismatched types\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0},\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n\n    def _compare_primitive_with_scores(\n        self, gt_val: Any, pred_val: Any, field_name: str\n    ) -&gt; dict:\n        \"\"\"Enhanced primitive comparison that returns both metrics AND scores.\"\"\"\n        info = self.__class__._get_comparison_info(field_name)\n        raw_similarity = info.comparator.compare(gt_val, pred_val)\n        weight = info.weight\n        threshold = info.threshold\n\n        # For binary classification metrics, always use threshold\n        if raw_similarity &gt;= threshold:\n            metrics = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n            threshold_applied_score = raw_similarity\n        else:\n            metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n            # For score calculation, respect clip_under_threshold setting\n            threshold_applied_score = (\n                0.0 if info.clip_under_threshold else raw_similarity\n            )\n\n        # UNIFIED STRUCTURE: Always use 'overall' for metrics\n        # 'fields' key omitted for primitive leaf nodes (semantic meaning: not a parent container)\n        return {\n            \"overall\": metrics,\n            \"raw_similarity_score\": raw_similarity,\n            \"similarity_score\": raw_similarity,\n            \"threshold_applied_score\": threshold_applied_score,\n            \"weight\": weight,\n        }\n\n    def _compare_primitive_list_with_scores(\n        self, gt_list: List[Any], pred_list: List[Any], field_name: str\n    ) -&gt; dict:\n        \"\"\"Enhanced primitive list comparison that returns both metrics AND scores with hierarchical structure.\n\n        DESIGN DECISION: Universal Hierarchical Structure\n        ===============================================\n        This method returns a hierarchical structure {\"overall\": {...}, \"fields\": {...}} even for\n        primitive lists (List[str], List[int], etc.) to maintain API consistency across all field types.\n\n        Why this approach:\n        - CONSISTENCY: All list fields use the same access pattern: cm[\"fields\"][name][\"overall\"]\n        - TEST COMPATIBILITY: Multiple test files expect this pattern for both primitive and structured lists\n        - PREDICTABLE API: Consumers don't need to check field type before accessing metrics\n\n        Trade-offs:\n        - Creates vestigial \"fields\": {} objects for primitive lists that will never be populated\n        - Slightly more verbose structure than necessary for leaf nodes\n        - Architecturally less pure than type-based structure (primitives flat, structured hierarchical)\n\n        Alternative considered but rejected:\n        - Type-based structure where List[primitive] \u2192 flat, List[StructuredModel] \u2192 hierarchical\n        - Would require updating multiple test files and consumer code to handle mixed access patterns\n        - More architecturally pure but breaks backward compatibility\n\n        Future consideration: If we ever refactor the entire confusion matrix API, we could move to\n        type-based structure where the presence of \"fields\" key indicates structured vs primitive.\n        \"\"\"\n        # Get field configuration\n        info = self.__class__._get_comparison_info(field_name)\n        weight = info.weight\n        threshold = info.threshold\n\n        # CRITICAL FIX: Handle None values before checking length\n        # Convert None to empty list for consistent handling\n        if gt_list is None:\n            gt_list = []\n        if pred_list is None:\n            pred_list = []\n\n        # Handle empty/null list cases first - FIXED: Empty lists should be TN=1\n        if len(gt_list) == 0 and len(pred_list) == 0:\n            # Both empty lists should be TN=1\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                \"fields\": {},  # Empty for primitive lists\n                \"raw_similarity_score\": 1.0,  # Perfect match\n                \"similarity_score\": 1.0,\n                \"threshold_applied_score\": 1.0,\n                \"weight\": weight,\n            }\n        elif len(gt_list) == 0:\n            # GT empty, pred has items \u2192 False Alarms\n            return {\n                \"overall\": {\n                    \"tp\": 0,\n                    \"fa\": len(pred_list),\n                    \"fd\": 0,\n                    \"fp\": len(pred_list),\n                    \"tn\": 0,\n                    \"fn\": 0,\n                },\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n        elif len(pred_list) == 0:\n            # GT has items, pred empty \u2192 False Negatives\n            return {\n                \"overall\": {\n                    \"tp\": 0,\n                    \"fa\": 0,\n                    \"fd\": 0,\n                    \"fp\": 0,\n                    \"tn\": 0,\n                    \"fn\": len(gt_list),\n                },\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n\n        # For primitive lists, use the comparison logic from _compare_unordered_lists\n        # which properly handles the threshold-based matching\n        comparator = info.comparator\n        match_result = self._compare_unordered_lists(\n            gt_list, pred_list, comparator, threshold\n        )\n\n        # Extract the counts from the match result\n        tp = match_result.get(\"tp\", 0)\n        fd = match_result.get(\"fd\", 0)\n        fa = match_result.get(\"fa\", 0)\n        fn = match_result.get(\"fn\", 0)\n\n        # Use the overall_score from the match result for raw similarity\n        raw_similarity = match_result.get(\"overall_score\", 0.0)\n\n        # CRITICAL FIX: For lists, we NEVER clip under threshold - partial matches are important\n        threshold_applied_score = raw_similarity  # Always use raw score for lists\n\n        # Return hierarchical structure expected by tests\n        return {\n            \"overall\": {\"tp\": tp, \"fa\": fa, \"fd\": fd, \"fp\": fa + fd, \"tn\": 0, \"fn\": fn},\n            \"fields\": {},  # Empty for primitive lists - no nested structure\n            \"raw_similarity_score\": raw_similarity,\n            \"similarity_score\": raw_similarity,\n            \"threshold_applied_score\": threshold_applied_score,\n            \"weight\": weight,\n        }\n\n    def _compare_struct_list_with_scores(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        field_name: str,\n    ) -&gt; dict:\n        \"\"\"Enhanced structural list comparison that returns both metrics AND scores.\n\n        PHASE 2: Delegates to StructuredListComparator while maintaining identical behavior.\n        \"\"\"\n        # Import here to avoid circular imports\n        from .structured_list_comparator import StructuredListComparator\n\n        # Create comparator and delegate\n        comparator = StructuredListComparator(self)\n        return comparator.compare_struct_list_with_scores(\n            gt_list, pred_list, field_name\n        )\n\n    def _count_extra_fields_as_false_alarms(self, other: \"StructuredModel\") -&gt; int:\n        \"\"\"Count hallucinated fields (extra fields) in the prediction as False Alarms.\n\n        Args:\n            other: The predicted StructuredModel instance to check for extra fields\n\n        Returns:\n            Number of hallucinated fields that should count as False Alarms\n        \"\"\"\n        fa_count = 0\n\n        # Check if the other model has extra fields (hallucinated content)\n        if hasattr(other, \"__pydantic_extra__\"):\n            # Count each extra field as one False Alarm\n            fa_count += len(other.__pydantic_extra__)\n\n        # Also recursively check nested StructuredModel objects for extra fields\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            gt_val = getattr(self, field_name, None)\n            pred_val = getattr(other, field_name, None)\n\n            # Check nested StructuredModel objects\n            if isinstance(gt_val, StructuredModel) and isinstance(\n                pred_val, StructuredModel\n            ):\n                fa_count += gt_val._count_extra_fields_as_false_alarms(pred_val)\n\n            # Check lists of StructuredModel objects\n            elif (\n                isinstance(gt_val, list)\n                and isinstance(pred_val, list)\n                and gt_val\n                and isinstance(gt_val[0], StructuredModel)\n                and pred_val\n                and isinstance(pred_val[0], StructuredModel)\n            ):\n                # For lists, we need to match them up properly using Hungarian matching - OPTIMIZED: Single call gets all info\n                # to avoid double-counting in cases where the list comparison already\n                # handles unmatched items as FA. For now, let's recursively check each item.\n                hungarian_helper = HungarianHelper()\n                hungarian_info = hungarian_helper.get_complete_matching_info(\n                    gt_val, pred_val\n                )\n                matched_pairs = hungarian_info[\"matched_pairs\"]\n\n                # Count extra fields in matched pairs\n                for gt_idx, pred_idx, similarity in matched_pairs:\n                    if gt_idx &lt; len(gt_val) and pred_idx &lt; len(pred_val):\n                        gt_item = gt_val[gt_idx]\n                        pred_item = pred_val[pred_idx]\n                        fa_count += gt_item._count_extra_fields_as_false_alarms(\n                            pred_item\n                        )\n\n                # For unmatched prediction items, count their extra fields too\n                matched_pred_indices = {pred_idx for _, pred_idx, _ in matched_pairs}\n                for pred_idx, pred_item in enumerate(pred_val):\n                    if pred_idx not in matched_pred_indices and isinstance(\n                        pred_item, StructuredModel\n                    ):\n                        # For unmatched items, we need a dummy GT to compare against\n                        if gt_val:  # Use first GT item as template\n                            dummy_gt = gt_val[0]\n                            fa_count += dummy_gt._count_extra_fields_as_false_alarms(\n                                pred_item\n                            )\n                        else:\n                            # If no GT items, count all extra fields in this pred item\n                            if hasattr(pred_item, \"__pydantic_extra__\"):\n                                fa_count += len(pred_item.__pydantic_extra__)\n\n        return fa_count\n\n    def _aggregate_to_overall(self, field_result: dict, overall: dict) -&gt; None:\n        \"\"\"Simple aggregation to overall metrics.\"\"\"\n        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n            if isinstance(field_result, dict):\n                if metric in field_result:\n                    overall[metric] += field_result[metric]\n                elif \"overall\" in field_result and metric in field_result[\"overall\"]:\n                    overall[metric] += field_result[\"overall\"][metric]\n\n    def _calculate_aggregate_metrics(self, result: dict) -&gt; dict:\n        \"\"\"Calculate aggregate metrics for all nodes in the result tree.\n\n        CRITICAL FIX: Enhanced deep nesting traversal to handle arbitrary nesting depth.\n        The aggregate field contains the sum of all primitive field confusion matrices\n        below that node in the tree. This provides universal field-level granularity.\n\n        Args:\n            result: Result from compare_recursive with hierarchical structure\n\n        Returns:\n            Modified result with 'aggregate' fields added at each level\n        \"\"\"\n        if not isinstance(result, dict):\n            return result\n\n        # Make a copy to avoid modifying the original\n        result_copy = result.copy()\n\n        # Calculate aggregate for this node\n        aggregate_metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n\n        # Recursively process 'fields' first to get child aggregates\n        if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n            fields_copy = {}\n            for field_name, field_result in result_copy[\"fields\"].items():\n                if isinstance(field_result, dict):\n                    # Recursively calculate aggregate for child field\n                    processed_field = self._calculate_aggregate_metrics(field_result)\n                    fields_copy[field_name] = processed_field\n\n                    # CRITICAL FIX: Sum child's aggregate metrics to parent\n                    if \"aggregate\" in processed_field and self._has_basic_metrics(\n                        processed_field[\"aggregate\"]\n                    ):\n                        child_aggregate = processed_field[\"aggregate\"]\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            aggregate_metrics[metric] += child_aggregate.get(metric, 0)\n                else:\n                    # Non-dict field - keep as is\n                    fields_copy[field_name] = field_result\n            result_copy[\"fields\"] = fields_copy\n\n        # CRITICAL FIX: Enhanced leaf node detection for deep nesting\n        # Handle both empty fields dict and missing fields key as leaf indicators\n        is_leaf_node = (\n            \"fields\" not in result_copy\n            or not result_copy[\"fields\"]\n            or (\n                isinstance(result_copy[\"fields\"], dict)\n                and len(result_copy[\"fields\"]) == 0\n            )\n        )\n\n        if is_leaf_node:\n            # Check if this is a leaf node with basic metrics (either in \"overall\" or directly)\n            if \"overall\" in result_copy and self._has_basic_metrics(\n                result_copy[\"overall\"]\n            ):\n                # Hierarchical leaf node: aggregate = overall metrics\n                overall = result_copy[\"overall\"]\n                for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                    aggregate_metrics[metric] = overall.get(metric, 0)\n            elif self._has_basic_metrics(result_copy):\n                # CRITICAL FIX: Legacy primitive leaf node - wrap in \"overall\" structure\n                # This preserves Universal Aggregate Field structure compliance\n                legacy_metrics = {}\n                for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                    legacy_metrics[metric] = result_copy.get(metric, 0)\n                    aggregate_metrics[metric] = result_copy.get(metric, 0)\n\n                # Wrap legacy structure in \"overall\" key to maintain consistency\n                if not \"overall\" in result_copy:\n                    # Move all basic metrics to \"overall\" key\n                    result_copy[\"overall\"] = legacy_metrics\n                    # Remove basic metrics from top level to avoid duplication\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        if metric in result_copy:\n                            del result_copy[metric]\n                    # Preserve other keys like derived, raw_similarity_score, etc.\n\n        # CRITICAL FIX: Always sum child field metrics if no child aggregates were found\n        # This handles the deep nesting case where leaf nodes have overall metrics but empty fields\n        if (\n            aggregate_metrics[\"tp\"] == 0\n            and aggregate_metrics[\"fa\"] == 0\n            and aggregate_metrics[\"fd\"] == 0\n            and aggregate_metrics[\"fp\"] == 0\n            and aggregate_metrics[\"tn\"] == 0\n            and aggregate_metrics[\"fn\"] == 0\n        ):\n            # Check if we have fields with overall metrics that we can sum\n            if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n                for field_name, field_result in result_copy[\"fields\"].items():\n                    if isinstance(field_result, dict):\n                        # ENHANCED: Check for both direct metrics and overall metrics\n                        if \"overall\" in field_result and self._has_basic_metrics(\n                            field_result[\"overall\"]\n                        ):\n                            field_overall = field_result[\"overall\"]\n                            for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                                aggregate_metrics[metric] += field_overall.get(\n                                    metric, 0\n                                )\n                        elif self._has_basic_metrics(field_result):\n                            # Direct metrics (legacy format)\n                            for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                                aggregate_metrics[metric] += field_result.get(metric, 0)\n\n        # Add aggregate as a sibling of 'overall' and 'fields'\n        result_copy[\"aggregate\"] = aggregate_metrics\n\n        return result_copy\n\n    def _add_derived_metrics_to_result(self, result: dict) -&gt; dict:\n        \"\"\"Walk through result and add 'derived' fields with F1, precision, recall, accuracy.\n\n        Args:\n            result: Result from compare_recursive with basic TP, FP, FN, etc. metrics\n\n        Returns:\n            Modified result with 'derived' fields added at each level\n        \"\"\"\n        if not isinstance(result, dict):\n            return result\n\n        # Make a copy to avoid modifying the original\n        result_copy = result.copy()\n\n        # Add derived metrics to 'overall' if it exists and has basic metrics\n        if \"overall\" in result_copy and isinstance(result_copy[\"overall\"], dict):\n            overall = result_copy[\"overall\"]\n            if self._has_basic_metrics(overall):\n                metrics_helper = MetricsHelper()\n                overall[\"derived\"] = metrics_helper.calculate_derived_metrics(overall)\n\n                # Also add derived metrics to aggregate if it exists\n                if \"aggregate\" in overall and self._has_basic_metrics(\n                    overall[\"aggregate\"]\n                ):\n                    overall[\"aggregate\"][\"derived\"] = (\n                        metrics_helper.calculate_derived_metrics(overall[\"aggregate\"])\n                    )\n\n        # Add derived metrics to top-level aggregate if it exists\n        if \"aggregate\" in result_copy and self._has_basic_metrics(\n            result_copy[\"aggregate\"]\n        ):\n            metrics_helper = MetricsHelper()\n            result_copy[\"aggregate\"][\"derived\"] = (\n                metrics_helper.calculate_derived_metrics(result_copy[\"aggregate\"])\n            )\n\n        # Recursively process 'fields' if it exists\n        if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n            fields_copy = {}\n            for field_name, field_result in result_copy[\"fields\"].items():\n                if isinstance(field_result, dict):\n                    # Check if this is a hierarchical field (has overall/fields) or a unified structure field\n                    if \"overall\" in field_result and \"fields\" in field_result:\n                        # Hierarchical field - process recursively\n                        fields_copy[field_name] = self._add_derived_metrics_to_result(\n                            field_result\n                        )\n                    elif \"overall\" in field_result and self._has_basic_metrics(\n                        field_result[\"overall\"]\n                    ):\n                        # Unified structure field - add derived metrics to overall\n                        field_copy = field_result.copy()\n                        metrics_helper = MetricsHelper()\n                        field_copy[\"overall\"][\"derived\"] = (\n                            metrics_helper.calculate_derived_metrics(\n                                field_result[\"overall\"]\n                            )\n                        )\n\n                        # Also add derived metrics to aggregate if it exists\n                        if \"aggregate\" in field_copy and self._has_basic_metrics(\n                            field_copy[\"aggregate\"]\n                        ):\n                            field_copy[\"aggregate\"][\"derived\"] = (\n                                metrics_helper.calculate_derived_metrics(\n                                    field_copy[\"aggregate\"]\n                                )\n                            )\n\n                        fields_copy[field_name] = field_copy\n                    elif self._has_basic_metrics(field_result):\n                        # CRITICAL FIX: Legacy leaf field with basic metrics - wrap in \"overall\" structure\n                        field_copy = field_result.copy()\n                        metrics_helper = MetricsHelper()\n\n                        # Extract basic metrics and wrap in \"overall\" structure\n                        legacy_metrics = {}\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            if metric in field_copy:\n                                legacy_metrics[metric] = field_copy[metric]\n                                del field_copy[metric]  # Remove from top level\n\n                        # Add derived metrics to the legacy metrics\n                        legacy_metrics[\"derived\"] = (\n                            metrics_helper.calculate_derived_metrics(legacy_metrics)\n                        )\n\n                        # Wrap in \"overall\" structure\n                        field_copy[\"overall\"] = legacy_metrics\n\n                        fields_copy[field_name] = field_copy\n                    else:\n                        # Other structure - keep as is\n                        fields_copy[field_name] = field_result\n                else:\n                    # Non-dict field - keep as is\n                    fields_copy[field_name] = field_result\n            result_copy[\"fields\"] = fields_copy\n\n        return result_copy\n\n    def _has_basic_metrics(self, metrics_dict: dict) -&gt; bool:\n        \"\"\"Check if a dictionary has basic confusion matrix metrics.\n\n        Args:\n            metrics_dict: Dictionary to check\n\n        Returns:\n            True if it has the basic metrics (tp, fp, fn, etc.)\n        \"\"\"\n        basic_metrics = [\"tp\", \"fp\", \"fn\", \"tn\", \"fa\", \"fd\"]\n        return all(metric in metrics_dict for metric in basic_metrics)\n\n    def _classify_field_for_confusion_matrix(\n        self, field_name: str, other_value: Any, threshold: float = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Classify a field comparison according to the confusion matrix rules.\n\n        Args:\n            field_name: Name of the field being compared\n            other_value: Value to compare with\n            threshold: Threshold for matching (uses field's threshold if None)\n\n        Returns:\n            Dictionary with TP, FP, TN, FN, FD counts and derived metrics\n        \"\"\"\n        # Get field values\n        gt_value = getattr(self, field_name)\n        pred_value = other_value\n\n        # Get field configuration\n        info = self.__class__._get_comparison_info(field_name)\n        if threshold is None:\n            threshold = info.threshold\n        comparator = info.comparator\n\n        # Determine if values are null\n        gt_is_null = FieldHelper.is_null_value(gt_value)\n        pred_is_null = FieldHelper.is_null_value(pred_value)\n\n        # Calculate similarity if both aren't null\n        similarity = None\n        if not gt_is_null and not pred_is_null:\n            if isinstance(gt_value, StructuredModel) and isinstance(\n                pred_value, StructuredModel\n            ):\n                comparison = gt_value.compare_with(pred_value)\n                similarity = comparison[\"overall_score\"]\n            else:\n                # Use the field's configured comparator for primitive comparison\n                similarity = comparator.compare(gt_value, pred_value)\n            values_match = similarity &gt;= threshold\n        else:\n            values_match = False\n\n        # Apply confusion matrix classification\n        if gt_is_null and pred_is_null:\n            # TN: Both null\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0}\n        elif gt_is_null and not pred_is_null:\n            # FA: GT null, prediction non-null (False Alarm)\n            result = {\"tp\": 0, \"fa\": 1, \"fd\": 0, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n        elif not gt_is_null and pred_is_null:\n            # FN: GT non-null, prediction null\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 1}\n        elif values_match:\n            # TP: Both non-null and match\n            result = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n        else:\n            # FD: Both non-null but don't match (False Discovery)\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n\n        # Add derived metrics\n        metrics_helper = MetricsHelper()\n        result[\"derived\"] = metrics_helper.calculate_derived_metrics(result)\n        # Don't include similarity_score in the result as tests don't expect it\n\n        return result\n\n    def _calculate_list_confusion_matrix(\n        self, field_name: str, other_list: List[Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Calculate confusion matrix for a list field, including nested field metrics.\n\n        Args:\n            field_name: Name of the list field being compared\n            other_list: Predicted list to compare with\n\n        Returns:\n            Dictionary with:\n            - Top-level TP, FP, TN, FN, FD, FA counts and derived metrics for the list field\n            - nested_fields: Dict with metrics for individual fields within list items (e.g., \"transactions.date\")\n            - non_matches: List of individual object-level non-matches for detailed analysis\n        \"\"\"\n        gt_list = getattr(self, field_name)\n        pred_list = other_list\n\n        # Initialize result structure\n        result = {\n            \"tp\": 0,\n            \"fa\": 0,\n            \"fd\": 0,\n            \"fp\": 0,\n            \"tn\": 0,\n            \"fn\": 0,\n            \"nested_fields\": {},  # Store nested field metrics here\n            \"non_matches\": [],  # Store individual object-level non-matches here\n        }\n\n        # Handle null cases first\n        if FieldHelper.is_null_value(gt_list) and FieldHelper.is_null_value(pred_list):\n            result.update({\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0})\n        elif FieldHelper.is_null_value(gt_list):\n            result.update(\n                {\n                    \"tp\": 0,\n                    \"fa\": len(pred_list),\n                    \"fd\": 0,\n                    \"fp\": len(pred_list),\n                    \"tn\": 0,\n                    \"fn\": 0,\n                }\n            )\n            # Add non-matches for each FA item using NonMatchesHelper\n            non_matches_helper = NonMatchesHelper()\n            result[\"non_matches\"] = non_matches_helper.add_non_matches_for_null_cases(\n                field_name, gt_list, pred_list\n            )\n        elif FieldHelper.is_null_value(pred_list):\n            result.update(\n                {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": len(gt_list)}\n            )\n            # Add non-matches for each FN item using NonMatchesHelper\n            non_matches_helper = NonMatchesHelper()\n            result[\"non_matches\"] = non_matches_helper.add_non_matches_for_null_cases(\n                field_name, gt_list, pred_list\n            )\n        else:\n            # Use existing comparison logic for list-level metrics\n            info = self.__class__._get_comparison_info(field_name)\n            comparator = info.comparator\n            threshold = info.threshold\n\n            # Reuse existing Hungarian matching logic\n            match_result = self._compare_unordered_lists(\n                gt_list, pred_list, comparator, threshold\n            )\n\n            # Use the detailed confusion matrix results directly from Hungarian matcher\n            result.update(\n                {\n                    \"tp\": match_result[\"tp\"],\n                    \"fa\": match_result[\n                        \"fa\"\n                    ],  # False alarms (unmatched prediction items)\n                    \"fd\": match_result[\n                        \"fd\"\n                    ],  # False discoveries (matches below threshold)\n                    \"fp\": match_result[\"fp\"],  # Total false positives (fa + fd)\n                    \"tn\": 0,\n                    \"fn\": match_result[\"fn\"],  # False negatives (unmatched GT items)\n                }\n            )\n\n            # Collect individual object-level non-matches using NonMatchesHelper\n            if gt_list and isinstance(gt_list[0], StructuredModel):\n                non_matches_helper = NonMatchesHelper()\n                non_matches = non_matches_helper.collect_list_non_matches(\n                    field_name, gt_list, pred_list\n                )\n                result[\"non_matches\"] = non_matches\n\n            # If list contains StructuredModel objects, calculate nested field metrics\n            if gt_list and isinstance(gt_list[0], StructuredModel):\n                nested_metrics = self._calculate_nested_field_metrics(\n                    field_name, gt_list, pred_list, threshold\n                )\n                result[\"nested_fields\"] = nested_metrics\n\n        # For List[StructuredModel], we should NOT aggregate nested fields to list level\n        # List level metrics represent object-level matches from Hungarian algorithm\n        # Nested field metrics represent field-level matches within those objects\n        # They are separate concerns and should not be aggregated\n\n        # Only aggregate if this is explicitly marked as an aggregate field AND it's not a list\n        is_aggregate = self.__class__._is_aggregate_field(field_name)\n        if is_aggregate and not isinstance(gt_list, list):\n            # Initialize top-level confusion matrix values to 0\n            result[\"tp\"] = 0\n            result[\"fa\"] = 0\n            result[\"fd\"] = 0\n            result[\"fp\"] = 0\n            result[\"tn\"] = 0\n            result[\"fn\"] = 0\n            # Sum up the confusion matrix values from nested fields\n            for field, field_metrics in result[\"nested_fields\"].items():\n                result[\"tp\"] += field_metrics[\"tp\"]\n                result[\"fa\"] += field_metrics[\"fa\"]\n                result[\"fd\"] += field_metrics[\"fd\"]\n                result[\"fp\"] += field_metrics[\"fp\"]\n                result[\"tn\"] += field_metrics[\"tn\"]\n                result[\"fn\"] += field_metrics[\"fn\"]\n\n        # Add derived metrics\n        metrics_helper = MetricsHelper()\n        result[\"derived\"] = metrics_helper.calculate_derived_metrics(result)\n\n        return result\n\n    def _calculate_nested_field_metrics(\n        self,\n        list_field_name: str,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        threshold: float,\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Calculate confusion matrix metrics for individual fields within list items.\n\n        THRESHOLD-GATED RECURSION: Only perform recursive field analysis for object pairs\n        with similarity &gt;= StructuredModel.match_threshold. Poor matches and unmatched\n        items are treated as atomic units.\n\n        Args:\n            list_field_name: Name of the parent list field (e.g., \"transactions\")\n            gt_list: Ground truth list of StructuredModel objects\n            pred_list: Predicted list of StructuredModel objects\n            threshold: Matching threshold (not used for threshold-gating)\n\n        Returns:\n            Dictionary mapping nested field paths to their confusion matrix metrics\n            E.g., {\"transactions.date\": {...}, \"transactions.description\": {...}}\n        \"\"\"\n        nested_metrics = {}\n\n        if not gt_list or not isinstance(gt_list[0], StructuredModel):\n            return nested_metrics\n\n        # Get the model class from the first item\n        model_class = gt_list[0].__class__\n\n        # CRITICAL FIX: Use field's threshold, not class's match_threshold\n        # Get the field info from the parent object to use the correct threshold\n        parent_field_info = self.__class__._get_comparison_info(list_field_name)\n        match_threshold = parent_field_info.threshold\n\n        # For each field in the nested model\n        for field_name in model_class.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            nested_field_path = f\"{list_field_name}.{field_name}\"\n\n            # Initialize aggregated counts for this nested field\n            total_tp = total_fa = total_fd = total_fp = total_tn = total_fn = 0\n\n            # Use HungarianHelper for Hungarian matching operations - OPTIMIZED: Single call gets all info\n            hungarian_helper = HungarianHelper()\n\n            # Use HungarianHelper to get optimal assignments with similarity scores\n            assignments = []\n            matched_pairs_with_scores = []\n            if gt_list and pred_list:\n                hungarian_info = hungarian_helper.get_complete_matching_info(\n                    gt_list, pred_list\n                )\n                matched_pairs_with_scores = hungarian_info[\"matched_pairs\"]\n                # Extract (gt_idx, pred_idx) pairs from the matched_pairs\n                assignments = [(i, j) for i, j, score in matched_pairs_with_scores]\n\n            # THRESHOLD-GATED RECURSION: Only process pairs that meet the match_threshold\n            for gt_idx, pred_idx, similarity_score in matched_pairs_with_scores:\n                if gt_idx &lt; len(gt_list) and pred_idx &lt; len(pred_list):\n                    gt_item = gt_list[gt_idx]\n                    pred_item = pred_list[pred_idx]\n\n                    # Handle floating point precision issues\n                    is_above_threshold = (\n                        similarity_score &gt;= match_threshold\n                        or abs(similarity_score - match_threshold) &lt; 1e-10\n                    )\n\n                    # Only perform recursive field analysis if similarity meets threshold\n                    if is_above_threshold:\n                        # Get field values\n                        gt_value = getattr(gt_item, field_name, None)\n                        pred_value = getattr(pred_item, field_name, None)\n\n                        # Check if this field is a List[StructuredModel] that needs recursive processing\n                        if (\n                            isinstance(gt_value, list)\n                            and isinstance(pred_value, list)\n                            and gt_value\n                            and isinstance(gt_value[0], StructuredModel)\n                        ):\n                            # Handle List[StructuredModel] recursively\n                            list_classification = (\n                                gt_item._calculate_list_confusion_matrix(\n                                    field_name, pred_value\n                                )\n                            )\n\n                            # Aggregate the list-level counts\n                            total_tp += list_classification[\"tp\"]\n                            total_fa += list_classification[\"fa\"]\n                            total_fd += list_classification[\"fd\"]\n                            total_fp += list_classification[\"fp\"]\n                            total_tn += list_classification[\"tn\"]\n                            total_fn += list_classification[\"fn\"]\n\n                            # IMPORTANT: Also collect the deeper nested field metrics\n                            if \"nested_fields\" in list_classification:\n                                for (\n                                    deeper_field_path,\n                                    deeper_metrics,\n                                ) in list_classification[\"nested_fields\"].items():\n                                    # Create the full path: e.g., \"products.attributes.name\"\n                                    full_deeper_path = (\n                                        f\"{list_field_name}.{deeper_field_path}\"\n                                    )\n\n                                    # Initialize or aggregate into the deeper nested metrics\n                                    if full_deeper_path not in nested_metrics:\n                                        nested_metrics[full_deeper_path] = {\n                                            \"tp\": 0,\n                                            \"fa\": 0,\n                                            \"fd\": 0,\n                                            \"fp\": 0,\n                                            \"tn\": 0,\n                                            \"fn\": 0,\n                                        }\n\n                                    nested_metrics[full_deeper_path][\"tp\"] += (\n                                        deeper_metrics[\"tp\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fa\"] += (\n                                        deeper_metrics[\"fa\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fd\"] += (\n                                        deeper_metrics[\"fd\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fp\"] += (\n                                        deeper_metrics[\"fp\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"tn\"] += (\n                                        deeper_metrics[\"tn\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fn\"] += (\n                                        deeper_metrics[\"fn\"]\n                                    )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            field_classification = (\n                                gt_item._classify_field_for_confusion_matrix(\n                                    field_name,\n                                    pred_value,\n                                    None,  # Use field's own threshold\n                                )\n                            )\n\n                            # Aggregate counts\n                            total_tp += field_classification[\"tp\"]\n                            total_fa += field_classification[\"fa\"]\n                            total_fd += field_classification[\"fd\"]\n                            total_fp += field_classification[\"fp\"]\n                            total_tn += field_classification[\"tn\"]\n                            total_fn += field_classification[\"fn\"]\n                    else:\n                        # Skip recursive analysis for pairs below threshold\n                        # These will be handled as FD at the object level\n                        pass\n\n            # Handle unmatched ground truth items (false negatives)\n            matched_gt_indices = set(idx for idx, _ in assignments)\n            for gt_idx, gt_item in enumerate(gt_list):\n                if gt_idx not in matched_gt_indices:\n                    gt_value = getattr(gt_item, field_name, None)\n                    if not FieldHelper.is_null_value(gt_value):\n                        # Check if this is a List[StructuredModel] that needs deeper processing for FN\n                        if (\n                            isinstance(gt_value, list)\n                            and gt_value\n                            and isinstance(gt_value[0], StructuredModel)\n                        ):\n                            # For List[StructuredModel], count each item in the list as a separate FN\n                            # and handle deeper nested fields\n                            total_fn += len(gt_value)  # Each list item is a separate FN\n\n                            # Also handle deeper nested fields for unmatched items\n                            dummy_empty_list = []  # Empty list for comparison\n                            list_classification = (\n                                gt_item._calculate_list_confusion_matrix(\n                                    field_name, dummy_empty_list\n                                )\n                            )\n                            if \"nested_fields\" in list_classification:\n                                for (\n                                    deeper_field_path,\n                                    deeper_metrics,\n                                ) in list_classification[\"nested_fields\"].items():\n                                    full_deeper_path = (\n                                        f\"{list_field_name}.{deeper_field_path}\"\n                                    )\n                                    if full_deeper_path not in nested_metrics:\n                                        nested_metrics[full_deeper_path] = {\n                                            \"tp\": 0,\n                                            \"fa\": 0,\n                                            \"fd\": 0,\n                                            \"fp\": 0,\n                                            \"tn\": 0,\n                                            \"fn\": 0,\n                                        }\n                                    nested_metrics[full_deeper_path][\"fn\"] += (\n                                        deeper_metrics[\"fn\"]\n                                    )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            total_fn += 1\n\n            # Handle unmatched prediction items (false alarms)\n            matched_pred_indices = set(idx for _, idx in assignments)\n            for pred_idx, pred_item in enumerate(pred_list):\n                if pred_idx not in matched_pred_indices:\n                    pred_value = getattr(pred_item, field_name, None)\n                    if not FieldHelper.is_null_value(pred_value):\n                        # Check if this is a List[StructuredModel] that needs deeper processing for FA\n                        if (\n                            isinstance(pred_value, list)\n                            and pred_value\n                            and isinstance(pred_value[0], StructuredModel)\n                        ):\n                            # For List[StructuredModel], count each item in the list as a separate FA\n                            # and handle deeper nested fields\n                            total_fa += len(\n                                pred_value\n                            )  # Each list item is a separate FA\n                            total_fp += len(\n                                pred_value\n                            )  # Each list item is also a separate FP\n\n                            # Also handle deeper nested fields for unmatched items\n                            dummy_empty_list = []  # Empty list for comparison\n                            # We need to create a dummy GT item for comparison to get the structure\n                            if gt_list:  # Use structure from an existing GT item\n                                dummy_gt_item = gt_list[0]\n                                list_classification = (\n                                    dummy_gt_item._calculate_list_confusion_matrix(\n                                        field_name, pred_value\n                                    )\n                                )\n                                if \"nested_fields\" in list_classification:\n                                    for (\n                                        deeper_field_path,\n                                        deeper_metrics,\n                                    ) in list_classification[\"nested_fields\"].items():\n                                        full_deeper_path = (\n                                            f\"{list_field_name}.{deeper_field_path}\"\n                                        )\n                                        if full_deeper_path not in nested_metrics:\n                                            nested_metrics[full_deeper_path] = {\n                                                \"tp\": 0,\n                                                \"fa\": 0,\n                                                \"fd\": 0,\n                                                \"fp\": 0,\n                                                \"tn\": 0,\n                                                \"fn\": 0,\n                                            }\n                                        nested_metrics[full_deeper_path][\"fa\"] += (\n                                            deeper_metrics[\"fa\"]\n                                        )\n                                        nested_metrics[full_deeper_path][\"fp\"] += (\n                                            deeper_metrics[\"fp\"]\n                                        )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            total_fa += 1\n                            total_fp += 1\n\n            # Store the aggregated metrics for this nested field\n            nested_metrics[nested_field_path] = {\n                \"tp\": total_tp,\n                \"fa\": total_fa,\n                \"fd\": total_fd,\n                \"fp\": total_fp,\n                \"tn\": total_tn,\n                \"fn\": total_fn,\n                \"derived\": MetricsHelper().calculate_derived_metrics(\n                    {\n                        \"tp\": total_tp,\n                        \"fa\": total_fa,\n                        \"fd\": total_fd,\n                        \"fp\": total_fp,\n                        \"tn\": total_tn,\n                        \"fn\": total_fn,\n                    }\n                ),\n            }\n\n        # Add derived metrics for all deeper nested fields that were collected\n        for deeper_path, deeper_metrics in nested_metrics.items():\n            if deeper_path != nested_field_path and \"derived\" not in deeper_metrics:\n                deeper_metrics[\"derived\"] = MetricsHelper().calculate_derived_metrics(\n                    {\n                        \"tp\": deeper_metrics[\"tp\"],\n                        \"fa\": deeper_metrics[\"fa\"],\n                        \"fd\": deeper_metrics[\"fd\"],\n                        \"fp\": deeper_metrics[\"fp\"],\n                        \"tn\": deeper_metrics[\"tn\"],\n                        \"fn\": deeper_metrics[\"fn\"],\n                    }\n                )\n\n        return nested_metrics\n\n    def _calculate_single_nested_field_metrics(\n        self,\n        parent_field_name: str,\n        gt_nested: \"StructuredModel\",\n        pred_nested: \"StructuredModel\",\n        parent_is_aggregate: bool = False,\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Calculate confusion matrix metrics for fields within a single nested StructuredModel.\n\n        Args:\n            parent_field_name: Name of the parent field (e.g., \"address\")\n            gt_nested: Ground truth nested StructuredModel\n            pred_nested: Predicted nested StructuredModel\n            parent_is_aggregate: Whether the parent field should aggregate child metrics\n\n        Returns:\n            Dictionary mapping nested field paths to their confusion matrix metrics\n            E.g., {\"address.street\": {...}, \"address.city\": {...}}\n        \"\"\"\n        nested_metrics = {}\n\n        if not isinstance(gt_nested, StructuredModel) or not isinstance(\n            pred_nested, StructuredModel\n        ):\n            # Handle case where one of the fields is a list of StructuredModel objects\n            if (\n                not isinstance(gt_nested, list)\n                or not gt_nested\n                or not isinstance(gt_nested[0], StructuredModel)\n            ):\n                return nested_metrics\n            return nested_metrics\n\n        # Initialize aggregation metrics for parent field if it's an aggregated field\n        parent_metrics = (\n            {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n            if parent_is_aggregate\n            else None\n        )\n\n        # Track which fields are aggregate fields themselves to avoid double counting\n        child_aggregate_fields = set()\n\n        # For each field in the nested model\n        for field_name in gt_nested.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            nested_field_path = f\"{parent_field_name}.{field_name}\"\n\n            # Check if this nested field is itself an aggregate field\n            is_child_aggregate = False\n            if hasattr(gt_nested.__class__, \"_is_aggregate_field\"):\n                is_child_aggregate = gt_nested.__class__._is_aggregate_field(field_name)\n                if is_child_aggregate:\n                    child_aggregate_fields.add(field_name)\n\n            # Get the field value from the prediction\n            pred_value = getattr(pred_nested, field_name, None)\n            gt_value = getattr(gt_nested, field_name)\n\n            # Handle lists of StructuredModel objects\n            if (\n                isinstance(gt_value, list)\n                and isinstance(pred_value, list)\n                and gt_value\n                and isinstance(gt_value[0], StructuredModel)\n            ):\n                # Use the list comparison logic for lists of StructuredModel objects\n                list_metrics = gt_nested._calculate_list_confusion_matrix(\n                    field_name, pred_value\n                )\n\n                # Store the metrics for this nested field\n                nested_metrics[nested_field_path] = {\n                    key: value\n                    for key, value in list_metrics.items()\n                    if key != \"nested_fields\"\n                }\n\n                # Add nested field metrics if available\n                if \"nested_fields\" in list_metrics:\n                    for sub_field, sub_metrics in list_metrics[\"nested_fields\"].items():\n                        full_path = f\"{nested_field_path}.{sub_field.split('.')[-1]}\"\n                        nested_metrics[full_path] = sub_metrics\n            else:\n                # Classify this field comparison\n                field_classification = gt_nested._classify_field_for_confusion_matrix(\n                    field_name, pred_value\n                )\n\n                # Store the metrics for this nested field\n                nested_metrics[nested_field_path] = field_classification\n\n                # Recursively calculate metrics for deeper nesting\n                deeper_metrics = self._calculate_single_nested_field_metrics(\n                    nested_field_path, gt_value, pred_value, is_child_aggregate\n                )\n                nested_metrics.update(deeper_metrics)\n\n                # If this is an aggregate child field, we need to use its aggregated metrics\n                # instead of the direct field comparison metrics\n                if is_child_aggregate and nested_field_path in deeper_metrics:\n                    # For an aggregate child field, we replace its direct metrics with\n                    # the aggregation of its children's metrics\n                    nested_metrics[nested_field_path] = deeper_metrics[\n                        nested_field_path\n                    ]\n\n            # For parent aggregation, we need to be careful not to double count metrics\n            if parent_is_aggregate:\n                if is_child_aggregate:\n                    # If child is an aggregate, use its aggregated metrics for parent\n                    if nested_field_path in deeper_metrics:\n                        child_agg_metrics = deeper_metrics[nested_field_path]\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            parent_metrics[metric] += child_agg_metrics.get(metric, 0)\n                else:\n                    # If child is not an aggregate, use its direct field metrics\n                    field_metrics = nested_metrics[nested_field_path]\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        parent_metrics[metric] += field_metrics.get(metric, 0)\n\n        # If parent is an aggregated field, add the aggregated metrics to the result\n        if parent_is_aggregate:\n            # Don't include metrics from child aggregate fields in the parent's metrics\n            # as they've already been counted through their own aggregation\n            for field_name in child_aggregate_fields:\n                nested_field_path = f\"{parent_field_name}.{field_name}\"\n                if nested_field_path in nested_metrics:\n                    # Don't double count these metrics in the parent\n                    field_metrics = nested_metrics[nested_field_path]\n                    # Subtract these metrics from parent_metrics to avoid double counting\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        parent_metrics[metric] -= field_metrics.get(metric, 0)\n\n            nested_metrics[parent_field_name] = parent_metrics\n            # Add derived metrics\n            nested_metrics[parent_field_name][\"derived\"] = (\n                MetricsHelper().calculate_derived_metrics(parent_metrics)\n            )\n\n        return nested_metrics\n\n    def _collect_enhanced_non_matches(\n        self, recursive_result: dict, other: \"StructuredModel\"\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Collect enhanced non-matches with object-level granularity.\n\n        Args:\n            recursive_result: Result from compare_recursive containing field comparison details\n            other: The predicted StructuredModel instance\n\n        Returns:\n            List of non-match dictionaries with enhanced object-level information\n        \"\"\"\n        all_non_matches = []\n\n        # Walk through the recursive result and collect non-matches\n        for field_name, field_result in recursive_result.get(\"fields\", {}).items():\n            gt_val = getattr(self, field_name)\n            pred_val = getattr(other, field_name, None)\n\n            # Check if this is a list field that should use object-level collection\n            if (\n                isinstance(gt_val, list)\n                and isinstance(pred_val, list)\n                and gt_val\n                and isinstance(gt_val[0], StructuredModel)\n            ):\n                # Use NonMatchesHelper for object-level collection\n                helper = NonMatchesHelper()\n                object_non_matches = helper.collect_list_non_matches(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(object_non_matches)\n\n            # Handle null list cases\n            elif (\n                (gt_val is None or (isinstance(gt_val, list) and len(gt_val) == 0))\n                and isinstance(pred_val, list)\n                and len(pred_val) &gt; 0\n            ):\n                # GT empty, pred has items \u2192 use helper for FA entries\n                helper = NonMatchesHelper()\n                null_non_matches = helper.add_non_matches_for_null_cases(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(null_non_matches)\n\n            elif (\n                isinstance(gt_val, list)\n                and len(gt_val) &gt; 0\n                and (\n                    pred_val is None\n                    or (isinstance(pred_val, list) and len(pred_val) == 0)\n                )\n            ):\n                # GT has items, pred empty \u2192 use helper for FN entries\n                helper = NonMatchesHelper()\n                null_non_matches = helper.add_non_matches_for_null_cases(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(null_non_matches)\n\n            else:\n                # Use existing field-level logic for non-list fields\n                # Extract metrics from field result to determine non-match type\n                if isinstance(field_result, dict) and \"overall\" in field_result:\n                    metrics = field_result[\"overall\"]\n                elif isinstance(field_result, dict):\n                    metrics = field_result\n                else:\n                    continue  # Skip if we can't extract metrics\n\n                # Create field-level non-match entries based on metrics (legacy format for backward compatibility)\n                if metrics.get(\"fa\", 0) &gt; 0:  # False Alarm\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_ALARM,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"details\": {\"reason\": \"unmatched prediction\"},\n                    }\n                    all_non_matches.append(entry)\n                elif metrics.get(\"fn\", 0) &gt; 0:  # False Negative\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_NEGATIVE,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"details\": {\"reason\": \"unmatched ground truth\"},\n                    }\n                    all_non_matches.append(entry)\n                elif metrics.get(\"fd\", 0) &gt; 0:  # False Discovery\n                    similarity = field_result.get(\"raw_similarity_score\")\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_DISCOVERY,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"similarity_score\": similarity,\n                        \"details\": {\"reason\": \"below threshold\"},\n                    }\n                    if similarity is not None:\n                        info = self._get_comparison_info(field_name)\n                        entry[\"details\"][\"reason\"] = (\n                            f\"below threshold ({similarity:.3f} &lt; {info.threshold})\"\n                        )\n                    all_non_matches.append(entry)\n\n                # ADDITIONAL: Handle nested StructuredModel objects for detailed non-match collection\n                if (\n                    isinstance(gt_val, StructuredModel)\n                    and isinstance(pred_val, StructuredModel)\n                    and \"fields\" in field_result\n                ):\n                    # Recursively collect non-matches from nested objects\n                    nested_non_matches = gt_val._collect_enhanced_non_matches(\n                        field_result, pred_val\n                    )\n                    # Prefix nested field paths with the parent field name\n                    for nested_nm in nested_non_matches:\n                        nested_nm[\"field_path\"] = (\n                            f\"{field_name}.{nested_nm['field_path']}\"\n                        )\n                        all_non_matches.append(nested_nm)\n\n        return all_non_matches\n\n    def _collect_non_matches(\n        self, other: \"StructuredModel\", base_path: str = \"\"\n    ) -&gt; List[NonMatchField]:\n        \"\"\"Collect non-matches for detailed analysis.\n\n        Args:\n            other: Other model to compare with\n            base_path: Base path for field naming (e.g., \"address\")\n\n        Returns:\n            List of NonMatchField objects documenting non-matches\n        \"\"\"\n        non_matches = []\n\n        # Handle null cases\n        if other is None:\n            non_matches.append(\n                NonMatchField(\n                    field_path=base_path or \"root\",\n                    non_match_type=NonMatchType.FALSE_NEGATIVE,\n                    ground_truth_value=self,\n                    prediction_value=None,\n                )\n            )\n            return non_matches\n\n        # Compare each field\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            field_path = f\"{base_path}.{field_name}\" if base_path else field_name\n            gt_value = getattr(self, field_name)\n            pred_value = getattr(other, field_name, None)\n\n            # Use existing field classification logic\n            if isinstance(pred_value, list):\n                classification = self._calculate_list_confusion_matrix(\n                    field_name, pred_value\n                )\n            else:\n                classification = self._classify_field_for_confusion_matrix(\n                    field_name, pred_value\n                )\n\n            # Document non-matches based on classification\n            if classification[\"fa\"] &gt; 0:  # False Alarm\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_ALARM,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                        similarity_score=classification.get(\"similarity_score\"),\n                    )\n                )\n            elif classification[\"fn\"] &gt; 0:  # False Negative\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_NEGATIVE,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                    )\n                )\n            elif classification[\"fd\"] &gt; 0:  # False Discovery\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_DISCOVERY,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                        similarity_score=classification.get(\"similarity_score\"),\n                    )\n                )\n\n            # Handle nested models recursively\n            if isinstance(gt_value, StructuredModel) and isinstance(\n                pred_value, StructuredModel\n            ):\n                nested_non_matches = gt_value._collect_non_matches(\n                    pred_value, field_path\n                )\n                non_matches.extend(nested_non_matches)\n\n        return non_matches\n\n    def compare(self, other: \"StructuredModel\") -&gt; float:\n        \"\"\"Compare this model with another and return a scalar similarity score.\n\n        Returns the overall weighted average score regardless of sufficient/necessary field matching.\n        This provides a more nuanced score for use in comparators.\n\n        Args:\n            other: Another instance of the same model to compare with\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        # We'll calculate the overall weighted score directly instead of using compare_with\n        # This ensures that sufficient/necessary field rules don't cause a zero score\n        # when at least some fields match\n\n        total_score = 0.0\n        total_weight = 0.0\n\n        for field_name in self.__class__.model_fields:\n            # Skip the extra_fields attribute in comparison\n            if field_name == \"extra_fields\":\n                continue\n            if hasattr(other, field_name):\n                # Get field configuration\n                info = self.__class__._get_comparison_info(field_name)\n                # Use weight from ComparableField object\n                weight = info.weight\n\n                # Compare field values WITHOUT applying thresholds\n                field_score = self.compare_field_raw(\n                    field_name, getattr(other, field_name)\n                )\n\n                # Update total score\n                total_score += field_score * weight\n                total_weight += weight\n\n        # Calculate overall score\n        if total_weight &gt; 0:\n            return total_score / total_weight\n        else:\n            return 0.0\n\n    def compare_with(\n        self,\n        other: \"StructuredModel\",\n        include_confusion_matrix: bool = False,\n        document_non_matches: bool = False,\n        evaluator_format: bool = False,\n        recall_with_fd: bool = False,\n        add_derived_metrics: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compare this model with another instance using SINGLE TRAVERSAL optimization.\n\n        Args:\n            other: Another instance of the same model to compare with\n            include_confusion_matrix: Whether to include confusion matrix calculations\n            document_non_matches: Whether to document non-matches for analysis\n            evaluator_format: Whether to format results for the evaluator\n            recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                            If False, use traditional recall (TP/(TP+FN))\n            add_derived_metrics: Whether to add derived metrics to confusion matrix\n\n        Returns:\n            Dictionary with comparison results including:\n            - field_scores: Scores for each field\n            - overall_score: Weighted average score\n            - all_fields_matched: Whether all fields matched\n            - confusion_matrix: (optional) Confusion matrix data if requested\n            - non_matches: (optional) Non-match documentation if requested\n        \"\"\"\n        # SINGLE TRAVERSAL: Get everything in one pass\n        recursive_result = self.compare_recursive(other)\n\n        # Extract scoring information from recursive result\n        field_scores = {}\n        for field_name, field_result in recursive_result[\"fields\"].items():\n            if isinstance(field_result, dict):\n                # Use threshold_applied_score when available, which respects clip_under_threshold setting\n                if \"threshold_applied_score\" in field_result:\n                    field_scores[field_name] = field_result[\"threshold_applied_score\"]\n                # Fallback to raw_similarity_score if threshold_applied_score not available\n                elif \"raw_similarity_score\" in field_result:\n                    field_scores[field_name] = field_result[\"raw_similarity_score\"]\n\n        # Extract overall metrics\n        overall_result = recursive_result[\"overall\"]\n        overall_score = overall_result.get(\"similarity_score\", 0.0)\n        all_fields_matched = overall_result.get(\"all_fields_matched\", False)\n\n        # Build basic result structure\n        result = {\n            \"field_scores\": field_scores,\n            \"overall_score\": overall_score,\n            \"all_fields_matched\": all_fields_matched,\n        }\n\n        # Add optional features using already-computed recursive result\n        if include_confusion_matrix:\n            confusion_matrix = recursive_result\n\n            # Add universal aggregate metrics to all nodes\n            confusion_matrix = self._calculate_aggregate_metrics(confusion_matrix)\n\n            # Add derived metrics if requested\n            if add_derived_metrics:\n                confusion_matrix = self._add_derived_metrics_to_result(confusion_matrix)\n\n            result[\"confusion_matrix\"] = confusion_matrix\n\n        # Add optional non-match documentation\n        if document_non_matches:\n            # NEW: Collect enhanced object-level non-matches\n            non_matches = self._collect_enhanced_non_matches(recursive_result, other)\n            result[\"non_matches\"] = non_matches\n\n        # If evaluator_format is requested, transform the result\n        if evaluator_format:\n            return self._format_for_evaluator(result, other, recall_with_fd)\n\n        return result\n\n    def _convert_score_to_binary_metrics(\n        self, score: float, threshold: float = 0.5\n    ) -&gt; Dict[str, float]:\n        \"\"\"Convert similarity score to binary classification metrics using MetricsHelper.\n\n        Args:\n            score: Similarity score [0-1]\n            threshold: Threshold for considering a match\n\n        Returns:\n            Dictionary with TP, FP, FN, TN counts converted to metrics\n        \"\"\"\n        metrics_helper = MetricsHelper()\n        return metrics_helper.convert_score_to_binary_metrics(score, threshold)\n\n    def _format_for_evaluator(\n        self,\n        result: Dict[str, Any],\n        other: \"StructuredModel\",\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Format comparison results for evaluator compatibility.\n\n        Args:\n            result: Standard comparison result from compare_with\n            other: The other model being compared\n            recall_with_fd: Whether to include FD in recall denominator\n\n        Returns:\n            Dictionary in evaluator format with overall, fields, confusion_matrix\n        \"\"\"\n        return EvaluatorFormatHelper.format_for_evaluator(\n            self, result, other, recall_with_fd\n        )\n\n    def _calculate_list_item_metrics(\n        self,\n        field_name: str,\n        gt_list: List[Any],\n        pred_list: List[Any],\n        recall_with_fd: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Calculate metrics for individual items in a list field.\n\n        Args:\n            field_name: Name of the list field\n            gt_list: Ground truth list\n            pred_list: Prediction list\n            recall_with_fd: Whether to include FD in recall denominator\n\n        Returns:\n            List of metrics dictionaries for each matched item pair\n        \"\"\"\n        return EvaluatorFormatHelper.calculate_list_item_metrics(\n            field_name, gt_list, pred_list, recall_with_fd\n        )\n\n    @classmethod\n    def model_json_schema(cls, **kwargs):\n        \"\"\"Override to add model-level comparison metadata.\n\n        Extends the standard Pydantic JSON schema with comparison metadata\n        at the field level.\n\n        Args:\n            **kwargs: Arguments to pass to the parent method\n\n        Returns:\n            JSON schema with added comparison metadata\n        \"\"\"\n        schema = super().model_json_schema(**kwargs)\n\n        # Add comparison metadata to each field in the schema\n        for field_name, field_info in cls.model_fields.items():\n            if field_name == \"extra_fields\":\n                continue\n\n            # Get the schema property for this field\n            if field_name not in schema.get(\"properties\", {}):\n                continue\n\n            field_props = schema[\"properties\"][field_name]\n\n            # Since ComparableField is now always a function, check for json_schema_extra\n            if hasattr(field_info, \"json_schema_extra\") and callable(\n                field_info.json_schema_extra\n            ):\n                # Fallback: Check for json_schema_extra function\n                temp_schema = {}\n                field_info.json_schema_extra(temp_schema)\n\n                if \"x-comparison\" in temp_schema:\n                    # Copy the comparison metadata from the temp schema to the real schema\n                    field_props[\"x-comparison\"] = temp_schema[\"x-comparison\"]\n\n        return schema\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Validate field configurations when a StructuredModel subclass is defined.</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Validate field configurations when a StructuredModel subclass is defined.\"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # Validate field configurations using class annotations since model_fields isn't populated yet\n    if hasattr(cls, \"__annotations__\"):\n        for field_name, field_type in cls.__annotations__.items():\n            if field_name == \"extra_fields\":\n                continue\n\n            # Get the field default value if it exists\n            field_default = getattr(cls, field_name, None)\n\n            # Since ComparableField is now always a function that returns a Field,\n            # we need to check if field_default has comparison metadata\n            if hasattr(field_default, \"json_schema_extra\") and callable(\n                field_default.json_schema_extra\n            ):\n                # Check for comparison metadata\n                temp_schema = {}\n                field_default.json_schema_extra(temp_schema)\n                if \"x-comparison\" in temp_schema:\n                    # This field was created with ComparableField function - validate constraints\n                    if cls._is_list_of_structured_model_type(field_type):\n                        comparison_config = temp_schema[\"x-comparison\"]\n\n                        # Threshold validation - only flag if explicitly set to non-default value\n                        threshold = comparison_config.get(\"threshold\", 0.5)\n                        if threshold != 0.5:  # Default threshold value\n                            raise ValueError(\n                                f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                f\"'threshold' parameter in ComparableField. Hungarian matching uses each \"\n                                f\"StructuredModel's 'match_threshold' class attribute instead. \"\n                                f\"Set 'match_threshold = {threshold}' on the list element class.\"\n                            )\n\n                        # Comparator validation - only flag if explicitly set to non-default type\n                        comparator_type = comparison_config.get(\n                            \"comparator_type\", \"LevenshteinComparator\"\n                        )\n                        if (\n                            comparator_type != \"LevenshteinComparator\"\n                        ):  # Default comparator type\n                            raise ValueError(\n                                f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                f\"'comparator' parameter in ComparableField. Object comparison uses each \"\n                                f\"StructuredModel's individual field comparators instead.\"\n                            )\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare","title":"<code>compare(other)</code>","text":"<p>Compare this model with another and return a scalar similarity score.</p> <p>Returns the overall weighted average score regardless of sufficient/necessary field matching. This provides a more nuanced score for use in comparators.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare(self, other: \"StructuredModel\") -&gt; float:\n    \"\"\"Compare this model with another and return a scalar similarity score.\n\n    Returns the overall weighted average score regardless of sufficient/necessary field matching.\n    This provides a more nuanced score for use in comparators.\n\n    Args:\n        other: Another instance of the same model to compare with\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    # We'll calculate the overall weighted score directly instead of using compare_with\n    # This ensures that sufficient/necessary field rules don't cause a zero score\n    # when at least some fields match\n\n    total_score = 0.0\n    total_weight = 0.0\n\n    for field_name in self.__class__.model_fields:\n        # Skip the extra_fields attribute in comparison\n        if field_name == \"extra_fields\":\n            continue\n        if hasattr(other, field_name):\n            # Get field configuration\n            info = self.__class__._get_comparison_info(field_name)\n            # Use weight from ComparableField object\n            weight = info.weight\n\n            # Compare field values WITHOUT applying thresholds\n            field_score = self.compare_field_raw(\n                field_name, getattr(other, field_name)\n            )\n\n            # Update total score\n            total_score += field_score * weight\n            total_weight += weight\n\n    # Calculate overall score\n    if total_weight &gt; 0:\n        return total_score / total_weight\n    else:\n        return 0.0\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_field","title":"<code>compare_field(field_name, other_value)</code>","text":"<p>Compare a single field with a value using the configured comparator.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to compare</p> required <code>other_value</code> <code>Any</code> <p>Value to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_field(self, field_name: str, other_value: Any) -&gt; float:\n    \"\"\"Compare a single field with a value using the configured comparator.\n\n    Args:\n        field_name: Name of the field to compare\n        other_value: Value to compare with\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    # Get our field value\n    my_value = getattr(self, field_name)\n\n    # If both values are StructuredModel instances, use recursive compare_with\n    if isinstance(my_value, StructuredModel) and isinstance(\n        other_value, StructuredModel\n    ):\n        # Use compare_with for rich comparison\n        comparison_result = my_value.compare_with(\n            other_value,\n            include_confusion_matrix=False,\n            document_non_matches=False,\n            evaluator_format=False,\n            recall_with_fd=False,\n        )\n        # Apply field-level threshold if configured\n        info = self._get_comparison_info(field_name)\n        raw_score = comparison_result[\"overall_score\"]\n        return (\n            raw_score\n            if raw_score &gt;= info.threshold or not info.clip_under_threshold\n            else 0.0\n        )\n\n    # CRITICAL FIX: For lists, don't clip under threshold for partial matches\n    if isinstance(my_value, list) and isinstance(other_value, list):\n        # Get field info\n        info = self._get_comparison_info(field_name)\n\n        # Use the raw comparison result without threshold clipping for lists\n        result = ComparisonHelper.compare_unordered_lists(\n            my_value, other_value, info.comparator, info.threshold\n        )\n\n        # Return the overall score directly (don't clip based on threshold for lists)\n        return result[\"overall_score\"]\n\n    # For other fields, use existing logic\n    return ComparisonHelper.compare_field_with_threshold(\n        self, field_name, other_value\n    )\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_field_raw","title":"<code>compare_field_raw(field_name, other_value)</code>","text":"<p>Compare a single field with a value WITHOUT applying thresholds.</p> <p>This version is used by the compare method to get raw similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to compare</p> required <code>other_value</code> <code>Any</code> <p>Value to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Raw similarity score between 0.0 and 1.0 without threshold filtering</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_field_raw(self, field_name: str, other_value: Any) -&gt; float:\n    \"\"\"Compare a single field with a value WITHOUT applying thresholds.\n\n    This version is used by the compare method to get raw similarity scores.\n\n    Args:\n        field_name: Name of the field to compare\n        other_value: Value to compare with\n\n    Returns:\n        Raw similarity score between 0.0 and 1.0 without threshold filtering\n    \"\"\"\n    # Get our field value\n    my_value = getattr(self, field_name)\n\n    # If both values are StructuredModel instances, use recursive compare_with\n    if isinstance(my_value, StructuredModel) and isinstance(\n        other_value, StructuredModel\n    ):\n        # Use compare_with for rich comparison, but extract the raw score\n        comparison_result = my_value.compare_with(\n            other_value,\n            include_confusion_matrix=False,\n            document_non_matches=False,\n            evaluator_format=False,\n            recall_with_fd=False,\n        )\n        return comparison_result[\"overall_score\"]\n\n    # For non-StructuredModel fields, use existing logic\n    return ComparisonHelper.compare_field_raw(self, field_name, other_value)\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_recursive","title":"<code>compare_recursive(other)</code>","text":"<p>The ONE clean recursive function that handles everything.</p> <p>Enhanced to capture BOTH confusion matrix metrics AND similarity scores in a single traversal to eliminate double traversal inefficiency.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with clean hierarchical structure:</p> <code>dict</code> <ul> <li>overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched</li> </ul> <code>dict</code> <ul> <li>fields: Recursive structure for each field with scores</li> </ul> <code>dict</code> <ul> <li>non_matches: List of non-matching items</li> </ul> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_recursive(self, other: \"StructuredModel\") -&gt; dict:\n    \"\"\"The ONE clean recursive function that handles everything.\n\n    Enhanced to capture BOTH confusion matrix metrics AND similarity scores\n    in a single traversal to eliminate double traversal inefficiency.\n\n    Args:\n        other: Another instance of the same model to compare with\n\n    Returns:\n        Dictionary with clean hierarchical structure:\n        - overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched\n        - fields: Recursive structure for each field with scores\n        - non_matches: List of non-matching items\n    \"\"\"\n    result = {\n        \"overall\": {\n            \"tp\": 0,\n            \"fa\": 0,\n            \"fd\": 0,\n            \"fp\": 0,\n            \"tn\": 0,\n            \"fn\": 0,\n            \"similarity_score\": 0.0,\n            \"all_fields_matched\": False,\n        },\n        \"fields\": {},\n        \"non_matches\": [],\n    }\n\n    # Score percolation variables\n    total_score = 0.0\n    total_weight = 0.0\n    threshold_matched_fields = set()\n\n    for field_name in self.__class__.model_fields:\n        if field_name == \"extra_fields\":\n            continue\n\n        gt_val = getattr(self, field_name)\n        pred_val = getattr(other, field_name, None)\n\n        # Enhanced dispatch returns both metrics AND scores\n        field_result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n\n        result[\"fields\"][field_name] = field_result\n\n        # Simple aggregation to overall metrics\n        self._aggregate_to_overall(field_result, result[\"overall\"])\n\n        # Score percolation - aggregate scores upward\n        if \"similarity_score\" in field_result and \"weight\" in field_result:\n            weight = field_result[\"weight\"]\n            threshold_applied_score = field_result[\"threshold_applied_score\"]\n            total_score += threshold_applied_score * weight\n            total_weight += weight\n\n            # Track threshold-matched fields\n            info = self._get_comparison_info(field_name)\n            if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n                threshold_matched_fields.add(field_name)\n\n    # CRITICAL FIX: Handle hallucinated fields (extra fields) as False Alarms\n    extra_fields_fa = self._count_extra_fields_as_false_alarms(other)\n    result[\"overall\"][\"fa\"] += extra_fields_fa\n    result[\"overall\"][\"fp\"] += extra_fields_fa\n\n    # Calculate overall similarity score from percolated scores\n    if total_weight &gt; 0:\n        result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n\n    # Determine all_fields_matched\n    model_fields_for_comparison = set(self.__class__.model_fields.keys()) - {\n        \"extra_fields\"\n    }\n    result[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(\n        model_fields_for_comparison\n    )\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_with","title":"<code>compare_with(other, include_confusion_matrix=False, document_non_matches=False, evaluator_format=False, recall_with_fd=False, add_derived_metrics=True)</code>","text":"<p>Compare this model with another instance using SINGLE TRAVERSAL optimization.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <code>include_confusion_matrix</code> <code>bool</code> <p>Whether to include confusion matrix calculations</p> <code>False</code> <code>document_non_matches</code> <code>bool</code> <p>Whether to document non-matches for analysis</p> <code>False</code> <code>evaluator_format</code> <code>bool</code> <p>Whether to format results for the evaluator</p> <code>False</code> <code>recall_with_fd</code> <code>bool</code> <p>If True, include FD in recall denominator (TP/(TP+FN+FD))             If False, use traditional recall (TP/(TP+FN))</p> <code>False</code> <code>add_derived_metrics</code> <code>bool</code> <p>Whether to add derived metrics to confusion matrix</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with comparison results including:</p> <code>Dict[str, Any]</code> <ul> <li>field_scores: Scores for each field</li> </ul> <code>Dict[str, Any]</code> <ul> <li>overall_score: Weighted average score</li> </ul> <code>Dict[str, Any]</code> <ul> <li>all_fields_matched: Whether all fields matched</li> </ul> <code>Dict[str, Any]</code> <ul> <li>confusion_matrix: (optional) Confusion matrix data if requested</li> </ul> <code>Dict[str, Any]</code> <ul> <li>non_matches: (optional) Non-match documentation if requested</li> </ul> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_with(\n    self,\n    other: \"StructuredModel\",\n    include_confusion_matrix: bool = False,\n    document_non_matches: bool = False,\n    evaluator_format: bool = False,\n    recall_with_fd: bool = False,\n    add_derived_metrics: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare this model with another instance using SINGLE TRAVERSAL optimization.\n\n    Args:\n        other: Another instance of the same model to compare with\n        include_confusion_matrix: Whether to include confusion matrix calculations\n        document_non_matches: Whether to document non-matches for analysis\n        evaluator_format: Whether to format results for the evaluator\n        recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                        If False, use traditional recall (TP/(TP+FN))\n        add_derived_metrics: Whether to add derived metrics to confusion matrix\n\n    Returns:\n        Dictionary with comparison results including:\n        - field_scores: Scores for each field\n        - overall_score: Weighted average score\n        - all_fields_matched: Whether all fields matched\n        - confusion_matrix: (optional) Confusion matrix data if requested\n        - non_matches: (optional) Non-match documentation if requested\n    \"\"\"\n    # SINGLE TRAVERSAL: Get everything in one pass\n    recursive_result = self.compare_recursive(other)\n\n    # Extract scoring information from recursive result\n    field_scores = {}\n    for field_name, field_result in recursive_result[\"fields\"].items():\n        if isinstance(field_result, dict):\n            # Use threshold_applied_score when available, which respects clip_under_threshold setting\n            if \"threshold_applied_score\" in field_result:\n                field_scores[field_name] = field_result[\"threshold_applied_score\"]\n            # Fallback to raw_similarity_score if threshold_applied_score not available\n            elif \"raw_similarity_score\" in field_result:\n                field_scores[field_name] = field_result[\"raw_similarity_score\"]\n\n    # Extract overall metrics\n    overall_result = recursive_result[\"overall\"]\n    overall_score = overall_result.get(\"similarity_score\", 0.0)\n    all_fields_matched = overall_result.get(\"all_fields_matched\", False)\n\n    # Build basic result structure\n    result = {\n        \"field_scores\": field_scores,\n        \"overall_score\": overall_score,\n        \"all_fields_matched\": all_fields_matched,\n    }\n\n    # Add optional features using already-computed recursive result\n    if include_confusion_matrix:\n        confusion_matrix = recursive_result\n\n        # Add universal aggregate metrics to all nodes\n        confusion_matrix = self._calculate_aggregate_metrics(confusion_matrix)\n\n        # Add derived metrics if requested\n        if add_derived_metrics:\n            confusion_matrix = self._add_derived_metrics_to_result(confusion_matrix)\n\n        result[\"confusion_matrix\"] = confusion_matrix\n\n    # Add optional non-match documentation\n    if document_non_matches:\n        # NEW: Collect enhanced object-level non-matches\n        non_matches = self._collect_enhanced_non_matches(recursive_result, other)\n        result[\"non_matches\"] = non_matches\n\n    # If evaluator_format is requested, transform the result\n    if evaluator_format:\n        return self._format_for_evaluator(result, other, recall_with_fd)\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.from_json","title":"<code>from_json(json_data)</code>  <code>classmethod</code>","text":"<p>Create a StructuredModel instance from JSON data.</p> <p>This method handles missing fields gracefully and stores extra fields in the extra_fields attribute.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>Dict[str, Any]</code> <p>Dictionary containing the JSON data</p> required <p>Returns:</p> Type Description <code>StructuredModel</code> <p>StructuredModel instance created from the JSON data</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef from_json(cls, json_data: Dict[str, Any]) -&gt; \"StructuredModel\":\n    \"\"\"Create a StructuredModel instance from JSON data.\n\n    This method handles missing fields gracefully and stores extra fields\n    in the extra_fields attribute.\n\n    Args:\n        json_data: Dictionary containing the JSON data\n\n    Returns:\n        StructuredModel instance created from the JSON data\n    \"\"\"\n    return ConfigurationHelper.from_json(cls, json_data)\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.model_from_json","title":"<code>model_from_json(config)</code>  <code>classmethod</code>","text":"<p>Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().</p> <p>This method leverages Pydantic's native dynamic model creation capabilities to ensure full compatibility with all Pydantic features while adding structured comparison functionality through inherited StructuredModel methods.</p> <p>The generated model inherits all StructuredModel capabilities: - compare_with() method for detailed comparisons - Field-level comparison configuration - Hungarian algorithm for list matching - Confusion matrix generation - JSON schema with comparison metadata</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>JSON configuration with fields, comparators, and model settings.    Required keys:    - fields: Dict mapping field names to field configurations    Optional keys:    - model_name: Name for the generated class (default: \"DynamicModel\")    - match_threshold: Overall matching threshold (default: 0.7)</p> <p>Field configuration format:    {        \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required        \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional        \"threshold\": 0.8,  # Optional, default 0.5        \"weight\": 2.0,     # Optional, default 1.0        \"required\": true,  # Optional, default false        \"default\": \"value\", # Optional        \"description\": \"Field description\",  # Optional        \"alias\": \"field_alias\",  # Optional        \"examples\": [\"example1\", \"example2\"]  # Optional    }</p> required <p>Returns:</p> Type Description <code>Type[StructuredModel]</code> <p>A fully functional StructuredModel subclass created with create_model()</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid or contains unsupported types/comparators</p> <code>KeyError</code> <p>If required configuration keys are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"model_name\": \"Product\",\n...     \"match_threshold\": 0.8,\n...     \"fields\": {\n...         \"name\": {\n...             \"type\": \"str\",\n...             \"comparator\": \"LevenshteinComparator\",\n...             \"threshold\": 0.8,\n...             \"weight\": 2.0,\n...             \"required\": True\n...         },\n...         \"price\": {\n...             \"type\": \"float\",\n...             \"comparator\": \"NumericComparator\",\n...             \"default\": 0.0\n...         }\n...     }\n... }\n&gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n&gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\nTrue\n&gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n&gt;&gt;&gt; product.name\n'Widget'\n&gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n&gt;&gt;&gt; result[\"overall_score\"]\n1.0\n</code></pre> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef model_from_json(cls, config: Dict[str, Any]) -&gt; Type[\"StructuredModel\"]:\n    \"\"\"Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().\n\n    This method leverages Pydantic's native dynamic model creation capabilities to ensure\n    full compatibility with all Pydantic features while adding structured comparison\n    functionality through inherited StructuredModel methods.\n\n    The generated model inherits all StructuredModel capabilities:\n    - compare_with() method for detailed comparisons\n    - Field-level comparison configuration\n    - Hungarian algorithm for list matching\n    - Confusion matrix generation\n    - JSON schema with comparison metadata\n\n    Args:\n        config: JSON configuration with fields, comparators, and model settings.\n               Required keys:\n               - fields: Dict mapping field names to field configurations\n               Optional keys:\n               - model_name: Name for the generated class (default: \"DynamicModel\")\n               - match_threshold: Overall matching threshold (default: 0.7)\n\n               Field configuration format:\n               {\n                   \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required\n                   \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional\n                   \"threshold\": 0.8,  # Optional, default 0.5\n                   \"weight\": 2.0,     # Optional, default 1.0\n                   \"required\": true,  # Optional, default false\n                   \"default\": \"value\", # Optional\n                   \"description\": \"Field description\",  # Optional\n                   \"alias\": \"field_alias\",  # Optional\n                   \"examples\": [\"example1\", \"example2\"]  # Optional\n               }\n\n    Returns:\n        A fully functional StructuredModel subclass created with create_model()\n\n    Raises:\n        ValueError: If configuration is invalid or contains unsupported types/comparators\n        KeyError: If required configuration keys are missing\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"model_name\": \"Product\",\n        ...     \"match_threshold\": 0.8,\n        ...     \"fields\": {\n        ...         \"name\": {\n        ...             \"type\": \"str\",\n        ...             \"comparator\": \"LevenshteinComparator\",\n        ...             \"threshold\": 0.8,\n        ...             \"weight\": 2.0,\n        ...             \"required\": True\n        ...         },\n        ...         \"price\": {\n        ...             \"type\": \"float\",\n        ...             \"comparator\": \"NumericComparator\",\n        ...             \"default\": 0.0\n        ...         }\n        ...     }\n        ... }\n        &gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n        &gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\n        True\n        &gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n        &gt;&gt;&gt; product.name\n        'Widget'\n        &gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n        &gt;&gt;&gt; result[\"overall_score\"]\n        1.0\n    \"\"\"\n    from pydantic import create_model\n    from .field_converter import convert_fields_config, validate_fields_config\n\n    # Validate configuration structure\n    if not isinstance(config, dict):\n        raise ValueError(\"Configuration must be a dictionary\")\n\n    if \"fields\" not in config:\n        raise ValueError(\"Configuration must contain 'fields' key\")\n\n    fields_config = config[\"fields\"]\n    if not isinstance(fields_config, dict) or len(fields_config) == 0:\n        raise ValueError(\"'fields' must be a non-empty dictionary\")\n\n    # Validate all field configurations before proceeding (including nested schema validation)\n    try:\n        from .field_converter import get_global_converter\n\n        converter = get_global_converter()\n\n        # First validate basic field configurations\n        validate_fields_config(fields_config)\n\n        # Then validate nested schema rules\n        for field_name, field_config in fields_config.items():\n            converter.validate_nested_field_schema(field_name, field_config)\n\n    except ValueError as e:\n        raise ValueError(f\"Invalid field configuration: {e}\")\n\n    # Extract model configuration\n    model_name = config.get(\"model_name\", \"DynamicModel\")\n    match_threshold = config.get(\"match_threshold\", 0.7)\n\n    # Validate model name\n    if not isinstance(model_name, str) or not model_name.isidentifier():\n        raise ValueError(\n            f\"model_name must be a valid Python identifier, got: {model_name}\"\n        )\n\n    # Validate match threshold\n    if not isinstance(match_threshold, (int, float)) or not (\n        0.0 &lt;= match_threshold &lt;= 1.0\n    ):\n        raise ValueError(\n            f\"match_threshold must be a number between 0.0 and 1.0, got: {match_threshold}\"\n        )\n\n    # Convert field configurations to Pydantic field definitions\n    try:\n        field_definitions = convert_fields_config(fields_config)\n    except ValueError as e:\n        raise ValueError(f\"Error converting field configurations: {e}\")\n\n    # Create the dynamic model extending StructuredModel\n    try:\n        DynamicClass = create_model(\n            model_name,\n            __base__=cls,  # Extend StructuredModel\n            **field_definitions,\n        )\n    except Exception as e:\n        raise ValueError(f\"Error creating dynamic model: {e}\")\n\n    # Set class-level attributes\n    DynamicClass.match_threshold = match_threshold\n\n    # Add configuration metadata for debugging/introspection\n    DynamicClass._model_config = config\n\n    return DynamicClass\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.model_json_schema","title":"<code>model_json_schema(**kwargs)</code>  <code>classmethod</code>","text":"<p>Override to add model-level comparison metadata.</p> <p>Extends the standard Pydantic JSON schema with comparison metadata at the field level.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arguments to pass to the parent method</p> <code>{}</code> <p>Returns:</p> Type Description <p>JSON schema with added comparison metadata</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef model_json_schema(cls, **kwargs):\n    \"\"\"Override to add model-level comparison metadata.\n\n    Extends the standard Pydantic JSON schema with comparison metadata\n    at the field level.\n\n    Args:\n        **kwargs: Arguments to pass to the parent method\n\n    Returns:\n        JSON schema with added comparison metadata\n    \"\"\"\n    schema = super().model_json_schema(**kwargs)\n\n    # Add comparison metadata to each field in the schema\n    for field_name, field_info in cls.model_fields.items():\n        if field_name == \"extra_fields\":\n            continue\n\n        # Get the schema property for this field\n        if field_name not in schema.get(\"properties\", {}):\n            continue\n\n        field_props = schema[\"properties\"][field_name]\n\n        # Since ComparableField is now always a function, check for json_schema_extra\n        if hasattr(field_info, \"json_schema_extra\") and callable(\n            field_info.json_schema_extra\n        ):\n            # Fallback: Check for json_schema_extra function\n            temp_schema = {}\n            field_info.json_schema_extra(temp_schema)\n\n            if \"x-comparison\" in temp_schema:\n                # Copy the comparison metadata from the temp schema to the real schema\n                field_props[\"x-comparison\"] = temp_schema[\"x-comparison\"]\n\n    return schema\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.comparable_field.ComparableField","title":"<code>stickler.structured_object_evaluator.models.comparable_field.ComparableField(comparator=None, threshold=0.5, weight=1.0, default=None, aggregate=False, clip_under_threshold=True, alias=None, description=None, examples=None, **field_kwargs)</code>","text":"<p>Create a Pydantic Field with comparison metadata.</p> <p>This function creates a proper Pydantic Field with embedded comparison configuration, enabling both comparison functionality and native Pydantic features like aliases.</p> <p>Parameters:</p> Name Type Description Default <code>comparator</code> <code>Optional[BaseComparator]</code> <p>Comparator to use for field comparison (default: LevenshteinComparator)</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity score to consider a match (default: 0.5)</p> <code>0.5</code> <code>weight</code> <code>float</code> <p>Weight of this field in overall score calculation (default: 1.0)</p> <code>1.0</code> <code>default</code> <code>Any</code> <p>Default value for the field (default: None)</p> <code>None</code> <code>aggregate</code> <code>bool</code> <p>DEPRECATED - This parameter is deprecated and will be removed in a future version.       Use the new universal 'aggregate' field in compare_with() output instead.</p> <code>False</code> <code>clip_under_threshold</code> <code>bool</code> <p>Whether to zero out scores below threshold (default: True)</p> <code>True</code> <code>alias</code> <code>Optional[str]</code> <p>Pydantic field alias for serialization (default: None)</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Field description for documentation (default: None)</p> <code>None</code> <code>examples</code> <code>Optional[list]</code> <p>Example values for the field (default: None)</p> <code>None</code> <code>**field_kwargs</code> <p>Additional Pydantic Field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>Pydantic Field with embedded comparison metadata</p> Example <p>class MyModel(StructuredModel):     # Basic usage (no alias):     name: str = ComparableField(threshold=0.8)</p> <pre><code># With alias (new feature):\nemail: str = ComparableField(\n    threshold=0.9,\n    alias=\"email_address\",\n    description=\"User's email\",\n    examples=[\"user@example.com\"]\n)\n</code></pre> Source code in <code>stickler/structured_object_evaluator/models/comparable_field.py</code> <pre><code>def ComparableField(\n    comparator: Optional[BaseComparator] = None,\n    threshold: float = 0.5,\n    weight: float = 1.0,\n    default: Any = None,\n    aggregate: bool = False,\n    clip_under_threshold: bool = True,\n    # Pydantic Field parameters (all optional, just like Field)\n    alias: Optional[str] = None,\n    description: Optional[str] = None,\n    examples: Optional[list] = None,\n    **field_kwargs,\n):\n    \"\"\"Create a Pydantic Field with comparison metadata.\n\n    This function creates a proper Pydantic Field with embedded comparison configuration,\n    enabling both comparison functionality and native Pydantic features like aliases.\n\n    Args:\n        comparator: Comparator to use for field comparison (default: LevenshteinComparator)\n        threshold: Minimum similarity score to consider a match (default: 0.5)\n        weight: Weight of this field in overall score calculation (default: 1.0)\n        default: Default value for the field (default: None)\n        aggregate: DEPRECATED - This parameter is deprecated and will be removed in a future version.\n                  Use the new universal 'aggregate' field in compare_with() output instead.\n        clip_under_threshold: Whether to zero out scores below threshold (default: True)\n        alias: Pydantic field alias for serialization (default: None)\n        description: Field description for documentation (default: None)\n        examples: Example values for the field (default: None)\n        **field_kwargs: Additional Pydantic Field arguments\n\n    Returns:\n        Pydantic Field with embedded comparison metadata\n\n    Example:\n        class MyModel(StructuredModel):\n            # Basic usage (no alias):\n            name: str = ComparableField(threshold=0.8)\n\n            # With alias (new feature):\n            email: str = ComparableField(\n                threshold=0.9,\n                alias=\"email_address\",\n                description=\"User's email\",\n                examples=[\"user@example.com\"]\n            )\n    \"\"\"\n    # Issue deprecation warning if aggregate=True is used\n    if aggregate:\n        warnings.warn(\n            \"The 'aggregate' parameter in ComparableField is deprecated and will be removed \"\n            \"in a future version. All nodes now automatically include an 'aggregate' field \"\n            \"in the compare_with() output that sums primitive field metrics below that node.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    # Create the actual comparator instance\n    actual_comparator = comparator or LevenshteinComparator()\n\n    # Create serializable metadata for JSON schema compatibility\n    serializable_metadata = {\n        \"comparator_type\": actual_comparator.__class__.__name__,\n        \"comparator_name\": getattr(actual_comparator, \"name\", \"unknown\"),\n        \"comparator_config\": getattr(actual_comparator, \"config\", {}),\n        \"threshold\": threshold,\n        \"weight\": weight,\n        \"clip_under_threshold\": clip_under_threshold,\n        \"aggregate\": aggregate,\n    }\n\n    # Create json_schema_extra function that stores runtime data\n    def json_schema_extra_func(schema: Dict[str, Any]) -&gt; None:\n        schema[\"x-comparison\"] = serializable_metadata\n\n    # HYBRID APPROACH: Store runtime instances as function attributes\n    # This works around FieldInfo's __slots__ restriction\n    json_schema_extra_func._comparator_instance = actual_comparator\n    json_schema_extra_func._threshold = threshold\n    json_schema_extra_func._weight = weight\n    json_schema_extra_func._clip_under_threshold = clip_under_threshold\n    json_schema_extra_func._aggregate = aggregate\n    json_schema_extra_func._comparison_metadata = serializable_metadata\n\n    # Merge with existing json_schema_extra if provided\n    existing_json_schema_extra = field_kwargs.get(\"json_schema_extra\", {})\n    if callable(existing_json_schema_extra):\n\n        def enhanced_json_schema_extra(schema: Dict[str, Any]) -&gt; None:\n            existing_json_schema_extra(schema)\n            json_schema_extra_func(schema)\n\n        # Copy our runtime data to the enhanced function\n        enhanced_json_schema_extra._comparator_instance = actual_comparator\n        enhanced_json_schema_extra._threshold = threshold\n        enhanced_json_schema_extra._weight = weight\n        enhanced_json_schema_extra._clip_under_threshold = clip_under_threshold\n        enhanced_json_schema_extra._aggregate = aggregate\n        enhanced_json_schema_extra._comparison_metadata = serializable_metadata\n        final_json_schema_extra = enhanced_json_schema_extra\n    elif isinstance(existing_json_schema_extra, dict):\n\n        def enhanced_json_schema_extra(schema: Dict[str, Any]) -&gt; None:\n            schema.update(existing_json_schema_extra)\n            json_schema_extra_func(schema)\n\n        # Copy our runtime data to the enhanced function\n        enhanced_json_schema_extra._comparator_instance = actual_comparator\n        enhanced_json_schema_extra._threshold = threshold\n        enhanced_json_schema_extra._weight = weight\n        enhanced_json_schema_extra._clip_under_threshold = clip_under_threshold\n        enhanced_json_schema_extra._aggregate = aggregate\n        enhanced_json_schema_extra._comparison_metadata = serializable_metadata\n        final_json_schema_extra = enhanced_json_schema_extra\n    else:\n        final_json_schema_extra = json_schema_extra_func\n\n    # Remove json_schema_extra from field_kwargs to avoid duplication\n    clean_field_kwargs = {\n        k: v for k, v in field_kwargs.items() if k != \"json_schema_extra\"\n    }\n\n    # Create the Field\n    field = Field(\n        default=default,\n        alias=alias,\n        description=description,\n        examples=examples,\n        json_schema_extra=final_json_schema_extra,\n        **clean_field_kwargs,\n    )\n\n    return field\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField","title":"<code>stickler.structured_object_evaluator.models.non_match_field.NonMatchField</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for documenting non-matches in structured object evaluation.</p> <p>This class stores detailed information about each non-match detected during the evaluation process, enabling more thorough analysis and debugging of evaluation results.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>class NonMatchField(BaseModel):\n    \"\"\"Model for documenting non-matches in structured object evaluation.\n\n    This class stores detailed information about each non-match detected\n    during the evaluation process, enabling more thorough analysis and\n    debugging of evaluation results.\n    \"\"\"\n\n    field_path: str = Field(\n        description=\"Dot-notation path to the field (e.g., 'address.city')\"\n    )\n    non_match_type: NonMatchType = Field(description=\"Type of non-match\")\n    ground_truth_value: Any = Field(description=\"Original ground truth value\")\n    prediction_value: Any = Field(description=\"Predicted value\")\n    similarity_score: Optional[float] = Field(\n        default=None, description=\"Similarity score if available\"\n    )\n    details: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Additional context or details\"\n    )\n    document_id: Optional[str] = Field(\n        default=None, description=\"ID of the document this non-match belongs to\"\n    )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the non-match document.\"\"\"\n        similarity_str = (\n            f\", similarity: {self.similarity_score:.4f}\"\n            if self.similarity_score is not None\n            else \"\"\n        )\n        doc_id_str = f\" (doc: {self.document_id})\" if self.document_id else \"\"\n        return (\n            f\"{self.non_match_type.value.upper()} at '{self.field_path}'{similarity_str}{doc_id_str}\\n\"\n            f\"  GT: {self.ground_truth_value}\\n\"\n            f\"  Pred: {self.prediction_value}\"\n        )\n\n    @staticmethod\n    def filter_by_type(\n        documents: List[\"NonMatchField\"], match_type: NonMatchType\n    ) -&gt; List[\"NonMatchField\"]:\n        \"\"\"\n        Filter non-match documents by their type.\n\n        Args:\n            documents: List of NonMatchField instances to filter\n            match_type: Type of non-match to filter for\n\n        Returns:\n            Filtered list of NonMatchField instances\n        \"\"\"\n        return [doc for doc in documents if doc.non_match_type == match_type]\n\n    @staticmethod\n    def export_to_dict(\n        documents: List[\"NonMatchField\"],\n    ) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Export a list of non-match documents to a dictionary for serialization.\n\n        Args:\n            documents: List of NonMatchField instances\n\n        Returns:\n            Dictionary with categorized non-matches\n        \"\"\"\n        result = {\"false_alarms\": [], \"false_discoveries\": [], \"false_negatives\": []}\n\n        for doc in documents:\n            # Create a simplified entry\n            entry = {\n                \"field_path\": doc.field_path,\n                \"ground_truth\": str(doc.ground_truth_value),\n                \"prediction\": str(doc.prediction_value),\n            }\n\n            if doc.similarity_score is not None:\n                entry[\"similarity_score\"] = doc.similarity_score\n\n            if doc.details:\n                entry[\"details\"] = doc.details\n\n            if doc.non_match_type == NonMatchType.FALSE_ALARM:\n                result[\"false_alarms\"].append(entry)\n            elif doc.non_match_type == NonMatchType.FALSE_DISCOVERY:\n                result[\"false_discoveries\"].append(entry)\n            elif doc.non_match_type == NonMatchType.FALSE_NEGATIVE:\n                result[\"false_negatives\"].append(entry)\n\n        return result\n\n    @staticmethod\n    def export_to_json(documents: List[\"NonMatchField\"], output_path: str):\n        \"\"\"\n        Export a list of non-match documents to a JSON file.\n\n        Args:\n            documents: List of NonMatchField instances\n            output_path: Path to save the JSON file\n        \"\"\"\n        # Create parent directories if needed\n        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n        # Export as dictionary\n        data = NonMatchField.export_to_dict(documents)\n\n        # Write to file\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2)\n\n    @staticmethod\n    def print_summary(documents: List[\"NonMatchField\"], detailed: bool = False):\n        \"\"\"\n        Print a summary of non-match documents.\n\n        Args:\n            documents: List of NonMatchField instances\n            detailed: Whether to print detailed information for each document\n        \"\"\"\n        # Count by type\n        false_alarms = NonMatchField.filter_by_type(documents, NonMatchType.FALSE_ALARM)\n        false_discoveries = NonMatchField.filter_by_type(\n            documents, NonMatchType.FALSE_DISCOVERY\n        )\n        false_negatives = NonMatchField.filter_by_type(\n            documents, NonMatchType.FALSE_NEGATIVE\n        )\n\n        # Print summary counts\n        print(f\"Non-matches summary:\")\n        print(f\"- False Alarms: {len(false_alarms)}\")\n        print(f\"- False Discoveries: {len(false_discoveries)}\")\n        print(f\"- False Negatives: {len(false_negatives)}\")\n\n        # Print details if requested\n        if detailed and documents:\n            print(\"\\nDetailed non-matches:\")\n            for i, doc in enumerate(documents):\n                print(f\"\\nNon-match #{i + 1}:\")\n                print(f\"- Type: {doc.non_match_type}\")\n                print(f\"- Field: {doc.field_path}\")\n                print(f\"- Ground truth: {doc.ground_truth_value}\")\n                print(f\"- Prediction: {doc.prediction_value}\")\n                if doc.similarity_score is not None:\n                    print(f\"- Similarity: {doc.similarity_score:.4f}\")\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the non-match document.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the non-match document.\"\"\"\n    similarity_str = (\n        f\", similarity: {self.similarity_score:.4f}\"\n        if self.similarity_score is not None\n        else \"\"\n    )\n    doc_id_str = f\" (doc: {self.document_id})\" if self.document_id else \"\"\n    return (\n        f\"{self.non_match_type.value.upper()} at '{self.field_path}'{similarity_str}{doc_id_str}\\n\"\n        f\"  GT: {self.ground_truth_value}\\n\"\n        f\"  Pred: {self.prediction_value}\"\n    )\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.export_to_dict","title":"<code>export_to_dict(documents)</code>  <code>staticmethod</code>","text":"<p>Export a list of non-match documents to a dictionary for serialization.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>Dictionary with categorized non-matches</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef export_to_dict(\n    documents: List[\"NonMatchField\"],\n) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"\n    Export a list of non-match documents to a dictionary for serialization.\n\n    Args:\n        documents: List of NonMatchField instances\n\n    Returns:\n        Dictionary with categorized non-matches\n    \"\"\"\n    result = {\"false_alarms\": [], \"false_discoveries\": [], \"false_negatives\": []}\n\n    for doc in documents:\n        # Create a simplified entry\n        entry = {\n            \"field_path\": doc.field_path,\n            \"ground_truth\": str(doc.ground_truth_value),\n            \"prediction\": str(doc.prediction_value),\n        }\n\n        if doc.similarity_score is not None:\n            entry[\"similarity_score\"] = doc.similarity_score\n\n        if doc.details:\n            entry[\"details\"] = doc.details\n\n        if doc.non_match_type == NonMatchType.FALSE_ALARM:\n            result[\"false_alarms\"].append(entry)\n        elif doc.non_match_type == NonMatchType.FALSE_DISCOVERY:\n            result[\"false_discoveries\"].append(entry)\n        elif doc.non_match_type == NonMatchType.FALSE_NEGATIVE:\n            result[\"false_negatives\"].append(entry)\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.export_to_json","title":"<code>export_to_json(documents, output_path)</code>  <code>staticmethod</code>","text":"<p>Export a list of non-match documents to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <code>output_path</code> <code>str</code> <p>Path to save the JSON file</p> required Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef export_to_json(documents: List[\"NonMatchField\"], output_path: str):\n    \"\"\"\n    Export a list of non-match documents to a JSON file.\n\n    Args:\n        documents: List of NonMatchField instances\n        output_path: Path to save the JSON file\n    \"\"\"\n    # Create parent directories if needed\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # Export as dictionary\n    data = NonMatchField.export_to_dict(documents)\n\n    # Write to file\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.filter_by_type","title":"<code>filter_by_type(documents, match_type)</code>  <code>staticmethod</code>","text":"<p>Filter non-match documents by their type.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances to filter</p> required <code>match_type</code> <code>NonMatchType</code> <p>Type of non-match to filter for</p> required <p>Returns:</p> Type Description <code>List[NonMatchField]</code> <p>Filtered list of NonMatchField instances</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef filter_by_type(\n    documents: List[\"NonMatchField\"], match_type: NonMatchType\n) -&gt; List[\"NonMatchField\"]:\n    \"\"\"\n    Filter non-match documents by their type.\n\n    Args:\n        documents: List of NonMatchField instances to filter\n        match_type: Type of non-match to filter for\n\n    Returns:\n        Filtered list of NonMatchField instances\n    \"\"\"\n    return [doc for doc in documents if doc.non_match_type == match_type]\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.print_summary","title":"<code>print_summary(documents, detailed=False)</code>  <code>staticmethod</code>","text":"<p>Print a summary of non-match documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <code>detailed</code> <code>bool</code> <p>Whether to print detailed information for each document</p> <code>False</code> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef print_summary(documents: List[\"NonMatchField\"], detailed: bool = False):\n    \"\"\"\n    Print a summary of non-match documents.\n\n    Args:\n        documents: List of NonMatchField instances\n        detailed: Whether to print detailed information for each document\n    \"\"\"\n    # Count by type\n    false_alarms = NonMatchField.filter_by_type(documents, NonMatchType.FALSE_ALARM)\n    false_discoveries = NonMatchField.filter_by_type(\n        documents, NonMatchType.FALSE_DISCOVERY\n    )\n    false_negatives = NonMatchField.filter_by_type(\n        documents, NonMatchType.FALSE_NEGATIVE\n    )\n\n    # Print summary counts\n    print(f\"Non-matches summary:\")\n    print(f\"- False Alarms: {len(false_alarms)}\")\n    print(f\"- False Discoveries: {len(false_discoveries)}\")\n    print(f\"- False Negatives: {len(false_negatives)}\")\n\n    # Print details if requested\n    if detailed and documents:\n        print(\"\\nDetailed non-matches:\")\n        for i, doc in enumerate(documents):\n            print(f\"\\nNon-match #{i + 1}:\")\n            print(f\"- Type: {doc.non_match_type}\")\n            print(f\"- Field: {doc.field_path}\")\n            print(f\"- Ground truth: {doc.ground_truth_value}\")\n            print(f\"- Prediction: {doc.prediction_value}\")\n            if doc.similarity_score is not None:\n                print(f\"- Similarity: {doc.similarity_score:.4f}\")\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.models.non_match_field.NonMatchType","title":"<code>stickler.structured_object_evaluator.models.non_match_field.NonMatchType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum defining the types of non-matches.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>class NonMatchType(str, Enum):\n    \"\"\"Enum defining the types of non-matches.\"\"\"\n\n    FALSE_ALARM = \"false_alarm\"  # GT null, prediction non-null\n    FALSE_DISCOVERY = \"false_discovery\"  # Both non-null but don't match\n    FALSE_NEGATIVE = \"false_negative\"  # GT non-null, prediction null\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator","title":"<code>stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator</code>","text":"<p>Evaluator for StructuredModel objects.</p> <p>This evaluator computes comprehensive metrics for StructuredModel objects, leveraging their built-in comparison capabilities. It includes confusion matrix calculations, field-level metrics, non-match documentation, and memory optimization capabilities.</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>class StructuredModelEvaluator:\n    \"\"\"\n    Evaluator for StructuredModel objects.\n\n    This evaluator computes comprehensive metrics for StructuredModel objects,\n    leveraging their built-in comparison capabilities. It includes confusion matrix\n    calculations, field-level metrics, non-match documentation, and memory optimization capabilities.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_class: Optional[Type[StructuredModel]] = None,\n        threshold: float = 0.5,\n        verbose: bool = False,\n        document_non_matches: bool = True,\n    ):\n        \"\"\"\n        Initialize the evaluator.\n\n        Args:\n            model_class: Optional StructuredModel class for type checking\n            threshold: Similarity threshold for considering a match\n            verbose: Whether to print detailed progress information\n            document_non_matches: Whether to document detailed non-match information\n        \"\"\"\n        self.model_class = model_class\n        self.threshold = threshold\n        self.verbose = verbose\n        self.peak_memory_usage = 0\n        self.start_memory = get_memory_usage()\n\n        # New attributes for documenting non-matches\n        self.document_non_matches = document_non_matches\n        self.non_match_documents: List[NonMatchField] = []\n\n        if self.verbose:\n            print(\n                f\"Initialized StructuredModelEvaluator. Starting memory: {self.start_memory:.2f} MB\"\n            )\n\n    def _check_memory(self):\n        \"\"\"Check current memory usage and update peak memory.\"\"\"\n        current_memory = get_memory_usage()\n\n        if current_memory &gt; self.peak_memory_usage:\n            self.peak_memory_usage = current_memory\n\n        if self.verbose and current_memory &gt; self.start_memory + 100:  # 100MB increase\n            print(f\"Memory usage increased: {current_memory:.2f} MB\")\n\n        return current_memory\n\n    def _calculate_metrics_from_binary(\n        self,\n        tp: float,\n        fp: float,\n        fn: float,\n        tn: float = 0.0,\n        fd: float = 0.0,\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate metrics from binary classification counts.\n\n        Args:\n            tp: True positive count\n            fp: False positive count\n            fn: False negative count\n            tn: True negative count (default 0)\n            fd: False discovery count (default 0) - used only when recall_with_fd=True\n            recall_with_fd: Whether to use alternative recall formula including FD in denominator\n\n        Returns:\n            Dictionary with precision, recall, F1, and accuracy\n        \"\"\"\n        # Calculate precision\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n\n        # Calculate recall based on the selected formula\n        if recall_with_fd:\n            # Alternative recall: TP / (TP + FN + FD)\n            recall = tp / (tp + fn + fd) if (tp + fn + fd) &gt; 0 else 0.0\n        else:\n            # Traditional recall: TP / (TP + FN)\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n\n        # Calculate F1 score\n        f1 = (\n            2 * (precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0.0\n        )\n\n        # Calculate accuracy\n        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) &gt; 0 else 0.0\n\n        return {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"accuracy\": accuracy,\n        }\n\n    def calculate_derived_confusion_matrix_metrics(\n        self, cm_counts: Dict[str, Union[int, float]]\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate derived metrics from confusion matrix counts.\n\n        This method uses MetricsHelper to maintain consistency and avoid code duplication.\n\n        Args:\n            cm_counts: Dictionary with confusion matrix counts containing keys:\n                      'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'\n\n        Returns:\n            Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy\n        \"\"\"\n        # Use MetricsHelper for consistent metric calculation\n        from stickler.structured_object_evaluator.models.metrics_helper import (\n            MetricsHelper,\n        )\n\n        metrics_helper = MetricsHelper()\n\n        # Convert counts to the format expected by MetricsHelper\n        metrics_dict = {\n            \"tp\": int(cm_counts.get(\"tp\", 0)),\n            \"fp\": int(cm_counts.get(\"fp\", 0)),\n            \"tn\": int(cm_counts.get(\"tn\", 0)),\n            \"fn\": int(cm_counts.get(\"fn\", 0)),\n            \"fd\": int(cm_counts.get(\"fd\", 0)),\n            \"fa\": int(cm_counts.get(\"fa\", 0)),\n        }\n\n        # Use MetricsHelper to calculate derived metrics\n        return metrics_helper.calculate_derived_metrics(metrics_dict)\n\n    def _convert_score_to_binary(self, score: float) -&gt; Dict[str, float]:\n        \"\"\"\n        Convert an ANLS Star score to binary classification counts.\n\n        Args:\n            score: ANLS Star similarity score [0-1]\n\n        Returns:\n            Dictionary with TP, FP, FN, TN counts\n        \"\"\"\n        # For a single field comparison, there are different approaches\n        # to convert a similarity score to binary classification:\n\n        # Approach used here: If score &gt;= threshold, count as TP with\n        # proportional value, otherwise count as partial FP and partial FN\n        if score &gt;= self.threshold:\n            # Handle as true positive with proportional credit\n            tp = score  # Proportional TP\n            fp = (\n                1 - score if score &lt; 1.0 else 0\n            )  # Proportional FP for imperfect matches\n            fn = 0\n            tn = 0\n        else:\n            # Handle as false classification\n            tp = 0\n            fp = score  # Give partial credit for similarity even if below threshold\n            fn = 1 - score  # More different = higher FN\n            tn = 0\n\n        return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n\n    def _is_null_value(self, value: Any) -&gt; bool:\n        \"\"\"\n        Determine if a value should be considered null or empty.\n\n        Args:\n            value: The value to check\n\n        Returns:\n            True if the value is null/empty, False otherwise\n        \"\"\"\n        if value is None:\n            return True\n        elif hasattr(value, \"__len__\") and not isinstance(\n            value, (str, bytes, bytearray)\n        ):\n            # Consider empty lists/collections as null values\n            return len(value) == 0\n        elif isinstance(value, (str, bytes, bytearray)):\n            return len(value.strip()) == 0\n        return False\n\n    def combine_cm_dicts(\n        self, cm1: Dict[str, int], cm2: Dict[str, int]\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Combine two confusion matrix dictionaries by adding corresponding values.\n\n        Args:\n            cm1: First confusion matrix dictionary\n            cm2: Second confusion matrix dictionary\n\n        Returns:\n            Combined confusion matrix dictionary\n        \"\"\"\n        return {\n            key: cm1.get(key, 0) + cm2.get(key, 0)\n            for key in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]\n        }\n\n    def add_non_match(\n        self,\n        field_path: str,\n        non_match_type: NonMatchType,\n        gt_value: Any,\n        pred_value: Any,\n        similarity_score: Optional[float] = None,\n        details: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Document a non-match with detailed information.\n\n        Args:\n            field_path: Dot-notation path to the field (e.g., 'address.city')\n            non_match_type: Type of non-match\n            gt_value: Ground truth value\n            pred_value: Predicted value\n            similarity_score: Optional similarity score if available\n            details: Optional additional context or details\n            document_id: Optional ID of the document this non-match belongs to\n        \"\"\"\n        if not self.document_non_matches:\n            return\n\n        self.non_match_documents.append(\n            NonMatchField(\n                field_path=field_path,\n                non_match_type=non_match_type,\n                ground_truth_value=gt_value,\n                prediction_value=pred_value,\n                similarity_score=similarity_score,\n                details=details or {},\n            )\n        )\n\n    def clear_non_match_documents(self):\n        \"\"\"Clear the stored non-match documents.\"\"\"\n        self.non_match_documents = []\n\n    def _convert_enhanced_non_match_to_field(\n        self, nm_dict: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Convert enhanced non-match format to NonMatchField format.\n\n        Args:\n            nm_dict: Enhanced non-match dictionary from StructuredModel\n\n        Returns:\n            Dictionary in NonMatchField format\n        \"\"\"\n        # Map enhanced format to NonMatchField format\n        converted = {\n            \"field_path\": nm_dict.get(\"field_path\", \"\"),\n            \"ground_truth_value\": nm_dict.get(\"ground_truth_value\"),\n            \"prediction_value\": nm_dict.get(\"prediction_value\"),\n            \"similarity_score\": nm_dict.get(\"similarity_score\"),\n            \"details\": nm_dict.get(\"details\", {}),\n        }\n\n        # The non_match_type is already a NonMatchType enum from StructuredModel\n        converted[\"non_match_type\"] = nm_dict.get(\"non_match_type\")\n\n        return converted\n\n    def _compare_models(\n        self, gt_model: StructuredModel, pred_model: StructuredModel\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compare two StructuredModel instances and return metrics.\n\n        Args:\n            gt_model: Ground truth model\n            pred_model: Predicted model\n\n        Returns:\n            Dict with comparison metrics including tp, fp, fn, tn, field_scores, overall_score\n        \"\"\"\n        # Check if inputs are valid StructuredModel instances\n        if not (\n            isinstance(gt_model, StructuredModel)\n            and isinstance(pred_model, StructuredModel)\n        ):\n            raise TypeError(\"Both models must be StructuredModel instances\")\n\n        # If model_class is specified, check type\n        if self.model_class and not (\n            isinstance(gt_model, self.model_class)\n            and isinstance(pred_model, self.model_class)\n        ):\n            raise TypeError(\n                f\"Both models must be instances of {self.model_class.__name__}\"\n            )\n\n        # Use the built-in compare_with method from StructuredModel\n        comparison_result = gt_model.compare_with(pred_model)\n\n        # Initialize metrics\n        tp = fp = fn = tn = 0\n\n        # Determine match status\n        if comparison_result[\"overall_score\"] &gt;= self.threshold:\n            # Good enough match\n            tp = 1\n        else:\n            # Not a good enough match\n            fp = 1\n\n        # Prepare result\n        result = {\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n            \"field_scores\": comparison_result[\"field_scores\"],\n            \"overall_score\": comparison_result[\"overall_score\"],\n            # match_status removed - now unnecessary\n        }\n\n        return result\n\n    def evaluate(\n        self,\n        ground_truth: StructuredModel,\n        predictions: StructuredModel,\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate predictions against ground truth and return comprehensive metrics.\n\n        Args:\n            ground_truth: Ground truth data (StructuredModel instance)\n            predictions: Predicted data (StructuredModel instance)\n            recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                            If False, use traditional recall (TP/(TP+FN))\n\n        Returns:\n            Dictionary with the following structure:\n\n            {\n                \"overall\": {\n                    \"precision\": float,     # Overall precision [0-1]\n                    \"recall\": float,        # Overall recall [0-1]\n                    \"f1\": float,           # Overall F1 score [0-1]\n                    \"accuracy\": float,     # Overall accuracy [0-1]\n                    \"anls_score\": float    # Overall ANLS similarity score [0-1]\n                },\n\n                \"fields\": {\n                    \"&lt;field_name&gt;\": {\n                        # For primitive fields (str, int, float, bool):\n                        \"precision\": float,\n                        \"recall\": float,\n                        \"f1\": float,\n                        \"accuracy\": float,\n                        \"anls_score\": float\n                    },\n\n                    \"&lt;list_field_name&gt;\": {\n                        # For list fields (e.g., products: List[Product]):\n                        \"overall\": {\n                            \"precision\": float,\n                            \"recall\": float,\n                            \"f1\": float,\n                            \"accuracy\": float,\n                            \"anls_score\": float\n                        },\n                        \"items\": [\n                            # Individual metrics for each matched item pair\n                            {\n                                \"overall\": {...},  # Item-level overall metrics\n                                \"fields\": {        # Field metrics within each item\n                                    \"&lt;nested_field&gt;\": {...}\n                                }\n                            }\n                        ]\n                    }\n                },\n\n                \"confusion_matrix\": {\n                    \"fields\": {\n                        # AGGREGATED metrics for all field types\n                        \"&lt;field_name&gt;\": {\n                            \"tp\": int,          # True positives\n                            \"fp\": int,          # False positives\n                            \"tn\": int,          # True negatives\n                            \"fn\": int,          # False negatives\n                            \"fd\": int,          # False discoveries (non-null but don't match)\n                            \"fa\": int,          # False alarms\n                            \"derived\": {\n                                \"cm_precision\": float,\n                                \"cm_recall\": float,\n                                \"cm_f1\": float,\n                                \"cm_accuracy\": float\n                            }\n                        },\n\n                        # For list fields with nested objects, aggregated field metrics:\n                        \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n                            # Aggregated counts across ALL instances in the list\n                            \"tp\": int,    # Total true positives for this field across all items\n                            \"fp\": int,    # Total false positives for this field across all items\n                            \"fn\": int,    # Total false negatives for this field across all items\n                            \"fd\": int,    # Total false discoveries for this field across all items\n                            \"fa\": int,    # Total false alarms for this field across all items\n                            \"derived\": {...}\n                        }\n                    },\n\n                    \"overall\": {\n                        # Overall confusion matrix aggregating all fields\n                        \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n                        \"derived\": {...}\n                    }\n                }\n            }\n\n        Key Usage Patterns:\n\n        1. **Individual Item Metrics** (per-instance analysis):\n           ```python\n           # Access metrics for each individual item in a list\n           for i, item_metrics in enumerate(results['fields']['products']['items']):\n               print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n           ```\n\n        2. **Aggregated Field Metrics** (recommended for field performance analysis):\n           ```python\n           # Access aggregated metrics across all instances of a field type\n           cm_fields = results['confusion_matrix']['fields']\n           product_id_performance = cm_fields['products.product_id']\n           print(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n           # Get all aggregated product field metrics\n           product_fields = {k: v for k, v in cm_fields.items()\n                           if k.startswith('products.')}\n           ```\n\n        3. **Helper Function for Aggregated Metrics**:\n           ```python\n           def get_aggregated_metrics(results, list_field_name):\n               '''Extract aggregated field metrics for a list field.'''\n               cm_fields = results['confusion_matrix']['fields']\n               prefix = f\"{list_field_name}.\"\n               return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n                      if k.startswith(prefix)}\n\n           # Usage:\n           product_metrics = get_aggregated_metrics(results, 'products')\n           print(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n           ```\n\n        Note:\n            - Use `results['fields'][field]['items']` for per-instance analysis\n            - Use `results['confusion_matrix']['fields'][field.subfield]` for aggregated field analysis\n            - Aggregated metrics provide rolled-up performance across all instances of a field type\n            - Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics\n        \"\"\"\n        # Clear any existing non-match documents\n        self.clear_non_match_documents()\n\n        # Use StructuredModel's enhanced comparison with evaluator format\n        # This pushes all the heavy lifting into the StructuredModel as requested\n        result = ground_truth.compare_with(\n            predictions,\n            include_confusion_matrix=True,\n            document_non_matches=self.document_non_matches,\n            evaluator_format=True,  # This makes StructuredModel return evaluator-compatible format\n            recall_with_fd=recall_with_fd,\n        )\n\n        # Add non-matches to evaluator's collection if they exist\n        if result.get(\"non_matches\"):\n            for nm_dict in result[\"non_matches\"]:\n                # Convert enhanced non-match format to NonMatchField format\n                converted_nm = self._convert_enhanced_non_match_to_field(nm_dict)\n                self.non_match_documents.append(NonMatchField(**converted_nm))\n\n        # Process derived metrics explicitly with recall_with_fd parameter\n        if \"confusion_matrix\" in result and \"overall\" in result[\"confusion_matrix\"]:\n            overall_cm = result[\"confusion_matrix\"][\"overall\"]\n\n            # Update derived metrics directly in the result\n            from stickler.structured_object_evaluator.models.metrics_helper import (\n                MetricsHelper,\n            )\n\n            metrics_helper = MetricsHelper()\n\n            # Apply correct recall_with_fd to overall metrics\n            derived_metrics = metrics_helper.calculate_derived_metrics(\n                overall_cm, recall_with_fd=recall_with_fd\n            )\n            result[\"confusion_matrix\"][\"overall\"][\"derived\"] = derived_metrics\n\n            # Copy these to the top-level metrics if needed\n            if \"overall\" in result:\n                result[\"overall\"][\"precision\"] = derived_metrics[\"cm_precision\"]\n                result[\"overall\"][\"recall\"] = derived_metrics[\"cm_recall\"]\n                result[\"overall\"][\"f1\"] = derived_metrics[\"cm_f1\"]\n\n        return result\n\n    def _format_evaluation_results(\n        self, comparison_result: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Format StructuredModel comparison results to match expected evaluator output format.\n\n        Args:\n            comparison_result: Result from StructuredModel.compare_with()\n\n        Returns:\n            Dictionary in the expected evaluator format\n        \"\"\"\n        # Extract components from StructuredModel result\n        field_scores = comparison_result[\"field_scores\"]\n        overall_score = comparison_result[\"overall_score\"]\n        confusion_matrix = comparison_result.get(\"confusion_matrix\", {})\n        non_matches = comparison_result.get(\"non_matches\", [])\n\n        # Calculate field metrics using existing logic for backward compatibility\n        field_metrics = {}\n\n        for field_name, score in field_scores.items():\n            # Convert field score to binary metrics using existing method\n            binary = self._convert_score_to_binary(score)\n            # For field metrics, fd is often not available directly, so we ignore recall_with_fd\n            metrics = self._calculate_metrics_from_binary(\n                binary[\"tp\"], binary[\"fp\"], binary[\"fn\"], binary[\"tn\"]\n            )\n            metrics[\"anls_score\"] = score\n            field_metrics[field_name] = metrics\n\n        # Calculate overall metrics\n        binary = self._convert_score_to_binary(overall_score)\n        # For overall metrics, use confusion_matrix data which should have fd\n        overall_fd = confusion_matrix.get(\"overall\", {}).get(\"fd\", 0)\n        overall_metrics = self._calculate_metrics_from_binary(\n            binary[\"tp\"],\n            binary[\"fp\"],\n            binary[\"fn\"],\n            binary[\"tn\"],\n            fd=overall_fd,\n            recall_with_fd=recall_with_fd,\n        )\n        overall_metrics[\"anls_score\"] = overall_score\n\n        # Add non-matches to evaluator's collection if they exist\n        if non_matches:\n            for nm_dict in non_matches:\n                self.non_match_documents.append(NonMatchField(**nm_dict))\n\n        # Prepare final result in expected format\n        result = {\n            \"overall\": overall_metrics,\n            \"fields\": field_metrics,\n            \"confusion_matrix\": confusion_matrix,\n            \"non_matches\": non_matches,\n        }\n\n        return result\n\n    def _compare_model_lists(\n        self, gt_models: List[StructuredModel], pred_models: List[StructuredModel]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compare two lists of StructuredModel instances using Hungarian matching.\n\n        Args:\n            gt_models: List of ground truth models\n            pred_models: List of predicted models\n\n        Returns:\n            Dict with comparison metrics including tp, fp, fn, overall_score\n        \"\"\"\n        # Handle empty lists\n        if not gt_models and not pred_models:\n            return {\n                \"tp\": 0,\n                \"fp\": 0,\n                \"fn\": 0,\n                \"tn\": 0,\n                \"overall_score\": 1.0,  # Empty lists are a perfect match\n            }\n\n        if not gt_models:\n            return {\n                \"tp\": 0,\n                \"fp\": len(pred_models),\n                \"fn\": 0,\n                \"tn\": 0,\n                \"overall_score\": 0.0,  # All predictions are false positives\n            }\n\n        if not pred_models:\n            return {\n                \"tp\": 0,\n                \"fp\": 0,\n                \"fn\": len(gt_models),\n                \"tn\": 0,\n                \"overall_score\": 0.0,  # All ground truths are false negatives\n            }\n\n        # Ensure all items are StructuredModel instances\n        if not all(\n            isinstance(model, StructuredModel) for model in gt_models + pred_models\n        ):\n            raise TypeError(\"All items in both lists must be StructuredModel instances\")\n\n        # If model_class is specified, check type for all models\n        if self.model_class:\n            if not all(\n                isinstance(model, self.model_class) for model in gt_models + pred_models\n            ):\n                raise TypeError(\n                    f\"All models must be instances of {self.model_class.__name__}\"\n                )\n\n        # Create a Hungarian matcher with StructuredModelComparator\n        hungarian = HungarianMatcher(StructuredModelComparator())\n\n        # Run Hungarian matching\n        tp, fp = hungarian(gt_models, pred_models)\n\n        # Calculate false negatives\n        fn = len(gt_models) - tp\n\n        # Calculate overall score (proportion of correct matches)\n        max_items = max(len(gt_models), len(pred_models))\n        overall_score = tp / max_items if max_items &gt; 0 else 1.0\n\n        return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": 0, \"overall_score\": overall_score}\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.__init__","title":"<code>__init__(model_class=None, threshold=0.5, verbose=False, document_non_matches=True)</code>","text":"<p>Initialize the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Optional[Type[StructuredModel]]</code> <p>Optional StructuredModel class for type checking</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Similarity threshold for considering a match</p> <code>0.5</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed progress information</p> <code>False</code> <code>document_non_matches</code> <code>bool</code> <p>Whether to document detailed non-match information</p> <code>True</code> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def __init__(\n    self,\n    model_class: Optional[Type[StructuredModel]] = None,\n    threshold: float = 0.5,\n    verbose: bool = False,\n    document_non_matches: bool = True,\n):\n    \"\"\"\n    Initialize the evaluator.\n\n    Args:\n        model_class: Optional StructuredModel class for type checking\n        threshold: Similarity threshold for considering a match\n        verbose: Whether to print detailed progress information\n        document_non_matches: Whether to document detailed non-match information\n    \"\"\"\n    self.model_class = model_class\n    self.threshold = threshold\n    self.verbose = verbose\n    self.peak_memory_usage = 0\n    self.start_memory = get_memory_usage()\n\n    # New attributes for documenting non-matches\n    self.document_non_matches = document_non_matches\n    self.non_match_documents: List[NonMatchField] = []\n\n    if self.verbose:\n        print(\n            f\"Initialized StructuredModelEvaluator. Starting memory: {self.start_memory:.2f} MB\"\n        )\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.add_non_match","title":"<code>add_non_match(field_path, non_match_type, gt_value, pred_value, similarity_score=None, details=None)</code>","text":"<p>Document a non-match with detailed information.</p> <p>Parameters:</p> Name Type Description Default <code>field_path</code> <code>str</code> <p>Dot-notation path to the field (e.g., 'address.city')</p> required <code>non_match_type</code> <code>NonMatchType</code> <p>Type of non-match</p> required <code>gt_value</code> <code>Any</code> <p>Ground truth value</p> required <code>pred_value</code> <code>Any</code> <p>Predicted value</p> required <code>similarity_score</code> <code>Optional[float]</code> <p>Optional similarity score if available</p> <code>None</code> <code>details</code> <code>Optional[Dict[str, Any]]</code> <p>Optional additional context or details</p> <code>None</code> <code>document_id</code> <p>Optional ID of the document this non-match belongs to</p> required Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def add_non_match(\n    self,\n    field_path: str,\n    non_match_type: NonMatchType,\n    gt_value: Any,\n    pred_value: Any,\n    similarity_score: Optional[float] = None,\n    details: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"\n    Document a non-match with detailed information.\n\n    Args:\n        field_path: Dot-notation path to the field (e.g., 'address.city')\n        non_match_type: Type of non-match\n        gt_value: Ground truth value\n        pred_value: Predicted value\n        similarity_score: Optional similarity score if available\n        details: Optional additional context or details\n        document_id: Optional ID of the document this non-match belongs to\n    \"\"\"\n    if not self.document_non_matches:\n        return\n\n    self.non_match_documents.append(\n        NonMatchField(\n            field_path=field_path,\n            non_match_type=non_match_type,\n            ground_truth_value=gt_value,\n            prediction_value=pred_value,\n            similarity_score=similarity_score,\n            details=details or {},\n        )\n    )\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.calculate_derived_confusion_matrix_metrics","title":"<code>calculate_derived_confusion_matrix_metrics(cm_counts)</code>","text":"<p>Calculate derived metrics from confusion matrix counts.</p> <p>This method uses MetricsHelper to maintain consistency and avoid code duplication.</p> <p>Parameters:</p> Name Type Description Default <code>cm_counts</code> <code>Dict[str, Union[int, float]]</code> <p>Dictionary with confusion matrix counts containing keys:       'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def calculate_derived_confusion_matrix_metrics(\n    self, cm_counts: Dict[str, Union[int, float]]\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate derived metrics from confusion matrix counts.\n\n    This method uses MetricsHelper to maintain consistency and avoid code duplication.\n\n    Args:\n        cm_counts: Dictionary with confusion matrix counts containing keys:\n                  'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'\n\n    Returns:\n        Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy\n    \"\"\"\n    # Use MetricsHelper for consistent metric calculation\n    from stickler.structured_object_evaluator.models.metrics_helper import (\n        MetricsHelper,\n    )\n\n    metrics_helper = MetricsHelper()\n\n    # Convert counts to the format expected by MetricsHelper\n    metrics_dict = {\n        \"tp\": int(cm_counts.get(\"tp\", 0)),\n        \"fp\": int(cm_counts.get(\"fp\", 0)),\n        \"tn\": int(cm_counts.get(\"tn\", 0)),\n        \"fn\": int(cm_counts.get(\"fn\", 0)),\n        \"fd\": int(cm_counts.get(\"fd\", 0)),\n        \"fa\": int(cm_counts.get(\"fa\", 0)),\n    }\n\n    # Use MetricsHelper to calculate derived metrics\n    return metrics_helper.calculate_derived_metrics(metrics_dict)\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.clear_non_match_documents","title":"<code>clear_non_match_documents()</code>","text":"<p>Clear the stored non-match documents.</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def clear_non_match_documents(self):\n    \"\"\"Clear the stored non-match documents.\"\"\"\n    self.non_match_documents = []\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.combine_cm_dicts","title":"<code>combine_cm_dicts(cm1, cm2)</code>","text":"<p>Combine two confusion matrix dictionaries by adding corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>cm1</code> <code>Dict[str, int]</code> <p>First confusion matrix dictionary</p> required <code>cm2</code> <code>Dict[str, int]</code> <p>Second confusion matrix dictionary</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Combined confusion matrix dictionary</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def combine_cm_dicts(\n    self, cm1: Dict[str, int], cm2: Dict[str, int]\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Combine two confusion matrix dictionaries by adding corresponding values.\n\n    Args:\n        cm1: First confusion matrix dictionary\n        cm2: Second confusion matrix dictionary\n\n    Returns:\n        Combined confusion matrix dictionary\n    \"\"\"\n    return {\n        key: cm1.get(key, 0) + cm2.get(key, 0)\n        for key in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]\n    }\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.evaluate","title":"<code>evaluate(ground_truth, predictions, recall_with_fd=False)</code>","text":"<p>Evaluate predictions against ground truth and return comprehensive metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <code>StructuredModel</code> <p>Ground truth data (StructuredModel instance)</p> required <code>predictions</code> <code>StructuredModel</code> <p>Predicted data (StructuredModel instance)</p> required <code>recall_with_fd</code> <code>bool</code> <p>If True, include FD in recall denominator (TP/(TP+FN+FD))             If False, use traditional recall (TP/(TP+FN))</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with the following structure:</p> <code>Dict[str, Any]</code> <p>{ \"overall\": {     \"precision\": float,     # Overall precision [0-1]     \"recall\": float,        # Overall recall [0-1]     \"f1\": float,           # Overall F1 score [0-1]     \"accuracy\": float,     # Overall accuracy [0-1]     \"anls_score\": float    # Overall ANLS similarity score [0-1] },</p> <p>\"fields\": {     \"\": {         # For primitive fields (str, int, float, bool):         \"precision\": float,         \"recall\": float,         \"f1\": float,         \"accuracy\": float,         \"anls_score\": float     }, <pre><code>\"&lt;list_field_name&gt;\": {\n    # For list fields (e.g., products: List[Product]):\n    \"overall\": {\n        \"precision\": float,\n        \"recall\": float,\n        \"f1\": float,\n        \"accuracy\": float,\n        \"anls_score\": float\n    },\n    \"items\": [\n        # Individual metrics for each matched item pair\n        {\n            \"overall\": {...},  # Item-level overall metrics\n            \"fields\": {        # Field metrics within each item\n                \"&lt;nested_field&gt;\": {...}\n            }\n        }\n    ]\n}\n</code></pre> <p>},</p> <p>\"confusion_matrix\": {     \"fields\": {         # AGGREGATED metrics for all field types         \"\": {             \"tp\": int,          # True positives             \"fp\": int,          # False positives             \"tn\": int,          # True negatives             \"fn\": int,          # False negatives             \"fd\": int,          # False discoveries (non-null but don't match)             \"fa\": int,          # False alarms             \"derived\": {                 \"cm_precision\": float,                 \"cm_recall\": float,                 \"cm_f1\": float,                 \"cm_accuracy\": float             }         }, <pre><code>    # For list fields with nested objects, aggregated field metrics:\n    \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n        # Aggregated counts across ALL instances in the list\n        \"tp\": int,    # Total true positives for this field across all items\n        \"fp\": int,    # Total false positives for this field across all items\n        \"fn\": int,    # Total false negatives for this field across all items\n        \"fd\": int,    # Total false discoveries for this field across all items\n        \"fa\": int,    # Total false alarms for this field across all items\n        \"derived\": {...}\n    }\n},\n\n\"overall\": {\n    # Overall confusion matrix aggregating all fields\n    \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n    \"derived\": {...}\n}\n</code></pre> <p>}</p> <code>Dict[str, Any]</code> <p>}</p> <p>Key Usage Patterns:</p> <ol> <li> <p>Individual Item Metrics (per-instance analysis):    <pre><code># Access metrics for each individual item in a list\nfor i, item_metrics in enumerate(results['fields']['products']['items']):\n    print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n</code></pre></p> </li> <li> <p>Aggregated Field Metrics (recommended for field performance analysis):    <pre><code># Access aggregated metrics across all instances of a field type\ncm_fields = results['confusion_matrix']['fields']\nproduct_id_performance = cm_fields['products.product_id']\nprint(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n# Get all aggregated product field metrics\nproduct_fields = {k: v for k, v in cm_fields.items()\n                if k.startswith('products.')}\n</code></pre></p> </li> <li> <p>Helper Function for Aggregated Metrics:    <pre><code>def get_aggregated_metrics(results, list_field_name):\n    '''Extract aggregated field metrics for a list field.'''\n    cm_fields = results['confusion_matrix']['fields']\n    prefix = f\"{list_field_name}.\"\n    return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n           if k.startswith(prefix)}\n\n# Usage:\nproduct_metrics = get_aggregated_metrics(results, 'products')\nprint(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n</code></pre></p> </li> </ol> Note <ul> <li>Use <code>results['fields'][field]['items']</code> for per-instance analysis</li> <li>Use <code>results['confusion_matrix']['fields'][field.subfield]</code> for aggregated field analysis</li> <li>Aggregated metrics provide rolled-up performance across all instances of a field type</li> <li>Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics</li> </ul> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def evaluate(\n    self,\n    ground_truth: StructuredModel,\n    predictions: StructuredModel,\n    recall_with_fd: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate predictions against ground truth and return comprehensive metrics.\n\n    Args:\n        ground_truth: Ground truth data (StructuredModel instance)\n        predictions: Predicted data (StructuredModel instance)\n        recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                        If False, use traditional recall (TP/(TP+FN))\n\n    Returns:\n        Dictionary with the following structure:\n\n        {\n            \"overall\": {\n                \"precision\": float,     # Overall precision [0-1]\n                \"recall\": float,        # Overall recall [0-1]\n                \"f1\": float,           # Overall F1 score [0-1]\n                \"accuracy\": float,     # Overall accuracy [0-1]\n                \"anls_score\": float    # Overall ANLS similarity score [0-1]\n            },\n\n            \"fields\": {\n                \"&lt;field_name&gt;\": {\n                    # For primitive fields (str, int, float, bool):\n                    \"precision\": float,\n                    \"recall\": float,\n                    \"f1\": float,\n                    \"accuracy\": float,\n                    \"anls_score\": float\n                },\n\n                \"&lt;list_field_name&gt;\": {\n                    # For list fields (e.g., products: List[Product]):\n                    \"overall\": {\n                        \"precision\": float,\n                        \"recall\": float,\n                        \"f1\": float,\n                        \"accuracy\": float,\n                        \"anls_score\": float\n                    },\n                    \"items\": [\n                        # Individual metrics for each matched item pair\n                        {\n                            \"overall\": {...},  # Item-level overall metrics\n                            \"fields\": {        # Field metrics within each item\n                                \"&lt;nested_field&gt;\": {...}\n                            }\n                        }\n                    ]\n                }\n            },\n\n            \"confusion_matrix\": {\n                \"fields\": {\n                    # AGGREGATED metrics for all field types\n                    \"&lt;field_name&gt;\": {\n                        \"tp\": int,          # True positives\n                        \"fp\": int,          # False positives\n                        \"tn\": int,          # True negatives\n                        \"fn\": int,          # False negatives\n                        \"fd\": int,          # False discoveries (non-null but don't match)\n                        \"fa\": int,          # False alarms\n                        \"derived\": {\n                            \"cm_precision\": float,\n                            \"cm_recall\": float,\n                            \"cm_f1\": float,\n                            \"cm_accuracy\": float\n                        }\n                    },\n\n                    # For list fields with nested objects, aggregated field metrics:\n                    \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n                        # Aggregated counts across ALL instances in the list\n                        \"tp\": int,    # Total true positives for this field across all items\n                        \"fp\": int,    # Total false positives for this field across all items\n                        \"fn\": int,    # Total false negatives for this field across all items\n                        \"fd\": int,    # Total false discoveries for this field across all items\n                        \"fa\": int,    # Total false alarms for this field across all items\n                        \"derived\": {...}\n                    }\n                },\n\n                \"overall\": {\n                    # Overall confusion matrix aggregating all fields\n                    \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n                    \"derived\": {...}\n                }\n            }\n        }\n\n    Key Usage Patterns:\n\n    1. **Individual Item Metrics** (per-instance analysis):\n       ```python\n       # Access metrics for each individual item in a list\n       for i, item_metrics in enumerate(results['fields']['products']['items']):\n           print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n       ```\n\n    2. **Aggregated Field Metrics** (recommended for field performance analysis):\n       ```python\n       # Access aggregated metrics across all instances of a field type\n       cm_fields = results['confusion_matrix']['fields']\n       product_id_performance = cm_fields['products.product_id']\n       print(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n       # Get all aggregated product field metrics\n       product_fields = {k: v for k, v in cm_fields.items()\n                       if k.startswith('products.')}\n       ```\n\n    3. **Helper Function for Aggregated Metrics**:\n       ```python\n       def get_aggregated_metrics(results, list_field_name):\n           '''Extract aggregated field metrics for a list field.'''\n           cm_fields = results['confusion_matrix']['fields']\n           prefix = f\"{list_field_name}.\"\n           return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n                  if k.startswith(prefix)}\n\n       # Usage:\n       product_metrics = get_aggregated_metrics(results, 'products')\n       print(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n       ```\n\n    Note:\n        - Use `results['fields'][field]['items']` for per-instance analysis\n        - Use `results['confusion_matrix']['fields'][field.subfield]` for aggregated field analysis\n        - Aggregated metrics provide rolled-up performance across all instances of a field type\n        - Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics\n    \"\"\"\n    # Clear any existing non-match documents\n    self.clear_non_match_documents()\n\n    # Use StructuredModel's enhanced comparison with evaluator format\n    # This pushes all the heavy lifting into the StructuredModel as requested\n    result = ground_truth.compare_with(\n        predictions,\n        include_confusion_matrix=True,\n        document_non_matches=self.document_non_matches,\n        evaluator_format=True,  # This makes StructuredModel return evaluator-compatible format\n        recall_with_fd=recall_with_fd,\n    )\n\n    # Add non-matches to evaluator's collection if they exist\n    if result.get(\"non_matches\"):\n        for nm_dict in result[\"non_matches\"]:\n            # Convert enhanced non-match format to NonMatchField format\n            converted_nm = self._convert_enhanced_non_match_to_field(nm_dict)\n            self.non_match_documents.append(NonMatchField(**converted_nm))\n\n    # Process derived metrics explicitly with recall_with_fd parameter\n    if \"confusion_matrix\" in result and \"overall\" in result[\"confusion_matrix\"]:\n        overall_cm = result[\"confusion_matrix\"][\"overall\"]\n\n        # Update derived metrics directly in the result\n        from stickler.structured_object_evaluator.models.metrics_helper import (\n            MetricsHelper,\n        )\n\n        metrics_helper = MetricsHelper()\n\n        # Apply correct recall_with_fd to overall metrics\n        derived_metrics = metrics_helper.calculate_derived_metrics(\n            overall_cm, recall_with_fd=recall_with_fd\n        )\n        result[\"confusion_matrix\"][\"overall\"][\"derived\"] = derived_metrics\n\n        # Copy these to the top-level metrics if needed\n        if \"overall\" in result:\n            result[\"overall\"][\"precision\"] = derived_metrics[\"cm_precision\"]\n            result[\"overall\"][\"recall\"] = derived_metrics[\"cm_recall\"]\n            result[\"overall\"][\"f1\"] = derived_metrics[\"cm_f1\"]\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.utils.anls_score.compare_structured_models","title":"<code>stickler.structured_object_evaluator.utils.anls_score.compare_structured_models(gt, pred)</code>","text":"<p>Compare a ground truth model with a prediction.</p> <p>This function wraps the compare_with method of StructuredModel for a more explicit API.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>StructuredModel</code> <p>Ground truth model</p> required <code>pred</code> <code>StructuredModel</code> <p>Prediction model</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Comparison result dictionary</p> Source code in <code>stickler/structured_object_evaluator/utils/anls_score.py</code> <pre><code>def compare_structured_models(\n    gt: StructuredModel, pred: StructuredModel\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare a ground truth model with a prediction.\n\n    This function wraps the compare_with method of StructuredModel for\n    a more explicit API.\n\n    Args:\n        gt: Ground truth model\n        pred: Prediction model\n\n    Returns:\n        Comparison result dictionary\n    \"\"\"\n    return gt.compare_with(pred)\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.utils.anls_score.anls_score","title":"<code>stickler.structured_object_evaluator.utils.anls_score.anls_score(gt, pred, return_gt=False, return_key_scores=False)</code>","text":"<p>Calculate ANLS* score between two objects.</p> <p>This function provides a simple API for getting an ANLS* score between any two objects, similar to the original anls_score function.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>Any</code> <p>Ground truth object</p> required <code>pred</code> <code>Any</code> <p>Prediction object</p> required <code>return_gt</code> <code>bool</code> <p>Whether to return the closest ground truth</p> <code>False</code> <code>return_key_scores</code> <code>bool</code> <p>Whether to return detailed key scores</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>Either just the overall score (float), or a tuple with the score and</p> <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>closest ground truth, or a tuple with the score, closest ground truth,</p> <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>and key scores.</p> Source code in <code>stickler/structured_object_evaluator/utils/anls_score.py</code> <pre><code>def anls_score(\n    gt: Any, pred: Any, return_gt: bool = False, return_key_scores: bool = False\n) -&gt; Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]:\n    \"\"\"Calculate ANLS* score between two objects.\n\n    This function provides a simple API for getting an ANLS* score\n    between any two objects, similar to the original anls_score function.\n\n    Args:\n        gt: Ground truth object\n        pred: Prediction object\n        return_gt: Whether to return the closest ground truth\n        return_key_scores: Whether to return detailed key scores\n\n    Returns:\n        Either just the overall score (float), or a tuple with the score and\n        closest ground truth, or a tuple with the score, closest ground truth,\n        and key scores.\n    \"\"\"\n    import warnings\n    from ..trees.base import ANLSTree\n\n    # Store original gt object for possible return\n    original_gt = gt\n\n    # Handle classical QA dataset compatibility\n    gt_is_list_str = isinstance(gt, list) and all(isinstance(x, str) for x in gt)\n    pred_is_str = isinstance(pred, str)\n    if gt_is_list_str and pred_is_str:\n        warnings.warn(\n            \"Treating ground truth as a list of options. This is a compatibility mode for ST-VQA-like datasets.\"\n        )\n        gt = tuple(gt)\n\n    # Create trees from the objects\n    gt_tree = ANLSTree.make_tree(gt, is_gt=True)\n    pred_tree = ANLSTree.make_tree(pred, is_gt=False)\n\n    # Calculate ANLS score\n    score, closest_gt, key_scores = gt_tree.anls(pred_tree)\n\n    # Determine what to return for gt (smart detection)\n    gt_to_return = original_gt if hasattr(original_gt, \"model_dump\") else closest_gt\n\n    # Return the requested information\n    if return_gt and return_key_scores:\n        from .key_scores import construct_nested_dict\n\n        key_scores_dict = construct_nested_dict(key_scores)\n        return score, gt_to_return, key_scores_dict\n    elif return_gt:\n        return score, gt_to_return\n    elif return_key_scores:\n        from .key_scores import construct_nested_dict\n\n        key_scores_dict = construct_nested_dict(key_scores)\n        return score, key_scores_dict\n    else:\n        return score\n</code></pre>"},{"location":"SDK-Docs/api/#stickler.structured_object_evaluator.utils.compare_json.compare_json","title":"<code>stickler.structured_object_evaluator.utils.compare_json.compare_json(gt_json, pred_json, model_cls)</code>","text":"<p>Compare JSON objects using a StructuredModel.</p> <p>This function is a utility for comparing raw JSON objects using a StructuredModel class. It handles missing fields and extra fields gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>gt_json</code> <code>Dict[str, Any]</code> <p>Ground truth JSON</p> required <code>pred_json</code> <code>Dict[str, Any]</code> <p>Prediction JSON</p> required <code>model_cls</code> <code>Type[StructuredModel]</code> <p>StructuredModel class to use for comparison</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with comparison results</p> Source code in <code>stickler/structured_object_evaluator/utils/compare_json.py</code> <pre><code>def compare_json(\n    gt_json: Dict[str, Any], pred_json: Dict[str, Any], model_cls: Type[StructuredModel]\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare JSON objects using a StructuredModel.\n\n    This function is a utility for comparing raw JSON objects using a\n    StructuredModel class. It handles missing fields and extra fields gracefully.\n\n    Args:\n        gt_json: Ground truth JSON\n        pred_json: Prediction JSON\n        model_cls: StructuredModel class to use for comparison\n\n    Returns:\n        Dictionary with comparison results\n    \"\"\"\n    try:\n        # Try to convert both JSONs to structured models\n        gt_model = model_cls.from_json(gt_json)\n        pred_model = model_cls.from_json(pred_json)\n\n        # Compare the models\n        return gt_model.compare_with(pred_model)\n    except Exception as e:\n        # Return error details if conversion fails\n        return {\n            \"error\": str(e),\n            \"overall_score\": 0.0,\n            \"field_scores\": {},\n            \"all_fields_matched\": False,\n        }\n</code></pre>"},{"location":"SDK-Docs/comparators/","title":"Comparators","text":""},{"location":"SDK-Docs/comparators/#stickler.comparators","title":"<code>stickler.comparators</code>","text":"<p>Common comparators for key information evaluation.</p> <p>This package contains comparators that are shared between the traditional and ANLS Star evaluation systems. These comparators implement a unified interface that works with both systems.</p>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator","title":"<code>stickler.comparators.BaseComparator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all comparators.</p> <p>This class defines the interface that all comparators must implement. Comparators are used to compare two values and return a similarity score between 0.0 and 1.0, where 1.0 means the values are identical.</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>class BaseComparator(ABC):\n    \"\"\"Base class for all comparators.\n\n    This class defines the interface that all comparators must implement.\n    Comparators are used to compare two values and return a similarity score\n    between 0.0 and 1.0, where 1.0 means the values are identical.\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.7):\n        \"\"\"Initialize the comparator.\n\n        Args:\n            threshold: Similarity threshold (0.0-1.0)\n        \"\"\"\n        self.threshold = threshold\n\n    @abstractmethod\n    def compare(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Compare two values and return a similarity score.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        pass\n\n    def __call__(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Make the comparator callable.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        return self.compare(str1, str2)\n\n    def binary_compare(self, str1: Any, str2: Any) -&gt; Tuple[int, int]:\n        \"\"\"Compare two values and return a binary result as (tp, fp) tuple.\n\n        This method converts the continuous similarity score to a binary decision\n        based on the threshold. If the similarity is greater than or equal to the\n        threshold, it returns (1, 0) indicating true positive. Otherwise, it returns\n        (0, 1) indicating false positive.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            Tuple of (tp, fp) where tp is 1 if similar, 0 otherwise,\n            and fp is the opposite\n        \"\"\"\n        score = self.compare(str1, str2)\n        if score &gt;= self.threshold:\n            return (1, 0)  # True positive\n        else:\n            return (0, 1)  # False positive\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation for serialization.\"\"\"\n        return self.__class__.__name__\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed string representation.\"\"\"\n        return f\"{self.__class__.__name__}(threshold={self.threshold})\"\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.__call__","title":"<code>__call__(str1, str2)</code>","text":"<p>Make the comparator callable.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>def __call__(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Make the comparator callable.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    return self.compare(str1, str2)\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.__init__","title":"<code>__init__(threshold=0.7)</code>","text":"<p>Initialize the comparator.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold (0.0-1.0)</p> <code>0.7</code> Source code in <code>stickler/comparators/base.py</code> <pre><code>def __init__(self, threshold: float = 0.7):\n    \"\"\"Initialize the comparator.\n\n    Args:\n        threshold: Similarity threshold (0.0-1.0)\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.__repr__","title":"<code>__repr__()</code>","text":"<p>Detailed string representation.</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed string representation.\"\"\"\n    return f\"{self.__class__.__name__}(threshold={self.threshold})\"\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.__str__","title":"<code>__str__()</code>","text":"<p>String representation for serialization.</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation for serialization.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.binary_compare","title":"<code>binary_compare(str1, str2)</code>","text":"<p>Compare two values and return a binary result as (tp, fp) tuple.</p> <p>This method converts the continuous similarity score to a binary decision based on the threshold. If the similarity is greater than or equal to the threshold, it returns (1, 0) indicating true positive. Otherwise, it returns (0, 1) indicating false positive.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (tp, fp) where tp is 1 if similar, 0 otherwise,</p> <code>int</code> <p>and fp is the opposite</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>def binary_compare(self, str1: Any, str2: Any) -&gt; Tuple[int, int]:\n    \"\"\"Compare two values and return a binary result as (tp, fp) tuple.\n\n    This method converts the continuous similarity score to a binary decision\n    based on the threshold. If the similarity is greater than or equal to the\n    threshold, it returns (1, 0) indicating true positive. Otherwise, it returns\n    (0, 1) indicating false positive.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        Tuple of (tp, fp) where tp is 1 if similar, 0 otherwise,\n        and fp is the opposite\n    \"\"\"\n    score = self.compare(str1, str2)\n    if score &gt;= self.threshold:\n        return (1, 0)  # True positive\n    else:\n        return (0, 1)  # False positive\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BaseComparator.compare","title":"<code>compare(str1, str2)</code>  <code>abstractmethod</code>","text":"<p>Compare two values and return a similarity score.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/comparators/base.py</code> <pre><code>@abstractmethod\ndef compare(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Compare two values and return a similarity score.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    pass\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.ExactComparator","title":"<code>stickler.comparators.ExactComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator that checks for exact string matching.</p> <p>This comparator removes whitespace and punctuation before comparison. It returns 1.0 for exact matches and 0.0 otherwise.</p> Example <pre><code>comparator = ExactComparator()\n\n# Returns 1.0 (exact match after normalization)\ncomparator.compare(\"hello, world!\", \"hello world\")\n\n# Returns 0.0 (different strings)\ncomparator.compare(\"hello\", \"goodbye\")\n</code></pre> Source code in <code>stickler/comparators/exact.py</code> <pre><code>class ExactComparator(BaseComparator):\n    \"\"\"Comparator that checks for exact string matching.\n\n    This comparator removes whitespace and punctuation before comparison.\n    It returns 1.0 for exact matches and 0.0 otherwise.\n\n    Example:\n        ```python\n        comparator = ExactComparator()\n\n        # Returns 1.0 (exact match after normalization)\n        comparator.compare(\"hello, world!\", \"hello world\")\n\n        # Returns 0.0 (different strings)\n        comparator.compare(\"hello\", \"goodbye\")\n        ```\n    \"\"\"\n\n    def __init__(self, threshold: float = 1.0, case_sensitive: bool = False):\n        \"\"\"Initialize the comparator.\n\n        Args:\n            threshold: Similarity threshold (default 1.0)\n            case_sensitive: Whether comparison is case sensitive (default False)\n        \"\"\"\n        super().__init__(threshold=threshold)\n        self.case_sensitive = case_sensitive\n\n    def compare(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Compare two values with exact string matching.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            1.0 if the strings match exactly after normalization, 0.0 otherwise\n        \"\"\"\n        if str1 is None and str2 is None:\n            return 1.0\n        if str1 is None or str2 is None:\n            return 0.0\n\n        # Convert to strings if they aren't already\n        str1 = str(str1)\n        str2 = str(str2)\n\n        # Apply case normalization if needed\n        if not self.case_sensitive:\n            str1 = lowercase(str1)\n            str2 = lowercase(str2)\n\n        # Remove whitespace and punctuation\n        normalized1 = strip_punctuation_space(str1)\n        normalized2 = strip_punctuation_space(str2)\n\n        # Compare normalized strings\n        return 1.0 if normalized1 == normalized2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.ExactComparator.__init__","title":"<code>__init__(threshold=1.0, case_sensitive=False)</code>","text":"<p>Initialize the comparator.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold (default 1.0)</p> <code>1.0</code> <code>case_sensitive</code> <code>bool</code> <p>Whether comparison is case sensitive (default False)</p> <code>False</code> Source code in <code>stickler/comparators/exact.py</code> <pre><code>def __init__(self, threshold: float = 1.0, case_sensitive: bool = False):\n    \"\"\"Initialize the comparator.\n\n    Args:\n        threshold: Similarity threshold (default 1.0)\n        case_sensitive: Whether comparison is case sensitive (default False)\n    \"\"\"\n    super().__init__(threshold=threshold)\n    self.case_sensitive = case_sensitive\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.ExactComparator.compare","title":"<code>compare(str1, str2)</code>","text":"<p>Compare two values with exact string matching.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>float</code> <p>1.0 if the strings match exactly after normalization, 0.0 otherwise</p> Source code in <code>stickler/comparators/exact.py</code> <pre><code>def compare(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Compare two values with exact string matching.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        1.0 if the strings match exactly after normalization, 0.0 otherwise\n    \"\"\"\n    if str1 is None and str2 is None:\n        return 1.0\n    if str1 is None or str2 is None:\n        return 0.0\n\n    # Convert to strings if they aren't already\n    str1 = str(str1)\n    str2 = str(str2)\n\n    # Apply case normalization if needed\n    if not self.case_sensitive:\n        str1 = lowercase(str1)\n        str2 = lowercase(str2)\n\n    # Remove whitespace and punctuation\n    normalized1 = strip_punctuation_space(str1)\n    normalized2 = strip_punctuation_space(str2)\n\n    # Compare normalized strings\n    return 1.0 if normalized1 == normalized2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LevenshteinComparator","title":"<code>stickler.comparators.LevenshteinComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator using Levenshtein distance for string similarity.</p> <p>This class implements the Levenshtein distance algorithm for measuring the difference between two strings. It calculates a normalized similarity score between 0 and 1.</p> Source code in <code>stickler/comparators/levenshtein.py</code> <pre><code>class LevenshteinComparator(BaseComparator):\n    \"\"\"Comparator using Levenshtein distance for string similarity.\n\n    This class implements the Levenshtein distance algorithm for measuring\n    the difference between two strings. It calculates a normalized similarity\n    score between 0 and 1.\n    \"\"\"\n\n    def __init__(self, normalize: bool = True, threshold: float = 0.7):\n        \"\"\"Initialize the comparator.\n\n        Args:\n            normalize: Whether to normalize input strings\n                      (strip whitespace, lowercase) before comparison\n            threshold: Similarity threshold (default 0.7)\n        \"\"\"\n        super().__init__(threshold=threshold)\n        self._normalize = normalize\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the name of the comparator.\"\"\"\n        return \"levenshtein\"\n\n    @property\n    def config(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Return configuration parameters.\"\"\"\n        return {\"normalize\": self._normalize}\n\n    def compare(self, s1: Any, s2: Any) -&gt; float:\n        \"\"\"\n        Compare two strings using Levenshtein distance.\n\n        Args:\n            s1: First string or value\n            s2: Second string or value\n\n        Returns:\n            Similarity score between 0.0 and 1.0, with 1.0 indicating identical\n\n        Raises:\n            TypeError: If either input is a dictionary, as dictionaries are not suitable\n                      for Levenshtein distance comparison and should be handled through\n                      structured models instead.\n        \"\"\"\n        # Reject dictionaries - they should be broken down into proper StructuredModel subclasses\n        if isinstance(s1, dict) or isinstance(s2, dict):\n            raise TypeError(\n                \"Dictionary objects cannot be compared using LevenshteinComparator. \"\n                \"Use a StructuredModel subclass with properly defined fields instead.\"\n            )\n\n        # Convert to strings and handle None values\n        s1 = \"\" if s1 is None else str(s1)\n        s2 = \"\" if s2 is None else str(s2)\n\n        # Normalize strings if enabled\n        if self._normalize:\n            s1 = \" \".join(s1.strip().lower().split())\n            s2 = \" \".join(s2.strip().lower().split())\n\n        # Handle empty strings\n        if not s1 and not s2:\n            return 1.0\n\n        # Calculate Levenshtein distance\n        dist = self._levenshtein_distance(s1, s2)\n        str_length = max(len(s1), len(s2))\n\n        if str_length == 0:\n            return 1.0\n\n        # Convert distance to similarity (1.0 - normalized_distance)\n        return 1.0 - (float(dist) / float(str_length))\n\n    @staticmethod\n    def _levenshtein_distance(s1: str, s2: str) -&gt; int:\n        \"\"\"\n        Calculate the Levenshtein distance between two strings.\n\n        Args:\n            s1: First string\n            s2: Second string\n\n        Returns:\n            The Levenshtein distance as an integer\n        \"\"\"\n        if len(s1) &gt; len(s2):\n            s1, s2 = s2, s1\n\n        distances = range(len(s1) + 1)\n        for i2, c2 in enumerate(s2):\n            distances_ = [i2 + 1]\n            for i1, c1 in enumerate(s1):\n                if c1 == c2:\n                    distances_.append(distances[i1])\n                else:\n                    distances_.append(\n                        1 + min((distances[i1], distances[i1 + 1], distances_[-1]))\n                    )\n            distances = distances_\n        return distances[-1]\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LevenshteinComparator.config","title":"<code>config</code>  <code>property</code>","text":"<p>Return configuration parameters.</p>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LevenshteinComparator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the name of the comparator.</p>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LevenshteinComparator.__init__","title":"<code>__init__(normalize=True, threshold=0.7)</code>","text":"<p>Initialize the comparator.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether to normalize input strings       (strip whitespace, lowercase) before comparison</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Similarity threshold (default 0.7)</p> <code>0.7</code> Source code in <code>stickler/comparators/levenshtein.py</code> <pre><code>def __init__(self, normalize: bool = True, threshold: float = 0.7):\n    \"\"\"Initialize the comparator.\n\n    Args:\n        normalize: Whether to normalize input strings\n                  (strip whitespace, lowercase) before comparison\n        threshold: Similarity threshold (default 0.7)\n    \"\"\"\n    super().__init__(threshold=threshold)\n    self._normalize = normalize\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LevenshteinComparator.compare","title":"<code>compare(s1, s2)</code>","text":"<p>Compare two strings using Levenshtein distance.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <code>Any</code> <p>First string or value</p> required <code>s2</code> <code>Any</code> <p>Second string or value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0, with 1.0 indicating identical</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either input is a dictionary, as dictionaries are not suitable       for Levenshtein distance comparison and should be handled through       structured models instead.</p> Source code in <code>stickler/comparators/levenshtein.py</code> <pre><code>def compare(self, s1: Any, s2: Any) -&gt; float:\n    \"\"\"\n    Compare two strings using Levenshtein distance.\n\n    Args:\n        s1: First string or value\n        s2: Second string or value\n\n    Returns:\n        Similarity score between 0.0 and 1.0, with 1.0 indicating identical\n\n    Raises:\n        TypeError: If either input is a dictionary, as dictionaries are not suitable\n                  for Levenshtein distance comparison and should be handled through\n                  structured models instead.\n    \"\"\"\n    # Reject dictionaries - they should be broken down into proper StructuredModel subclasses\n    if isinstance(s1, dict) or isinstance(s2, dict):\n        raise TypeError(\n            \"Dictionary objects cannot be compared using LevenshteinComparator. \"\n            \"Use a StructuredModel subclass with properly defined fields instead.\"\n        )\n\n    # Convert to strings and handle None values\n    s1 = \"\" if s1 is None else str(s1)\n    s2 = \"\" if s2 is None else str(s2)\n\n    # Normalize strings if enabled\n    if self._normalize:\n        s1 = \" \".join(s1.strip().lower().split())\n        s2 = \" \".join(s2.strip().lower().split())\n\n    # Handle empty strings\n    if not s1 and not s2:\n        return 1.0\n\n    # Calculate Levenshtein distance\n    dist = self._levenshtein_distance(s1, s2)\n    str_length = max(len(s1), len(s2))\n\n    if str_length == 0:\n        return 1.0\n\n    # Convert distance to similarity (1.0 - normalized_distance)\n    return 1.0 - (float(dist) / float(str_length))\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.NumericComparator","title":"<code>stickler.comparators.NumericComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator for numeric values with configurable tolerance.</p> <p>This comparator extracts and compares numeric values from strings or numbers. It supports relative and absolute tolerance for comparison.</p> Example <pre><code># Default exact matching\nexact = NumericComparator()\nexact.compare(\"123\", \"123.0\")  # Returns 1.0\nexact.compare(\"123\", \"124\")    # Returns 0.0\n\n# With tolerance\napprox = NumericComparator(relative_tolerance=0.1)  # 10% tolerance\napprox.compare(\"100\", \"109\")   # Returns 1.0 (within 10%)\napprox.compare(\"100\", \"111\")   # Returns 0.0 (beyond 10%)\n</code></pre> Source code in <code>stickler/comparators/numeric.py</code> <pre><code>class NumericComparator(BaseComparator):\n    \"\"\"Comparator for numeric values with configurable tolerance.\n\n    This comparator extracts and compares numeric values from strings or numbers.\n    It supports relative and absolute tolerance for comparison.\n\n    Example:\n        ```python\n        # Default exact matching\n        exact = NumericComparator()\n        exact.compare(\"123\", \"123.0\")  # Returns 1.0\n        exact.compare(\"123\", \"124\")    # Returns 0.0\n\n        # With tolerance\n        approx = NumericComparator(relative_tolerance=0.1)  # 10% tolerance\n        approx.compare(\"100\", \"109\")   # Returns 1.0 (within 10%)\n        approx.compare(\"100\", \"111\")   # Returns 0.0 (beyond 10%)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        threshold: float = 1.0,\n        relative_tolerance: float = 0.0,\n        absolute_tolerance: float = 0.0,\n        tolerance: Optional[float] = None,\n    ):\n        \"\"\"Initialize the comparator.\n\n        Args:\n            threshold: Similarity threshold (default 1.0)\n            relative_tolerance: Relative tolerance for comparison (default 0.0)\n            absolute_tolerance: Absolute tolerance for comparison (default 0.0)\n            tolerance: Alias for absolute_tolerance (for backward compatibility)\n        \"\"\"\n        super().__init__(threshold=threshold)\n        self.relative_tolerance = relative_tolerance\n\n        # Handle tolerance alias for backward compatibility\n        if tolerance is not None:\n            if absolute_tolerance != 0.0:\n                raise ValueError(\n                    \"Cannot specify both 'tolerance' and 'absolute_tolerance'. Use 'absolute_tolerance'.\"\n                )\n            self.absolute_tolerance = tolerance\n        else:\n            self.absolute_tolerance = absolute_tolerance\n\n    def compare(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Compare two values numerically.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            1.0 if the numbers match within tolerance, 0.0 otherwise\n        \"\"\"\n        if str1 is None and str2 is None:\n            return 1.0\n        if str1 is None or str2 is None:\n            return 0.0\n\n        # Extract numeric values\n        num1 = self._extract_number(str1)\n        num2 = self._extract_number(str2)\n\n        if num1 is None or num2 is None:\n            return 0.0\n\n        # Check equality with tolerance\n        if self._numbers_equal(num1, num2):\n            return 1.0\n\n        return 0.0\n\n    def _extract_number(self, value: Any) -&gt; Union[Decimal, None]:\n        \"\"\"Extract a numeric value from a string or number.\n\n        Args:\n            value: Value to extract a number from\n\n        Returns:\n            Decimal value or None if no valid number could be extracted\n        \"\"\"\n        if isinstance(value, (int, float)):\n            return Decimal(str(value))\n\n        if not isinstance(value, str):\n            value = str(value)\n\n        # Check for accounting notation: (123) means -123\n        is_negative = False\n        if value.startswith(\"(\") and value.endswith(\")\"):\n            value = value[1:-1]  # Remove the parentheses\n            is_negative = True\n\n        # Remove common currency symbols and other non-numeric characters\n        value = re.sub(r\"[^0-9.-]\", \"\", value)\n\n        # Handle empty string\n        if not value:\n            return None\n\n        # Try to convert to Decimal\n        try:\n            decimal_value = Decimal(value)\n            # Apply negative sign if accounting notation was used\n            if is_negative:\n                decimal_value = -decimal_value\n            return decimal_value\n        except InvalidOperation:\n            return None\n\n    def _numbers_equal(self, num1: Decimal, num2: Decimal) -&gt; bool:\n        \"\"\"Check if two numbers are equal within tolerance.\n\n        Args:\n            num1: First number\n            num2: Second number\n\n        Returns:\n            True if numbers are equal within tolerance, False otherwise\n        \"\"\"\n        if num1 == num2:\n            return True\n\n        # Check with relative tolerance\n        if self.relative_tolerance &gt; 0:\n            # Handle zero case\n            if num1 == 0:\n                return abs(num2) &lt;= self.relative_tolerance\n\n            # Calculate relative difference using num1 as base\n            relative_diff = abs(num1 - num2) / abs(num1)\n            if relative_diff &lt;= self.relative_tolerance:\n                return True\n\n        # Check with absolute tolerance\n        if self.absolute_tolerance &gt; 0:\n            if abs(num1 - num2) &lt;= self.absolute_tolerance:\n                return True\n\n        return False\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.NumericComparator.__init__","title":"<code>__init__(threshold=1.0, relative_tolerance=0.0, absolute_tolerance=0.0, tolerance=None)</code>","text":"<p>Initialize the comparator.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold (default 1.0)</p> <code>1.0</code> <code>relative_tolerance</code> <code>float</code> <p>Relative tolerance for comparison (default 0.0)</p> <code>0.0</code> <code>absolute_tolerance</code> <code>float</code> <p>Absolute tolerance for comparison (default 0.0)</p> <code>0.0</code> <code>tolerance</code> <code>Optional[float]</code> <p>Alias for absolute_tolerance (for backward compatibility)</p> <code>None</code> Source code in <code>stickler/comparators/numeric.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 1.0,\n    relative_tolerance: float = 0.0,\n    absolute_tolerance: float = 0.0,\n    tolerance: Optional[float] = None,\n):\n    \"\"\"Initialize the comparator.\n\n    Args:\n        threshold: Similarity threshold (default 1.0)\n        relative_tolerance: Relative tolerance for comparison (default 0.0)\n        absolute_tolerance: Absolute tolerance for comparison (default 0.0)\n        tolerance: Alias for absolute_tolerance (for backward compatibility)\n    \"\"\"\n    super().__init__(threshold=threshold)\n    self.relative_tolerance = relative_tolerance\n\n    # Handle tolerance alias for backward compatibility\n    if tolerance is not None:\n        if absolute_tolerance != 0.0:\n            raise ValueError(\n                \"Cannot specify both 'tolerance' and 'absolute_tolerance'. Use 'absolute_tolerance'.\"\n            )\n        self.absolute_tolerance = tolerance\n    else:\n        self.absolute_tolerance = absolute_tolerance\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.NumericComparator.compare","title":"<code>compare(str1, str2)</code>","text":"<p>Compare two values numerically.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>float</code> <p>1.0 if the numbers match within tolerance, 0.0 otherwise</p> Source code in <code>stickler/comparators/numeric.py</code> <pre><code>def compare(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Compare two values numerically.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        1.0 if the numbers match within tolerance, 0.0 otherwise\n    \"\"\"\n    if str1 is None and str2 is None:\n        return 1.0\n    if str1 is None or str2 is None:\n        return 0.0\n\n    # Extract numeric values\n    num1 = self._extract_number(str1)\n    num2 = self._extract_number(str2)\n\n    if num1 is None or num2 is None:\n        return 0.0\n\n    # Check equality with tolerance\n    if self._numbers_equal(num1, num2):\n        return 1.0\n\n    return 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.NumericExactC","title":"<code>stickler.comparators.NumericExactC = NumericComparator</code>  <code>module-attribute</code>","text":""},{"location":"SDK-Docs/comparators/#stickler.comparators.FuzzyComparator","title":"<code>stickler.comparators.FuzzyComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator for fuzzy string matching.</p> <p>This comparator uses the rapidfuzz library to calculate similarity between strings using advanced Levenshtein distance calculations. It provides better fuzzy matching than basic Levenshtein for many use cases.</p> <p>If rapidfuzz is not available, this will raise an ImportError when instantiated.</p> Source code in <code>stickler/comparators/fuzzy.py</code> <pre><code>class FuzzyComparator(BaseComparator):\n    \"\"\"Comparator for fuzzy string matching.\n\n    This comparator uses the rapidfuzz library to calculate similarity between\n    strings using advanced Levenshtein distance calculations. It provides better\n    fuzzy matching than basic Levenshtein for many use cases.\n\n    If rapidfuzz is not available, this will raise an ImportError when instantiated.\n    \"\"\"\n\n    def __init__(\n        self, method: str = \"ratio\", normalize: bool = True, threshold: float = 0.7\n    ):\n        \"\"\"Initialize the fuzzy comparator.\n\n        Args:\n            method: The fuzzy matching method to use. Options:\n                - \"ratio\": Standard Levenshtein distance ratio\n                - \"partial_ratio\": Partial string matching\n                - \"token_sort_ratio\": Token-based matching with sorting\n                - \"token_set_ratio\": Token-based matching with set operations\n            normalize: Whether to normalize input strings before comparison\n                      (strip whitespace, lowercase)\n            threshold: Similarity threshold (default 0.7)\n\n        Raises:\n            ImportError: If rapidfuzz library is not available\n        \"\"\"\n        super().__init__(threshold=threshold)\n\n        if not RAPIDFUZZ_AVAILABLE:\n            raise ImportError(\n                \"The rapidfuzz library is required for FuzzyComparator. \"\n                \"Install it with: pip install rapidfuzz\"\n            )\n\n        self._method = method\n        self._normalize = normalize\n\n        # Select the appropriate fuzzy matching function\n        self._fuzzy_func = {\n            \"ratio\": fuzz.ratio,\n            \"partial_ratio\": fuzz.partial_ratio,\n            \"token_sort_ratio\": fuzz.token_sort_ratio,\n            \"token_set_ratio\": fuzz.token_set_ratio,\n        }.get(method, fuzz.ratio)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the name of the comparator.\"\"\"\n        return f\"fuzzy_{self._method}\"\n\n    @property\n    def config(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Return configuration parameters.\"\"\"\n        return {\"method\": self._method, \"normalize\": self._normalize}\n\n    def compare(self, value1: Any, value2: Any) -&gt; float:\n        \"\"\"Compare two strings using fuzzy matching.\n\n        Args:\n            value1: First string or value\n            value2: Second string or value\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        # Handle None values\n        if value1 is None and value2 is None:\n            return 1.0\n        elif value1 is None or value2 is None:\n            return 0.0\n\n        # Convert to strings\n        s1 = str(value1)\n        s2 = str(value2)\n\n        # Normalize if enabled\n        if self._normalize:\n            s1 = s1.strip().lower()\n            s2 = s2.strip().lower()\n\n        # Calculate fuzzy match score and normalize to 0.0-1.0\n        if s1 == \"\" and s2 == \"\":\n            return 1.0\n\n        # Use the selected fuzzy matching function\n        try:\n            return self._fuzzy_func(s1, s2) / 100.0\n        except Exception:\n            # Fall back to basic comparison if fuzzy match fails\n            return 1.0 if s1 == s2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.FuzzyComparator.config","title":"<code>config</code>  <code>property</code>","text":"<p>Return configuration parameters.</p>"},{"location":"SDK-Docs/comparators/#stickler.comparators.FuzzyComparator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the name of the comparator.</p>"},{"location":"SDK-Docs/comparators/#stickler.comparators.FuzzyComparator.__init__","title":"<code>__init__(method='ratio', normalize=True, threshold=0.7)</code>","text":"<p>Initialize the fuzzy comparator.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The fuzzy matching method to use. Options: - \"ratio\": Standard Levenshtein distance ratio - \"partial_ratio\": Partial string matching - \"token_sort_ratio\": Token-based matching with sorting - \"token_set_ratio\": Token-based matching with set operations</p> <code>'ratio'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize input strings before comparison       (strip whitespace, lowercase)</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Similarity threshold (default 0.7)</p> <code>0.7</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If rapidfuzz library is not available</p> Source code in <code>stickler/comparators/fuzzy.py</code> <pre><code>def __init__(\n    self, method: str = \"ratio\", normalize: bool = True, threshold: float = 0.7\n):\n    \"\"\"Initialize the fuzzy comparator.\n\n    Args:\n        method: The fuzzy matching method to use. Options:\n            - \"ratio\": Standard Levenshtein distance ratio\n            - \"partial_ratio\": Partial string matching\n            - \"token_sort_ratio\": Token-based matching with sorting\n            - \"token_set_ratio\": Token-based matching with set operations\n        normalize: Whether to normalize input strings before comparison\n                  (strip whitespace, lowercase)\n        threshold: Similarity threshold (default 0.7)\n\n    Raises:\n        ImportError: If rapidfuzz library is not available\n    \"\"\"\n    super().__init__(threshold=threshold)\n\n    if not RAPIDFUZZ_AVAILABLE:\n        raise ImportError(\n            \"The rapidfuzz library is required for FuzzyComparator. \"\n            \"Install it with: pip install rapidfuzz\"\n        )\n\n    self._method = method\n    self._normalize = normalize\n\n    # Select the appropriate fuzzy matching function\n    self._fuzzy_func = {\n        \"ratio\": fuzz.ratio,\n        \"partial_ratio\": fuzz.partial_ratio,\n        \"token_sort_ratio\": fuzz.token_sort_ratio,\n        \"token_set_ratio\": fuzz.token_set_ratio,\n    }.get(method, fuzz.ratio)\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.FuzzyComparator.compare","title":"<code>compare(value1, value2)</code>","text":"<p>Compare two strings using fuzzy matching.</p> <p>Parameters:</p> Name Type Description Default <code>value1</code> <code>Any</code> <p>First string or value</p> required <code>value2</code> <code>Any</code> <p>Second string or value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/comparators/fuzzy.py</code> <pre><code>def compare(self, value1: Any, value2: Any) -&gt; float:\n    \"\"\"Compare two strings using fuzzy matching.\n\n    Args:\n        value1: First string or value\n        value2: Second string or value\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    # Handle None values\n    if value1 is None and value2 is None:\n        return 1.0\n    elif value1 is None or value2 is None:\n        return 0.0\n\n    # Convert to strings\n    s1 = str(value1)\n    s2 = str(value2)\n\n    # Normalize if enabled\n    if self._normalize:\n        s1 = s1.strip().lower()\n        s2 = s2.strip().lower()\n\n    # Calculate fuzzy match score and normalize to 0.0-1.0\n    if s1 == \"\" and s2 == \"\":\n        return 1.0\n\n    # Use the selected fuzzy matching function\n    try:\n        return self._fuzzy_func(s1, s2) / 100.0\n    except Exception:\n        # Fall back to basic comparison if fuzzy match fails\n        return 1.0 if s1 == s2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BERTComparator","title":"<code>stickler.comparators.BERTComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator that uses BERT embeddings for semantic similarity.</p> <p>This comparator uses the BERTScore metric to calculate semantic similarity between strings, returning the f1 score as the similarity measure.</p> Example <pre><code>comparator = BERTComparator(threshold=0.8)\n\n# Returns similarity score based on semantic similarity\nscore = comparator.compare(\"The cat sat on the mat\", \"A feline was sitting on a rug\")\n</code></pre> Source code in <code>stickler/comparators/bert.py</code> <pre><code>class BERTComparator(BaseComparator):\n    \"\"\"Comparator that uses BERT embeddings for semantic similarity.\n\n    This comparator uses the BERTScore metric to calculate semantic similarity\n    between strings, returning the f1 score as the similarity measure.\n\n    Example:\n        ```python\n        comparator = BERTComparator(threshold=0.8)\n\n        # Returns similarity score based on semantic similarity\n        score = comparator.compare(\"The cat sat on the mat\", \"A feline was sitting on a rug\")\n        ```\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.7):\n        \"\"\"Initialize the BERTComparator.\n\n        Args:\n            threshold: Similarity threshold (0.0-1.0)\n        \"\"\"\n        super().__init__(threshold=threshold)\n        if model is None:\n            raise ImportError(\n                \"BERTScore model could not be loaded. Please install 'evaluate' package.\"\n            )\n\n    def compare(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Compare two strings using BERT semantic similarity.\n\n        Args:\n            str1: First string\n            str2: Second string\n\n        Returns:\n            Similarity score between 0.0 and 1.0 based on BERTScore f1\n        \"\"\"\n        if str1 is None or str2 is None:\n            return 0.0\n\n        # Convert to strings if they aren't already\n        str1 = str(str1)\n        str2 = str(str2)\n\n        # Strip punctuation and whitespace\n        str1_clean = strip_punctuation_space(str1)\n        str2_clean = strip_punctuation_space(str2)\n\n        # Handle empty strings\n        if not str1_clean or not str2_clean:\n            return 1.0 if str1_clean == str2_clean else 0.0\n\n        try:\n            # Calculate BERT score\n            result = model.compute(\n                predictions=[str1_clean], references=[str2_clean], lang=\"en\"\n            )\n\n            # Return f1 score\n            return result[\"f1\"][0]\n        except Exception as e:\n            # Fallback to direct comparison\n            print(f\"BERT comparison error: {str(e)}\")\n            return 1.0 if str1_clean == str2_clean else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BERTComparator.__init__","title":"<code>__init__(threshold=0.7)</code>","text":"<p>Initialize the BERTComparator.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold (0.0-1.0)</p> <code>0.7</code> Source code in <code>stickler/comparators/bert.py</code> <pre><code>def __init__(self, threshold: float = 0.7):\n    \"\"\"Initialize the BERTComparator.\n\n    Args:\n        threshold: Similarity threshold (0.0-1.0)\n    \"\"\"\n    super().__init__(threshold=threshold)\n    if model is None:\n        raise ImportError(\n            \"BERTScore model could not be loaded. Please install 'evaluate' package.\"\n        )\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.BERTComparator.compare","title":"<code>compare(str1, str2)</code>","text":"<p>Compare two strings using BERT semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First string</p> required <code>str2</code> <code>Any</code> <p>Second string</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0 based on BERTScore f1</p> Source code in <code>stickler/comparators/bert.py</code> <pre><code>def compare(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Compare two strings using BERT semantic similarity.\n\n    Args:\n        str1: First string\n        str2: Second string\n\n    Returns:\n        Similarity score between 0.0 and 1.0 based on BERTScore f1\n    \"\"\"\n    if str1 is None or str2 is None:\n        return 0.0\n\n    # Convert to strings if they aren't already\n    str1 = str(str1)\n    str2 = str(str2)\n\n    # Strip punctuation and whitespace\n    str1_clean = strip_punctuation_space(str1)\n    str2_clean = strip_punctuation_space(str2)\n\n    # Handle empty strings\n    if not str1_clean or not str2_clean:\n        return 1.0 if str1_clean == str2_clean else 0.0\n\n    try:\n        # Calculate BERT score\n        result = model.compute(\n            predictions=[str1_clean], references=[str2_clean], lang=\"en\"\n        )\n\n        # Return f1 score\n        return result[\"f1\"][0]\n    except Exception as e:\n        # Fallback to direct comparison\n        print(f\"BERT comparison error: {str(e)}\")\n        return 1.0 if str1_clean == str2_clean else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.SemanticComparator","title":"<code>stickler.comparators.SemanticComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator that uses embeddings for semantic similarity.</p> <p>This comparator uses embeddings from a model (default: Titan) to calculate semantic similarity between strings.</p> <p>Attributes:</p> Name Type Description <code>SIMILARITY_FUNCTIONS</code> <p>Dictionary of similarity functions</p> <code>bc</code> <p>BedrockClient instance</p> <code>model_id</code> <p>Model ID to use for embeddings</p> <code>embedding_function</code> <p>Function to generate embeddings</p> <code>sim_function</code> <p>Name of the similarity function to use</p> <code>similarity_function</code> <p>The actual similarity function</p> Source code in <code>stickler/comparators/semantic.py</code> <pre><code>class SemanticComparator(BaseComparator):\n    \"\"\"Comparator that uses embeddings for semantic similarity.\n\n    This comparator uses embeddings from a model (default: Titan) to calculate\n    semantic similarity between strings.\n\n    Attributes:\n        SIMILARITY_FUNCTIONS: Dictionary of similarity functions\n        bc: BedrockClient instance\n        model_id: Model ID to use for embeddings\n        embedding_function: Function to generate embeddings\n        sim_function: Name of the similarity function to use\n        similarity_function: The actual similarity function\n    \"\"\"\n\n    SIMILARITY_FUNCTIONS = {\n        \"cosine_similarity\": lambda x, y: 1 - spatial.distance.cosine(x, y)\n    }\n\n    def __init__(\n        self,\n        model_id: str = \"amazon.titan-embed-text-v2:0\",\n        sim_function: str = \"cosine_similarity\",\n        embedding_function: Optional[Callable] = None,\n        threshold: float = 0.7,\n    ):\n        \"\"\"Initialize the SemanticComparator.\n\n        Args:\n            model_id: Model ID to use for embeddings\n            sim_function: Name of the similarity function to use\n            embedding_function: Optional custom embedding function\n            threshold: Similarity threshold (0.0-1.0)\n\n        Raises:\n            ImportError: If BedrockClient is not available and no embedding_function is provided\n        \"\"\"\n        super().__init__(threshold=threshold)\n\n        if embedding_function is not None:\n            self.embedding_function = embedding_function\n        else:\n            self.model_id = (model_id,)\n            self.embedding_function = partial(\n                generate_bedrock_embedding, model_id=model_id\n            )\n\n        self.sim_function = sim_function\n        self.similarity_function = self.SIMILARITY_FUNCTIONS[self.sim_function]\n\n    def compare(self, str1: str, str2: str) -&gt; float:\n        \"\"\"Compare two strings using semantic similarity.\n\n        Args:\n            str1: First string\n            str2: Second string\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        if str1 is None or str2 is None:\n            return 0.0\n\n        try:\n            x, y = self.embedding_function(str1), self.embedding_function(str2)\n            return self.similarity_function(x, y)\n        except Exception:\n            # Fallback to string equality if embedding fails\n            return 1.0 if str1 == str2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.SemanticComparator.__init__","title":"<code>__init__(model_id='amazon.titan-embed-text-v2:0', sim_function='cosine_similarity', embedding_function=None, threshold=0.7)</code>","text":"<p>Initialize the SemanticComparator.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model ID to use for embeddings</p> <code>'amazon.titan-embed-text-v2:0'</code> <code>sim_function</code> <code>str</code> <p>Name of the similarity function to use</p> <code>'cosine_similarity'</code> <code>embedding_function</code> <code>Optional[Callable]</code> <p>Optional custom embedding function</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Similarity threshold (0.0-1.0)</p> <code>0.7</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If BedrockClient is not available and no embedding_function is provided</p> Source code in <code>stickler/comparators/semantic.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"amazon.titan-embed-text-v2:0\",\n    sim_function: str = \"cosine_similarity\",\n    embedding_function: Optional[Callable] = None,\n    threshold: float = 0.7,\n):\n    \"\"\"Initialize the SemanticComparator.\n\n    Args:\n        model_id: Model ID to use for embeddings\n        sim_function: Name of the similarity function to use\n        embedding_function: Optional custom embedding function\n        threshold: Similarity threshold (0.0-1.0)\n\n    Raises:\n        ImportError: If BedrockClient is not available and no embedding_function is provided\n    \"\"\"\n    super().__init__(threshold=threshold)\n\n    if embedding_function is not None:\n        self.embedding_function = embedding_function\n    else:\n        self.model_id = (model_id,)\n        self.embedding_function = partial(\n            generate_bedrock_embedding, model_id=model_id\n        )\n\n    self.sim_function = sim_function\n    self.similarity_function = self.SIMILARITY_FUNCTIONS[self.sim_function]\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.SemanticComparator.compare","title":"<code>compare(str1, str2)</code>","text":"<p>Compare two strings using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>str</code> <p>First string</p> required <code>str2</code> <code>str</code> <p>Second string</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/comparators/semantic.py</code> <pre><code>def compare(self, str1: str, str2: str) -&gt; float:\n    \"\"\"Compare two strings using semantic similarity.\n\n    Args:\n        str1: First string\n        str2: Second string\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    if str1 is None or str2 is None:\n        return 0.0\n\n    try:\n        x, y = self.embedding_function(str1), self.embedding_function(str2)\n        return self.similarity_function(x, y)\n    except Exception:\n        # Fallback to string equality if embedding fails\n        return 1.0 if str1 == str2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LLMComparator","title":"<code>stickler.comparators.LLMComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator that uses LLM to determine semantic equivalence.</p> <p>This comparator uses an LLM to determine if two values are semantically equivalent, returning 1.0 if True and 0.0 if False.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <p>Prompt template to use for comparison</p> <code>model_id</code> <p>Model ID to use for LLM</p> <code>temp</code> <p>Temperature for LLM inference</p> <code>system_prompt</code> <p>System prompt for the LLM</p> Source code in <code>stickler/comparators/llm.py</code> <pre><code>class LLMComparator(BaseComparator):\n    \"\"\"Comparator that uses LLM to determine semantic equivalence.\n\n    This comparator uses an LLM to determine if two values are semantically\n    equivalent, returning 1.0 if True and 0.0 if False.\n\n    Attributes:\n        prompt: Prompt template to use for comparison\n        model_id: Model ID to use for LLM\n        temp: Temperature for LLM inference\n        system_prompt: System prompt for the LLM\n    \"\"\"\n\n    def __init__(\n        self, prompt: str, model_id: str, temp: float = 0.5, threshold: float = 0.5\n    ):\n        \"\"\"Initialize the LLMComparator.\n\n        Args:\n            prompt: Prompt template to use for comparison\n            model_id: Model ID to use for LLM\n            temp: Temperature for LLM inference\n            threshold: Similarity threshold (0.0-1.0)\n        \"\"\"\n        super().__init__(threshold=threshold)\n        self.prompt = prompt\n        self.temp = temp\n        self.model_id = model_id\n        self.system_prompt = \"You are an evaluation assistant. Carefully decide if the two values are same or not. Respond only with 'TRUE' or 'FALSE', nothing else.\"\n\n    def compare(self, str1: Any, str2: Any) -&gt; float:\n        \"\"\"Compare two values using LLM.\n\n        Args:\n            str1: First value\n            str2: Second value\n\n        Returns:\n            1.0 if LLM determines values are equivalent, 0.0 otherwise\n        \"\"\"\n        if str1 is None or str2 is None:\n            return 0.0\n        raise Exception(\"This implementation is not working yet!\")\n        ci = ClaudeInvoker(\n            self.prompt,\n            self.model_id,\n            system_prompt=self.system_prompt,\n            temperature=self.temp,\n        )\n\n        kwargs = {\"value1\": str1, \"value2\": str2}\n\n        try:\n            response = ci.inference(kwargs)\n        except Exception as e:\n            print(f\"LLM error: {str(e)}\")\n            sleep(2)\n            response = ci.inference(kwargs)\n\n        result = response == \"TRUE\"\n        if result:\n            print(\n                \"WARNING: LLM evaluation returned True. Please refine the prompt or review the result.\"\n            )\n            return 1.0\n        else:\n            return 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LLMComparator.__init__","title":"<code>__init__(prompt, model_id, temp=0.5, threshold=0.5)</code>","text":"<p>Initialize the LLMComparator.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt template to use for comparison</p> required <code>model_id</code> <code>str</code> <p>Model ID to use for LLM</p> required <code>temp</code> <code>float</code> <p>Temperature for LLM inference</p> <code>0.5</code> <code>threshold</code> <code>float</code> <p>Similarity threshold (0.0-1.0)</p> <code>0.5</code> Source code in <code>stickler/comparators/llm.py</code> <pre><code>def __init__(\n    self, prompt: str, model_id: str, temp: float = 0.5, threshold: float = 0.5\n):\n    \"\"\"Initialize the LLMComparator.\n\n    Args:\n        prompt: Prompt template to use for comparison\n        model_id: Model ID to use for LLM\n        temp: Temperature for LLM inference\n        threshold: Similarity threshold (0.0-1.0)\n    \"\"\"\n    super().__init__(threshold=threshold)\n    self.prompt = prompt\n    self.temp = temp\n    self.model_id = model_id\n    self.system_prompt = \"You are an evaluation assistant. Carefully decide if the two values are same or not. Respond only with 'TRUE' or 'FALSE', nothing else.\"\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.LLMComparator.compare","title":"<code>compare(str1, str2)</code>","text":"<p>Compare two values using LLM.</p> <p>Parameters:</p> Name Type Description Default <code>str1</code> <code>Any</code> <p>First value</p> required <code>str2</code> <code>Any</code> <p>Second value</p> required <p>Returns:</p> Type Description <code>float</code> <p>1.0 if LLM determines values are equivalent, 0.0 otherwise</p> Source code in <code>stickler/comparators/llm.py</code> <pre><code>def compare(self, str1: Any, str2: Any) -&gt; float:\n    \"\"\"Compare two values using LLM.\n\n    Args:\n        str1: First value\n        str2: Second value\n\n    Returns:\n        1.0 if LLM determines values are equivalent, 0.0 otherwise\n    \"\"\"\n    if str1 is None or str2 is None:\n        return 0.0\n    raise Exception(\"This implementation is not working yet!\")\n    ci = ClaudeInvoker(\n        self.prompt,\n        self.model_id,\n        system_prompt=self.system_prompt,\n        temperature=self.temp,\n    )\n\n    kwargs = {\"value1\": str1, \"value2\": str2}\n\n    try:\n        response = ci.inference(kwargs)\n    except Exception as e:\n        print(f\"LLM error: {str(e)}\")\n        sleep(2)\n        response = ci.inference(kwargs)\n\n    result = response == \"TRUE\"\n    if result:\n        print(\n            \"WARNING: LLM evaluation returned True. Please refine the prompt or review the result.\"\n        )\n        return 1.0\n    else:\n        return 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.StructuredModelComparator","title":"<code>stickler.comparators.StructuredModelComparator</code>","text":"<p>               Bases: <code>BaseComparator</code></p> <p>Comparator for structured model objects.</p> <p>This comparator is designed to work with StructuredModel instances, leveraging their built-in comparison capabilities.</p> Source code in <code>stickler/comparators/structured.py</code> <pre><code>class StructuredModelComparator(BaseComparator):\n    \"\"\"Comparator for structured model objects.\n\n    This comparator is designed to work with StructuredModel instances,\n    leveraging their built-in comparison capabilities.\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.7, strict_types: bool = False):\n        \"\"\"Initialize the comparator.\n\n        Args:\n            threshold: Similarity threshold (0.0-1.0)\n            strict_types: If True, will raise TypeError when non-StructuredModel objects are compared\n        \"\"\"\n        super().__init__(threshold)\n        self.strict_types = strict_types\n\n    def compare(self, model1: Any, model2: Any) -&gt; float:\n        \"\"\"Compare two structured model instances.\n\n        This method uses the built-in compare method of StructuredModel objects\n        if available, otherwise falls back to basic equality comparison.\n\n        Args:\n            model1: First model (ideally a StructuredModel instance)\n            model2: Second model (ideally a StructuredModel instance)\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n\n        Raises:\n            TypeError: When strict_types=True and comparing non-StructuredModel objects\n        \"\"\"\n        # In strict mode, enforce StructuredModel types (used in tests)\n        # For string values, always raise TypeError in strict mode\n        if self.strict_types and isinstance(model1, str) and isinstance(model2, str):\n            raise TypeError(\n                \"StructuredModelComparator can only compare StructuredModel instances\"\n            )\n\n        # Handle None values\n        if model1 is None or model2 is None:\n            return 1.0 if model1 == model2 else 0.0\n\n        # Check if both objects have a compare method (duck typing)\n        if hasattr(model1, \"compare\") and callable(model1.compare):\n            return model1.compare(model2)\n\n        # Fall back to equality check for non-StructuredModel objects\n        return 1.0 if model1 == model2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.StructuredModelComparator.__init__","title":"<code>__init__(threshold=0.7, strict_types=False)</code>","text":"<p>Initialize the comparator.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold (0.0-1.0)</p> <code>0.7</code> <code>strict_types</code> <code>bool</code> <p>If True, will raise TypeError when non-StructuredModel objects are compared</p> <code>False</code> Source code in <code>stickler/comparators/structured.py</code> <pre><code>def __init__(self, threshold: float = 0.7, strict_types: bool = False):\n    \"\"\"Initialize the comparator.\n\n    Args:\n        threshold: Similarity threshold (0.0-1.0)\n        strict_types: If True, will raise TypeError when non-StructuredModel objects are compared\n    \"\"\"\n    super().__init__(threshold)\n    self.strict_types = strict_types\n</code></pre>"},{"location":"SDK-Docs/comparators/#stickler.comparators.StructuredModelComparator.compare","title":"<code>compare(model1, model2)</code>","text":"<p>Compare two structured model instances.</p> <p>This method uses the built-in compare method of StructuredModel objects if available, otherwise falls back to basic equality comparison.</p> <p>Parameters:</p> Name Type Description Default <code>model1</code> <code>Any</code> <p>First model (ideally a StructuredModel instance)</p> required <code>model2</code> <code>Any</code> <p>Second model (ideally a StructuredModel instance)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>When strict_types=True and comparing non-StructuredModel objects</p> Source code in <code>stickler/comparators/structured.py</code> <pre><code>def compare(self, model1: Any, model2: Any) -&gt; float:\n    \"\"\"Compare two structured model instances.\n\n    This method uses the built-in compare method of StructuredModel objects\n    if available, otherwise falls back to basic equality comparison.\n\n    Args:\n        model1: First model (ideally a StructuredModel instance)\n        model2: Second model (ideally a StructuredModel instance)\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n\n    Raises:\n        TypeError: When strict_types=True and comparing non-StructuredModel objects\n    \"\"\"\n    # In strict mode, enforce StructuredModel types (used in tests)\n    # For string values, always raise TypeError in strict mode\n    if self.strict_types and isinstance(model1, str) and isinstance(model2, str):\n        raise TypeError(\n            \"StructuredModelComparator can only compare StructuredModel instances\"\n        )\n\n    # Handle None values\n    if model1 is None or model2 is None:\n        return 1.0 if model1 == model2 else 0.0\n\n    # Check if both objects have a compare method (duck typing)\n    if hasattr(model1, \"compare\") and callable(model1.compare):\n        return model1.compare(model2)\n\n    # Fall back to equality check for non-StructuredModel objects\n    return 1.0 if model1 == model2 else 0.0\n</code></pre>"},{"location":"SDK-Docs/evaluator/","title":"Evaluator","text":""},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator","title":"<code>stickler.structured_object_evaluator.evaluator</code>","text":"<p>Evaluator for StructuredModel objects.</p> <p>This module provides an evaluator class for computing metrics on StructuredModel objects, leveraging their built-in comparison capabilities to generate comprehensive metrics. It also supports documenting non-matches (false positives, false negatives) for detailed analysis.</p>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator","title":"<code>stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator</code>","text":"<p>Evaluator for StructuredModel objects.</p> <p>This evaluator computes comprehensive metrics for StructuredModel objects, leveraging their built-in comparison capabilities. It includes confusion matrix calculations, field-level metrics, non-match documentation, and memory optimization capabilities.</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>class StructuredModelEvaluator:\n    \"\"\"\n    Evaluator for StructuredModel objects.\n\n    This evaluator computes comprehensive metrics for StructuredModel objects,\n    leveraging their built-in comparison capabilities. It includes confusion matrix\n    calculations, field-level metrics, non-match documentation, and memory optimization capabilities.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_class: Optional[Type[StructuredModel]] = None,\n        threshold: float = 0.5,\n        verbose: bool = False,\n        document_non_matches: bool = True,\n    ):\n        \"\"\"\n        Initialize the evaluator.\n\n        Args:\n            model_class: Optional StructuredModel class for type checking\n            threshold: Similarity threshold for considering a match\n            verbose: Whether to print detailed progress information\n            document_non_matches: Whether to document detailed non-match information\n        \"\"\"\n        self.model_class = model_class\n        self.threshold = threshold\n        self.verbose = verbose\n        self.peak_memory_usage = 0\n        self.start_memory = get_memory_usage()\n\n        # New attributes for documenting non-matches\n        self.document_non_matches = document_non_matches\n        self.non_match_documents: List[NonMatchField] = []\n\n        if self.verbose:\n            print(\n                f\"Initialized StructuredModelEvaluator. Starting memory: {self.start_memory:.2f} MB\"\n            )\n\n    def _check_memory(self):\n        \"\"\"Check current memory usage and update peak memory.\"\"\"\n        current_memory = get_memory_usage()\n\n        if current_memory &gt; self.peak_memory_usage:\n            self.peak_memory_usage = current_memory\n\n        if self.verbose and current_memory &gt; self.start_memory + 100:  # 100MB increase\n            print(f\"Memory usage increased: {current_memory:.2f} MB\")\n\n        return current_memory\n\n    def _calculate_metrics_from_binary(\n        self,\n        tp: float,\n        fp: float,\n        fn: float,\n        tn: float = 0.0,\n        fd: float = 0.0,\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate metrics from binary classification counts.\n\n        Args:\n            tp: True positive count\n            fp: False positive count\n            fn: False negative count\n            tn: True negative count (default 0)\n            fd: False discovery count (default 0) - used only when recall_with_fd=True\n            recall_with_fd: Whether to use alternative recall formula including FD in denominator\n\n        Returns:\n            Dictionary with precision, recall, F1, and accuracy\n        \"\"\"\n        # Calculate precision\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n\n        # Calculate recall based on the selected formula\n        if recall_with_fd:\n            # Alternative recall: TP / (TP + FN + FD)\n            recall = tp / (tp + fn + fd) if (tp + fn + fd) &gt; 0 else 0.0\n        else:\n            # Traditional recall: TP / (TP + FN)\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n\n        # Calculate F1 score\n        f1 = (\n            2 * (precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0.0\n        )\n\n        # Calculate accuracy\n        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) &gt; 0 else 0.0\n\n        return {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"accuracy\": accuracy,\n        }\n\n    def calculate_derived_confusion_matrix_metrics(\n        self, cm_counts: Dict[str, Union[int, float]]\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate derived metrics from confusion matrix counts.\n\n        This method uses MetricsHelper to maintain consistency and avoid code duplication.\n\n        Args:\n            cm_counts: Dictionary with confusion matrix counts containing keys:\n                      'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'\n\n        Returns:\n            Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy\n        \"\"\"\n        # Use MetricsHelper for consistent metric calculation\n        from stickler.structured_object_evaluator.models.metrics_helper import (\n            MetricsHelper,\n        )\n\n        metrics_helper = MetricsHelper()\n\n        # Convert counts to the format expected by MetricsHelper\n        metrics_dict = {\n            \"tp\": int(cm_counts.get(\"tp\", 0)),\n            \"fp\": int(cm_counts.get(\"fp\", 0)),\n            \"tn\": int(cm_counts.get(\"tn\", 0)),\n            \"fn\": int(cm_counts.get(\"fn\", 0)),\n            \"fd\": int(cm_counts.get(\"fd\", 0)),\n            \"fa\": int(cm_counts.get(\"fa\", 0)),\n        }\n\n        # Use MetricsHelper to calculate derived metrics\n        return metrics_helper.calculate_derived_metrics(metrics_dict)\n\n    def _convert_score_to_binary(self, score: float) -&gt; Dict[str, float]:\n        \"\"\"\n        Convert an ANLS Star score to binary classification counts.\n\n        Args:\n            score: ANLS Star similarity score [0-1]\n\n        Returns:\n            Dictionary with TP, FP, FN, TN counts\n        \"\"\"\n        # For a single field comparison, there are different approaches\n        # to convert a similarity score to binary classification:\n\n        # Approach used here: If score &gt;= threshold, count as TP with\n        # proportional value, otherwise count as partial FP and partial FN\n        if score &gt;= self.threshold:\n            # Handle as true positive with proportional credit\n            tp = score  # Proportional TP\n            fp = (\n                1 - score if score &lt; 1.0 else 0\n            )  # Proportional FP for imperfect matches\n            fn = 0\n            tn = 0\n        else:\n            # Handle as false classification\n            tp = 0\n            fp = score  # Give partial credit for similarity even if below threshold\n            fn = 1 - score  # More different = higher FN\n            tn = 0\n\n        return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n\n    def _is_null_value(self, value: Any) -&gt; bool:\n        \"\"\"\n        Determine if a value should be considered null or empty.\n\n        Args:\n            value: The value to check\n\n        Returns:\n            True if the value is null/empty, False otherwise\n        \"\"\"\n        if value is None:\n            return True\n        elif hasattr(value, \"__len__\") and not isinstance(\n            value, (str, bytes, bytearray)\n        ):\n            # Consider empty lists/collections as null values\n            return len(value) == 0\n        elif isinstance(value, (str, bytes, bytearray)):\n            return len(value.strip()) == 0\n        return False\n\n    def combine_cm_dicts(\n        self, cm1: Dict[str, int], cm2: Dict[str, int]\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Combine two confusion matrix dictionaries by adding corresponding values.\n\n        Args:\n            cm1: First confusion matrix dictionary\n            cm2: Second confusion matrix dictionary\n\n        Returns:\n            Combined confusion matrix dictionary\n        \"\"\"\n        return {\n            key: cm1.get(key, 0) + cm2.get(key, 0)\n            for key in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]\n        }\n\n    def add_non_match(\n        self,\n        field_path: str,\n        non_match_type: NonMatchType,\n        gt_value: Any,\n        pred_value: Any,\n        similarity_score: Optional[float] = None,\n        details: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Document a non-match with detailed information.\n\n        Args:\n            field_path: Dot-notation path to the field (e.g., 'address.city')\n            non_match_type: Type of non-match\n            gt_value: Ground truth value\n            pred_value: Predicted value\n            similarity_score: Optional similarity score if available\n            details: Optional additional context or details\n            document_id: Optional ID of the document this non-match belongs to\n        \"\"\"\n        if not self.document_non_matches:\n            return\n\n        self.non_match_documents.append(\n            NonMatchField(\n                field_path=field_path,\n                non_match_type=non_match_type,\n                ground_truth_value=gt_value,\n                prediction_value=pred_value,\n                similarity_score=similarity_score,\n                details=details or {},\n            )\n        )\n\n    def clear_non_match_documents(self):\n        \"\"\"Clear the stored non-match documents.\"\"\"\n        self.non_match_documents = []\n\n    def _convert_enhanced_non_match_to_field(\n        self, nm_dict: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Convert enhanced non-match format to NonMatchField format.\n\n        Args:\n            nm_dict: Enhanced non-match dictionary from StructuredModel\n\n        Returns:\n            Dictionary in NonMatchField format\n        \"\"\"\n        # Map enhanced format to NonMatchField format\n        converted = {\n            \"field_path\": nm_dict.get(\"field_path\", \"\"),\n            \"ground_truth_value\": nm_dict.get(\"ground_truth_value\"),\n            \"prediction_value\": nm_dict.get(\"prediction_value\"),\n            \"similarity_score\": nm_dict.get(\"similarity_score\"),\n            \"details\": nm_dict.get(\"details\", {}),\n        }\n\n        # The non_match_type is already a NonMatchType enum from StructuredModel\n        converted[\"non_match_type\"] = nm_dict.get(\"non_match_type\")\n\n        return converted\n\n    def _compare_models(\n        self, gt_model: StructuredModel, pred_model: StructuredModel\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compare two StructuredModel instances and return metrics.\n\n        Args:\n            gt_model: Ground truth model\n            pred_model: Predicted model\n\n        Returns:\n            Dict with comparison metrics including tp, fp, fn, tn, field_scores, overall_score\n        \"\"\"\n        # Check if inputs are valid StructuredModel instances\n        if not (\n            isinstance(gt_model, StructuredModel)\n            and isinstance(pred_model, StructuredModel)\n        ):\n            raise TypeError(\"Both models must be StructuredModel instances\")\n\n        # If model_class is specified, check type\n        if self.model_class and not (\n            isinstance(gt_model, self.model_class)\n            and isinstance(pred_model, self.model_class)\n        ):\n            raise TypeError(\n                f\"Both models must be instances of {self.model_class.__name__}\"\n            )\n\n        # Use the built-in compare_with method from StructuredModel\n        comparison_result = gt_model.compare_with(pred_model)\n\n        # Initialize metrics\n        tp = fp = fn = tn = 0\n\n        # Determine match status\n        if comparison_result[\"overall_score\"] &gt;= self.threshold:\n            # Good enough match\n            tp = 1\n        else:\n            # Not a good enough match\n            fp = 1\n\n        # Prepare result\n        result = {\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n            \"field_scores\": comparison_result[\"field_scores\"],\n            \"overall_score\": comparison_result[\"overall_score\"],\n            # match_status removed - now unnecessary\n        }\n\n        return result\n\n    def evaluate(\n        self,\n        ground_truth: StructuredModel,\n        predictions: StructuredModel,\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate predictions against ground truth and return comprehensive metrics.\n\n        Args:\n            ground_truth: Ground truth data (StructuredModel instance)\n            predictions: Predicted data (StructuredModel instance)\n            recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                            If False, use traditional recall (TP/(TP+FN))\n\n        Returns:\n            Dictionary with the following structure:\n\n            {\n                \"overall\": {\n                    \"precision\": float,     # Overall precision [0-1]\n                    \"recall\": float,        # Overall recall [0-1]\n                    \"f1\": float,           # Overall F1 score [0-1]\n                    \"accuracy\": float,     # Overall accuracy [0-1]\n                    \"anls_score\": float    # Overall ANLS similarity score [0-1]\n                },\n\n                \"fields\": {\n                    \"&lt;field_name&gt;\": {\n                        # For primitive fields (str, int, float, bool):\n                        \"precision\": float,\n                        \"recall\": float,\n                        \"f1\": float,\n                        \"accuracy\": float,\n                        \"anls_score\": float\n                    },\n\n                    \"&lt;list_field_name&gt;\": {\n                        # For list fields (e.g., products: List[Product]):\n                        \"overall\": {\n                            \"precision\": float,\n                            \"recall\": float,\n                            \"f1\": float,\n                            \"accuracy\": float,\n                            \"anls_score\": float\n                        },\n                        \"items\": [\n                            # Individual metrics for each matched item pair\n                            {\n                                \"overall\": {...},  # Item-level overall metrics\n                                \"fields\": {        # Field metrics within each item\n                                    \"&lt;nested_field&gt;\": {...}\n                                }\n                            }\n                        ]\n                    }\n                },\n\n                \"confusion_matrix\": {\n                    \"fields\": {\n                        # AGGREGATED metrics for all field types\n                        \"&lt;field_name&gt;\": {\n                            \"tp\": int,          # True positives\n                            \"fp\": int,          # False positives\n                            \"tn\": int,          # True negatives\n                            \"fn\": int,          # False negatives\n                            \"fd\": int,          # False discoveries (non-null but don't match)\n                            \"fa\": int,          # False alarms\n                            \"derived\": {\n                                \"cm_precision\": float,\n                                \"cm_recall\": float,\n                                \"cm_f1\": float,\n                                \"cm_accuracy\": float\n                            }\n                        },\n\n                        # For list fields with nested objects, aggregated field metrics:\n                        \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n                            # Aggregated counts across ALL instances in the list\n                            \"tp\": int,    # Total true positives for this field across all items\n                            \"fp\": int,    # Total false positives for this field across all items\n                            \"fn\": int,    # Total false negatives for this field across all items\n                            \"fd\": int,    # Total false discoveries for this field across all items\n                            \"fa\": int,    # Total false alarms for this field across all items\n                            \"derived\": {...}\n                        }\n                    },\n\n                    \"overall\": {\n                        # Overall confusion matrix aggregating all fields\n                        \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n                        \"derived\": {...}\n                    }\n                }\n            }\n\n        Key Usage Patterns:\n\n        1. **Individual Item Metrics** (per-instance analysis):\n           ```python\n           # Access metrics for each individual item in a list\n           for i, item_metrics in enumerate(results['fields']['products']['items']):\n               print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n           ```\n\n        2. **Aggregated Field Metrics** (recommended for field performance analysis):\n           ```python\n           # Access aggregated metrics across all instances of a field type\n           cm_fields = results['confusion_matrix']['fields']\n           product_id_performance = cm_fields['products.product_id']\n           print(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n           # Get all aggregated product field metrics\n           product_fields = {k: v for k, v in cm_fields.items()\n                           if k.startswith('products.')}\n           ```\n\n        3. **Helper Function for Aggregated Metrics**:\n           ```python\n           def get_aggregated_metrics(results, list_field_name):\n               '''Extract aggregated field metrics for a list field.'''\n               cm_fields = results['confusion_matrix']['fields']\n               prefix = f\"{list_field_name}.\"\n               return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n                      if k.startswith(prefix)}\n\n           # Usage:\n           product_metrics = get_aggregated_metrics(results, 'products')\n           print(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n           ```\n\n        Note:\n            - Use `results['fields'][field]['items']` for per-instance analysis\n            - Use `results['confusion_matrix']['fields'][field.subfield]` for aggregated field analysis\n            - Aggregated metrics provide rolled-up performance across all instances of a field type\n            - Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics\n        \"\"\"\n        # Clear any existing non-match documents\n        self.clear_non_match_documents()\n\n        # Use StructuredModel's enhanced comparison with evaluator format\n        # This pushes all the heavy lifting into the StructuredModel as requested\n        result = ground_truth.compare_with(\n            predictions,\n            include_confusion_matrix=True,\n            document_non_matches=self.document_non_matches,\n            evaluator_format=True,  # This makes StructuredModel return evaluator-compatible format\n            recall_with_fd=recall_with_fd,\n        )\n\n        # Add non-matches to evaluator's collection if they exist\n        if result.get(\"non_matches\"):\n            for nm_dict in result[\"non_matches\"]:\n                # Convert enhanced non-match format to NonMatchField format\n                converted_nm = self._convert_enhanced_non_match_to_field(nm_dict)\n                self.non_match_documents.append(NonMatchField(**converted_nm))\n\n        # Process derived metrics explicitly with recall_with_fd parameter\n        if \"confusion_matrix\" in result and \"overall\" in result[\"confusion_matrix\"]:\n            overall_cm = result[\"confusion_matrix\"][\"overall\"]\n\n            # Update derived metrics directly in the result\n            from stickler.structured_object_evaluator.models.metrics_helper import (\n                MetricsHelper,\n            )\n\n            metrics_helper = MetricsHelper()\n\n            # Apply correct recall_with_fd to overall metrics\n            derived_metrics = metrics_helper.calculate_derived_metrics(\n                overall_cm, recall_with_fd=recall_with_fd\n            )\n            result[\"confusion_matrix\"][\"overall\"][\"derived\"] = derived_metrics\n\n            # Copy these to the top-level metrics if needed\n            if \"overall\" in result:\n                result[\"overall\"][\"precision\"] = derived_metrics[\"cm_precision\"]\n                result[\"overall\"][\"recall\"] = derived_metrics[\"cm_recall\"]\n                result[\"overall\"][\"f1\"] = derived_metrics[\"cm_f1\"]\n\n        return result\n\n    def _format_evaluation_results(\n        self, comparison_result: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Format StructuredModel comparison results to match expected evaluator output format.\n\n        Args:\n            comparison_result: Result from StructuredModel.compare_with()\n\n        Returns:\n            Dictionary in the expected evaluator format\n        \"\"\"\n        # Extract components from StructuredModel result\n        field_scores = comparison_result[\"field_scores\"]\n        overall_score = comparison_result[\"overall_score\"]\n        confusion_matrix = comparison_result.get(\"confusion_matrix\", {})\n        non_matches = comparison_result.get(\"non_matches\", [])\n\n        # Calculate field metrics using existing logic for backward compatibility\n        field_metrics = {}\n\n        for field_name, score in field_scores.items():\n            # Convert field score to binary metrics using existing method\n            binary = self._convert_score_to_binary(score)\n            # For field metrics, fd is often not available directly, so we ignore recall_with_fd\n            metrics = self._calculate_metrics_from_binary(\n                binary[\"tp\"], binary[\"fp\"], binary[\"fn\"], binary[\"tn\"]\n            )\n            metrics[\"anls_score\"] = score\n            field_metrics[field_name] = metrics\n\n        # Calculate overall metrics\n        binary = self._convert_score_to_binary(overall_score)\n        # For overall metrics, use confusion_matrix data which should have fd\n        overall_fd = confusion_matrix.get(\"overall\", {}).get(\"fd\", 0)\n        overall_metrics = self._calculate_metrics_from_binary(\n            binary[\"tp\"],\n            binary[\"fp\"],\n            binary[\"fn\"],\n            binary[\"tn\"],\n            fd=overall_fd,\n            recall_with_fd=recall_with_fd,\n        )\n        overall_metrics[\"anls_score\"] = overall_score\n\n        # Add non-matches to evaluator's collection if they exist\n        if non_matches:\n            for nm_dict in non_matches:\n                self.non_match_documents.append(NonMatchField(**nm_dict))\n\n        # Prepare final result in expected format\n        result = {\n            \"overall\": overall_metrics,\n            \"fields\": field_metrics,\n            \"confusion_matrix\": confusion_matrix,\n            \"non_matches\": non_matches,\n        }\n\n        return result\n\n    def _compare_model_lists(\n        self, gt_models: List[StructuredModel], pred_models: List[StructuredModel]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compare two lists of StructuredModel instances using Hungarian matching.\n\n        Args:\n            gt_models: List of ground truth models\n            pred_models: List of predicted models\n\n        Returns:\n            Dict with comparison metrics including tp, fp, fn, overall_score\n        \"\"\"\n        # Handle empty lists\n        if not gt_models and not pred_models:\n            return {\n                \"tp\": 0,\n                \"fp\": 0,\n                \"fn\": 0,\n                \"tn\": 0,\n                \"overall_score\": 1.0,  # Empty lists are a perfect match\n            }\n\n        if not gt_models:\n            return {\n                \"tp\": 0,\n                \"fp\": len(pred_models),\n                \"fn\": 0,\n                \"tn\": 0,\n                \"overall_score\": 0.0,  # All predictions are false positives\n            }\n\n        if not pred_models:\n            return {\n                \"tp\": 0,\n                \"fp\": 0,\n                \"fn\": len(gt_models),\n                \"tn\": 0,\n                \"overall_score\": 0.0,  # All ground truths are false negatives\n            }\n\n        # Ensure all items are StructuredModel instances\n        if not all(\n            isinstance(model, StructuredModel) for model in gt_models + pred_models\n        ):\n            raise TypeError(\"All items in both lists must be StructuredModel instances\")\n\n        # If model_class is specified, check type for all models\n        if self.model_class:\n            if not all(\n                isinstance(model, self.model_class) for model in gt_models + pred_models\n            ):\n                raise TypeError(\n                    f\"All models must be instances of {self.model_class.__name__}\"\n                )\n\n        # Create a Hungarian matcher with StructuredModelComparator\n        hungarian = HungarianMatcher(StructuredModelComparator())\n\n        # Run Hungarian matching\n        tp, fp = hungarian(gt_models, pred_models)\n\n        # Calculate false negatives\n        fn = len(gt_models) - tp\n\n        # Calculate overall score (proportion of correct matches)\n        max_items = max(len(gt_models), len(pred_models))\n        overall_score = tp / max_items if max_items &gt; 0 else 1.0\n\n        return {\"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": 0, \"overall_score\": overall_score}\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.__init__","title":"<code>__init__(model_class=None, threshold=0.5, verbose=False, document_non_matches=True)</code>","text":"<p>Initialize the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Optional[Type[StructuredModel]]</code> <p>Optional StructuredModel class for type checking</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Similarity threshold for considering a match</p> <code>0.5</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed progress information</p> <code>False</code> <code>document_non_matches</code> <code>bool</code> <p>Whether to document detailed non-match information</p> <code>True</code> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def __init__(\n    self,\n    model_class: Optional[Type[StructuredModel]] = None,\n    threshold: float = 0.5,\n    verbose: bool = False,\n    document_non_matches: bool = True,\n):\n    \"\"\"\n    Initialize the evaluator.\n\n    Args:\n        model_class: Optional StructuredModel class for type checking\n        threshold: Similarity threshold for considering a match\n        verbose: Whether to print detailed progress information\n        document_non_matches: Whether to document detailed non-match information\n    \"\"\"\n    self.model_class = model_class\n    self.threshold = threshold\n    self.verbose = verbose\n    self.peak_memory_usage = 0\n    self.start_memory = get_memory_usage()\n\n    # New attributes for documenting non-matches\n    self.document_non_matches = document_non_matches\n    self.non_match_documents: List[NonMatchField] = []\n\n    if self.verbose:\n        print(\n            f\"Initialized StructuredModelEvaluator. Starting memory: {self.start_memory:.2f} MB\"\n        )\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.add_non_match","title":"<code>add_non_match(field_path, non_match_type, gt_value, pred_value, similarity_score=None, details=None)</code>","text":"<p>Document a non-match with detailed information.</p> <p>Parameters:</p> Name Type Description Default <code>field_path</code> <code>str</code> <p>Dot-notation path to the field (e.g., 'address.city')</p> required <code>non_match_type</code> <code>NonMatchType</code> <p>Type of non-match</p> required <code>gt_value</code> <code>Any</code> <p>Ground truth value</p> required <code>pred_value</code> <code>Any</code> <p>Predicted value</p> required <code>similarity_score</code> <code>Optional[float]</code> <p>Optional similarity score if available</p> <code>None</code> <code>details</code> <code>Optional[Dict[str, Any]]</code> <p>Optional additional context or details</p> <code>None</code> <code>document_id</code> <p>Optional ID of the document this non-match belongs to</p> required Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def add_non_match(\n    self,\n    field_path: str,\n    non_match_type: NonMatchType,\n    gt_value: Any,\n    pred_value: Any,\n    similarity_score: Optional[float] = None,\n    details: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"\n    Document a non-match with detailed information.\n\n    Args:\n        field_path: Dot-notation path to the field (e.g., 'address.city')\n        non_match_type: Type of non-match\n        gt_value: Ground truth value\n        pred_value: Predicted value\n        similarity_score: Optional similarity score if available\n        details: Optional additional context or details\n        document_id: Optional ID of the document this non-match belongs to\n    \"\"\"\n    if not self.document_non_matches:\n        return\n\n    self.non_match_documents.append(\n        NonMatchField(\n            field_path=field_path,\n            non_match_type=non_match_type,\n            ground_truth_value=gt_value,\n            prediction_value=pred_value,\n            similarity_score=similarity_score,\n            details=details or {},\n        )\n    )\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.calculate_derived_confusion_matrix_metrics","title":"<code>calculate_derived_confusion_matrix_metrics(cm_counts)</code>","text":"<p>Calculate derived metrics from confusion matrix counts.</p> <p>This method uses MetricsHelper to maintain consistency and avoid code duplication.</p> <p>Parameters:</p> Name Type Description Default <code>cm_counts</code> <code>Dict[str, Union[int, float]]</code> <p>Dictionary with confusion matrix counts containing keys:       'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def calculate_derived_confusion_matrix_metrics(\n    self, cm_counts: Dict[str, Union[int, float]]\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate derived metrics from confusion matrix counts.\n\n    This method uses MetricsHelper to maintain consistency and avoid code duplication.\n\n    Args:\n        cm_counts: Dictionary with confusion matrix counts containing keys:\n                  'tp', 'fp', 'tn', 'fn', and optionally 'fd', 'fa'\n\n    Returns:\n        Dictionary with derived metrics: cm_precision, cm_recall, cm_f1, cm_accuracy\n    \"\"\"\n    # Use MetricsHelper for consistent metric calculation\n    from stickler.structured_object_evaluator.models.metrics_helper import (\n        MetricsHelper,\n    )\n\n    metrics_helper = MetricsHelper()\n\n    # Convert counts to the format expected by MetricsHelper\n    metrics_dict = {\n        \"tp\": int(cm_counts.get(\"tp\", 0)),\n        \"fp\": int(cm_counts.get(\"fp\", 0)),\n        \"tn\": int(cm_counts.get(\"tn\", 0)),\n        \"fn\": int(cm_counts.get(\"fn\", 0)),\n        \"fd\": int(cm_counts.get(\"fd\", 0)),\n        \"fa\": int(cm_counts.get(\"fa\", 0)),\n    }\n\n    # Use MetricsHelper to calculate derived metrics\n    return metrics_helper.calculate_derived_metrics(metrics_dict)\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.clear_non_match_documents","title":"<code>clear_non_match_documents()</code>","text":"<p>Clear the stored non-match documents.</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def clear_non_match_documents(self):\n    \"\"\"Clear the stored non-match documents.\"\"\"\n    self.non_match_documents = []\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.combine_cm_dicts","title":"<code>combine_cm_dicts(cm1, cm2)</code>","text":"<p>Combine two confusion matrix dictionaries by adding corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>cm1</code> <code>Dict[str, int]</code> <p>First confusion matrix dictionary</p> required <code>cm2</code> <code>Dict[str, int]</code> <p>Second confusion matrix dictionary</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Combined confusion matrix dictionary</p> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def combine_cm_dicts(\n    self, cm1: Dict[str, int], cm2: Dict[str, int]\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Combine two confusion matrix dictionaries by adding corresponding values.\n\n    Args:\n        cm1: First confusion matrix dictionary\n        cm2: Second confusion matrix dictionary\n\n    Returns:\n        Combined confusion matrix dictionary\n    \"\"\"\n    return {\n        key: cm1.get(key, 0) + cm2.get(key, 0)\n        for key in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]\n    }\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.evaluator.StructuredModelEvaluator.evaluate","title":"<code>evaluate(ground_truth, predictions, recall_with_fd=False)</code>","text":"<p>Evaluate predictions against ground truth and return comprehensive metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <code>StructuredModel</code> <p>Ground truth data (StructuredModel instance)</p> required <code>predictions</code> <code>StructuredModel</code> <p>Predicted data (StructuredModel instance)</p> required <code>recall_with_fd</code> <code>bool</code> <p>If True, include FD in recall denominator (TP/(TP+FN+FD))             If False, use traditional recall (TP/(TP+FN))</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with the following structure:</p> <code>Dict[str, Any]</code> <p>{ \"overall\": {     \"precision\": float,     # Overall precision [0-1]     \"recall\": float,        # Overall recall [0-1]     \"f1\": float,           # Overall F1 score [0-1]     \"accuracy\": float,     # Overall accuracy [0-1]     \"anls_score\": float    # Overall ANLS similarity score [0-1] },</p> <p>\"fields\": {     \"\": {         # For primitive fields (str, int, float, bool):         \"precision\": float,         \"recall\": float,         \"f1\": float,         \"accuracy\": float,         \"anls_score\": float     }, <pre><code>\"&lt;list_field_name&gt;\": {\n    # For list fields (e.g., products: List[Product]):\n    \"overall\": {\n        \"precision\": float,\n        \"recall\": float,\n        \"f1\": float,\n        \"accuracy\": float,\n        \"anls_score\": float\n    },\n    \"items\": [\n        # Individual metrics for each matched item pair\n        {\n            \"overall\": {...},  # Item-level overall metrics\n            \"fields\": {        # Field metrics within each item\n                \"&lt;nested_field&gt;\": {...}\n            }\n        }\n    ]\n}\n</code></pre> <p>},</p> <p>\"confusion_matrix\": {     \"fields\": {         # AGGREGATED metrics for all field types         \"\": {             \"tp\": int,          # True positives             \"fp\": int,          # False positives             \"tn\": int,          # True negatives             \"fn\": int,          # False negatives             \"fd\": int,          # False discoveries (non-null but don't match)             \"fa\": int,          # False alarms             \"derived\": {                 \"cm_precision\": float,                 \"cm_recall\": float,                 \"cm_f1\": float,                 \"cm_accuracy\": float             }         }, <pre><code>    # For list fields with nested objects, aggregated field metrics:\n    \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n        # Aggregated counts across ALL instances in the list\n        \"tp\": int,    # Total true positives for this field across all items\n        \"fp\": int,    # Total false positives for this field across all items\n        \"fn\": int,    # Total false negatives for this field across all items\n        \"fd\": int,    # Total false discoveries for this field across all items\n        \"fa\": int,    # Total false alarms for this field across all items\n        \"derived\": {...}\n    }\n},\n\n\"overall\": {\n    # Overall confusion matrix aggregating all fields\n    \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n    \"derived\": {...}\n}\n</code></pre> <p>}</p> <code>Dict[str, Any]</code> <p>}</p> <p>Key Usage Patterns:</p> <ol> <li> <p>Individual Item Metrics (per-instance analysis):    <pre><code># Access metrics for each individual item in a list\nfor i, item_metrics in enumerate(results['fields']['products']['items']):\n    print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n</code></pre></p> </li> <li> <p>Aggregated Field Metrics (recommended for field performance analysis):    <pre><code># Access aggregated metrics across all instances of a field type\ncm_fields = results['confusion_matrix']['fields']\nproduct_id_performance = cm_fields['products.product_id']\nprint(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n# Get all aggregated product field metrics\nproduct_fields = {k: v for k, v in cm_fields.items()\n                if k.startswith('products.')}\n</code></pre></p> </li> <li> <p>Helper Function for Aggregated Metrics:    <pre><code>def get_aggregated_metrics(results, list_field_name):\n    '''Extract aggregated field metrics for a list field.'''\n    cm_fields = results['confusion_matrix']['fields']\n    prefix = f\"{list_field_name}.\"\n    return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n           if k.startswith(prefix)}\n\n# Usage:\nproduct_metrics = get_aggregated_metrics(results, 'products')\nprint(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n</code></pre></p> </li> </ol> Note <ul> <li>Use <code>results['fields'][field]['items']</code> for per-instance analysis</li> <li>Use <code>results['confusion_matrix']['fields'][field.subfield]</code> for aggregated field analysis</li> <li>Aggregated metrics provide rolled-up performance across all instances of a field type</li> <li>Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics</li> </ul> Source code in <code>stickler/structured_object_evaluator/evaluator.py</code> <pre><code>def evaluate(\n    self,\n    ground_truth: StructuredModel,\n    predictions: StructuredModel,\n    recall_with_fd: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Evaluate predictions against ground truth and return comprehensive metrics.\n\n    Args:\n        ground_truth: Ground truth data (StructuredModel instance)\n        predictions: Predicted data (StructuredModel instance)\n        recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                        If False, use traditional recall (TP/(TP+FN))\n\n    Returns:\n        Dictionary with the following structure:\n\n        {\n            \"overall\": {\n                \"precision\": float,     # Overall precision [0-1]\n                \"recall\": float,        # Overall recall [0-1]\n                \"f1\": float,           # Overall F1 score [0-1]\n                \"accuracy\": float,     # Overall accuracy [0-1]\n                \"anls_score\": float    # Overall ANLS similarity score [0-1]\n            },\n\n            \"fields\": {\n                \"&lt;field_name&gt;\": {\n                    # For primitive fields (str, int, float, bool):\n                    \"precision\": float,\n                    \"recall\": float,\n                    \"f1\": float,\n                    \"accuracy\": float,\n                    \"anls_score\": float\n                },\n\n                \"&lt;list_field_name&gt;\": {\n                    # For list fields (e.g., products: List[Product]):\n                    \"overall\": {\n                        \"precision\": float,\n                        \"recall\": float,\n                        \"f1\": float,\n                        \"accuracy\": float,\n                        \"anls_score\": float\n                    },\n                    \"items\": [\n                        # Individual metrics for each matched item pair\n                        {\n                            \"overall\": {...},  # Item-level overall metrics\n                            \"fields\": {        # Field metrics within each item\n                                \"&lt;nested_field&gt;\": {...}\n                            }\n                        }\n                    ]\n                }\n            },\n\n            \"confusion_matrix\": {\n                \"fields\": {\n                    # AGGREGATED metrics for all field types\n                    \"&lt;field_name&gt;\": {\n                        \"tp\": int,          # True positives\n                        \"fp\": int,          # False positives\n                        \"tn\": int,          # True negatives\n                        \"fn\": int,          # False negatives\n                        \"fd\": int,          # False discoveries (non-null but don't match)\n                        \"fa\": int,          # False alarms\n                        \"derived\": {\n                            \"cm_precision\": float,\n                            \"cm_recall\": float,\n                            \"cm_f1\": float,\n                            \"cm_accuracy\": float\n                        }\n                    },\n\n                    # For list fields with nested objects, aggregated field metrics:\n                    \"&lt;list_field&gt;.&lt;nested_field&gt;\": {\n                        # Aggregated counts across ALL instances in the list\n                        \"tp\": int,    # Total true positives for this field across all items\n                        \"fp\": int,    # Total false positives for this field across all items\n                        \"fn\": int,    # Total false negatives for this field across all items\n                        \"fd\": int,    # Total false discoveries for this field across all items\n                        \"fa\": int,    # Total false alarms for this field across all items\n                        \"derived\": {...}\n                    }\n                },\n\n                \"overall\": {\n                    # Overall confusion matrix aggregating all fields\n                    \"tp\": int, \"fp\": int, \"tn\": int, \"fn\": int, \"fd\": int, \"fa\": int\n                    \"derived\": {...}\n                }\n            }\n        }\n\n    Key Usage Patterns:\n\n    1. **Individual Item Metrics** (per-instance analysis):\n       ```python\n       # Access metrics for each individual item in a list\n       for i, item_metrics in enumerate(results['fields']['products']['items']):\n           print(f\"Product {i}: {item_metrics['overall']['f1']}\")\n       ```\n\n    2. **Aggregated Field Metrics** (recommended for field performance analysis):\n       ```python\n       # Access aggregated metrics across all instances of a field type\n       cm_fields = results['confusion_matrix']['fields']\n       product_id_performance = cm_fields['products.product_id']\n       print(f\"Product ID field: {product_id_performance['derived']['cm_precision']}\")\n\n       # Get all aggregated product field metrics\n       product_fields = {k: v for k, v in cm_fields.items()\n                       if k.startswith('products.')}\n       ```\n\n    3. **Helper Function for Aggregated Metrics**:\n       ```python\n       def get_aggregated_metrics(results, list_field_name):\n           '''Extract aggregated field metrics for a list field.'''\n           cm_fields = results['confusion_matrix']['fields']\n           prefix = f\"{list_field_name}.\"\n           return {k.replace(prefix, ''): v for k, v in cm_fields.items()\n                  if k.startswith(prefix)}\n\n       # Usage:\n       product_metrics = get_aggregated_metrics(results, 'products')\n       print(f\"Product name precision: {product_metrics['name']['derived']['cm_precision']}\")\n       ```\n\n    Note:\n        - Use `results['fields'][field]['items']` for per-instance analysis\n        - Use `results['confusion_matrix']['fields'][field.subfield]` for aggregated field analysis\n        - Aggregated metrics provide rolled-up performance across all instances of a field type\n        - Confusion matrix metrics use standard TP/FP/TN/FN/FD classification with derived metrics\n    \"\"\"\n    # Clear any existing non-match documents\n    self.clear_non_match_documents()\n\n    # Use StructuredModel's enhanced comparison with evaluator format\n    # This pushes all the heavy lifting into the StructuredModel as requested\n    result = ground_truth.compare_with(\n        predictions,\n        include_confusion_matrix=True,\n        document_non_matches=self.document_non_matches,\n        evaluator_format=True,  # This makes StructuredModel return evaluator-compatible format\n        recall_with_fd=recall_with_fd,\n    )\n\n    # Add non-matches to evaluator's collection if they exist\n    if result.get(\"non_matches\"):\n        for nm_dict in result[\"non_matches\"]:\n            # Convert enhanced non-match format to NonMatchField format\n            converted_nm = self._convert_enhanced_non_match_to_field(nm_dict)\n            self.non_match_documents.append(NonMatchField(**converted_nm))\n\n    # Process derived metrics explicitly with recall_with_fd parameter\n    if \"confusion_matrix\" in result and \"overall\" in result[\"confusion_matrix\"]:\n        overall_cm = result[\"confusion_matrix\"][\"overall\"]\n\n        # Update derived metrics directly in the result\n        from stickler.structured_object_evaluator.models.metrics_helper import (\n            MetricsHelper,\n        )\n\n        metrics_helper = MetricsHelper()\n\n        # Apply correct recall_with_fd to overall metrics\n        derived_metrics = metrics_helper.calculate_derived_metrics(\n            overall_cm, recall_with_fd=recall_with_fd\n        )\n        result[\"confusion_matrix\"][\"overall\"][\"derived\"] = derived_metrics\n\n        # Copy these to the top-level metrics if needed\n        if \"overall\" in result:\n            result[\"overall\"][\"precision\"] = derived_metrics[\"cm_precision\"]\n            result[\"overall\"][\"recall\"] = derived_metrics[\"cm_recall\"]\n            result[\"overall\"][\"f1\"] = derived_metrics[\"cm_f1\"]\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator","title":"<code>stickler.structured_object_evaluator.bulk_structured_model_evaluator</code>","text":"<p>Stateful Bulk Evaluator for StructuredModel objects.</p> <p>This module provides a modern stateful bulk evaluator inspired by PyTorch Lightning's stateful metrics and scikit-learn's incremental learning patterns. It supports memory-efficient processing of large datasets through accumulation-based evaluation.</p>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator","title":"<code>stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator</code>","text":"<p>Stateful bulk evaluator for StructuredModel objects.</p> <p>Inspired by PyTorch Lightning's stateful metrics and scikit-learn's incremental learning patterns. This evaluator accumulates evaluation state across multiple document processing calls, enabling memory-efficient evaluation of arbitrarily large datasets without loading everything into memory at once.</p> <p>Key Features: - Stateful accumulation (like PyTorch Lightning metrics) - Memory-efficient streaming processing (like scikit-learn partial_fit) - External control over data flow and error handling - Checkpointing and recovery capabilities - Distributed processing support via state merging - Uses StructuredModel.compare_with() method directly</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>class BulkStructuredModelEvaluator:\n    \"\"\"\n    Stateful bulk evaluator for StructuredModel objects.\n\n    Inspired by PyTorch Lightning's stateful metrics and scikit-learn's incremental\n    learning patterns. This evaluator accumulates evaluation state across multiple\n    document processing calls, enabling memory-efficient evaluation of arbitrarily\n    large datasets without loading everything into memory at once.\n\n    Key Features:\n    - Stateful accumulation (like PyTorch Lightning metrics)\n    - Memory-efficient streaming processing (like scikit-learn partial_fit)\n    - External control over data flow and error handling\n    - Checkpointing and recovery capabilities\n    - Distributed processing support via state merging\n    - Uses StructuredModel.compare_with() method directly\n    \"\"\"\n\n    def __init__(\n        self,\n        target_schema: Type[StructuredModel],\n        verbose: bool = False,\n        document_non_matches: bool = True,\n        elide_errors: bool = False,\n        individual_results_jsonl: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the stateful bulk evaluator.\n\n        Args:\n            target_schema: StructuredModel class for validation and processing\n            verbose: Whether to print detailed progress information\n            document_non_matches: Whether to document detailed non-match information\n            elide_errors: If True, skip documents with errors; if False, accumulate error metrics\n            individual_results_jsonl: Optional path to JSONL file for appending individual comparison results\n        \"\"\"\n        self.target_schema = target_schema\n        self.verbose = verbose\n        self.document_non_matches = document_non_matches\n        self.elide_errors = elide_errors\n        self.individual_results_jsonl = individual_results_jsonl\n\n        # Initialize state\n        self.reset()\n\n        if self.verbose:\n            print(\n                f\"Initialized BulkStructuredModelEvaluator for {target_schema.__name__}\"\n            )\n            if self.individual_results_jsonl:\n                print(\n                    f\"Individual results will be appended to: {self.individual_results_jsonl}\"\n                )\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Clear all accumulated state and start fresh evaluation.\n\n        This method resets all internal counters, metrics, and error tracking\n        to initial state, enabling reuse of the same evaluator instance for\n        multiple evaluation runs.\n        \"\"\"\n        # Accumulated confusion matrix state using nested defaultdicts\n        self._confusion_matrix = {\n            \"overall\": defaultdict(int),\n            \"fields\": defaultdict(lambda: defaultdict(int)),\n        }\n\n        # Non-match tracking (when document_non_matches=True)\n        self._non_matches = []\n\n        # Error tracking\n        self._errors = []\n\n        # Processing statistics\n        self._processed_count = 0\n        self._start_time = time.time()\n\n        if self.verbose:\n            print(\"Reset evaluator state\")\n\n    def update(\n        self,\n        gt_model: StructuredModel,\n        pred_model: StructuredModel,\n        doc_id: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Process a single document pair and accumulate the results in internal state.\n\n        This is the core method for stateful evaluation, inspired by PyTorch Lightning's\n        training_step pattern. Each call processes one document pair and updates\n        the internal confusion matrix counters.\n\n        Args:\n            gt_model: Ground truth StructuredModel instance\n            pred_model: Predicted StructuredModel instance\n            doc_id: Optional document identifier for error tracking\n        \"\"\"\n        if doc_id is None:\n            doc_id = f\"doc_{self._processed_count}\"\n\n        try:\n            # Use compare_with method directly on the StructuredModel\n            # Pass document_non_matches to achieve parity with compare_with method\n            comparison_result = gt_model.compare_with(\n                pred_model,\n                include_confusion_matrix=True,\n                document_non_matches=self.document_non_matches,\n            )\n\n            # Collect non-matches if enabled\n            if self.document_non_matches and \"non_matches\" in comparison_result:\n                # Add doc_id to each non-match for bulk tracking\n                for non_match in comparison_result[\"non_matches\"]:\n                    non_match_with_doc = non_match.copy()\n                    non_match_with_doc[\"doc_id\"] = doc_id\n                    self._non_matches.append(non_match_with_doc)\n\n            # Simple JSONL append of raw comparison result (before any processing)\n            if self.individual_results_jsonl:\n                record = {\"doc_id\": doc_id, \"comparison_result\": comparison_result}\n                with open(self.individual_results_jsonl, \"a\", encoding=\"utf-8\") as f:\n                    f.write(json.dumps(record) + \"\\n\")\n\n            # Accumulate the results into our state (this flattens for aggregation)\n            self._accumulate_confusion_matrix(comparison_result[\"confusion_matrix\"])\n\n            self._processed_count += 1\n\n            if self.verbose and self._processed_count % 1000 == 0:\n                elapsed = time.time() - self._start_time\n                print(f\"Processed {self._processed_count} documents ({elapsed:.2f}s)\")\n\n        except Exception as e:\n            error_record = {\n                \"doc_id\": doc_id,\n                \"error\": str(e),\n                \"error_type\": type(e).__name__,\n            }\n\n            if not self.elide_errors:\n                self._errors.append(error_record)\n\n                # For errors, add a \"failed\" classification to overall metrics\n                # This represents complete failure to process the document\n                self._confusion_matrix[\"overall\"][\"fn\"] += 1\n\n            if self.verbose:\n                print(f\"Error processing document {doc_id}: {str(e)}\")\n\n    def update_batch(\n        self, batch_data: List[Tuple[StructuredModel, StructuredModel, Optional[str]]]\n    ) -&gt; None:\n        \"\"\"\n        Process multiple document pairs efficiently in a batch.\n\n        This method provides efficient batch processing by calling update()\n        multiple times with optional garbage collection for memory management.\n\n        Args:\n            batch_data: List of tuples containing (gt_model, pred_model, doc_id)\n        \"\"\"\n        batch_start = self._processed_count\n\n        for gt_model, pred_model, doc_id in batch_data:\n            self.update(gt_model, pred_model, doc_id)\n\n        # Garbage collection for large batches\n        if len(batch_data) &gt;= 1000:\n            gc.collect()\n\n        if self.verbose:\n            batch_size = self._processed_count - batch_start\n            print(f\"Processed batch of {batch_size} documents\")\n\n    def get_current_metrics(self) -&gt; ProcessEvaluation:\n        \"\"\"\n        Get current accumulated metrics without clearing state.\n\n        This method allows monitoring evaluation progress by returning current\n        metrics computed from accumulated state. Unlike compute(), this does\n        not clear the internal state.\n\n        Returns:\n            ProcessEvaluation with current accumulated metrics\n        \"\"\"\n        return self._build_process_evaluation()\n\n    def compute(self) -&gt; ProcessEvaluation:\n        \"\"\"\n        Calculate final aggregated metrics from accumulated state.\n\n        This method performs the final computation of all derived metrics from\n        the accumulated confusion matrix state, similar to PyTorch Lightning's\n        training_epoch_end pattern.\n\n        Returns:\n            ProcessEvaluation with final aggregated metrics\n        \"\"\"\n        result = self._build_process_evaluation()\n\n        if self.verbose:\n            total_time = time.time() - self._start_time\n            print(\n                f\"Final computation completed: {self._processed_count} documents in {total_time:.2f}s\"\n            )\n            print(f\"Overall accuracy: {result.metrics.get('cm_accuracy', 0.0):.3f}\")\n\n        return result\n\n    def _accumulate_confusion_matrix(self, cm_result: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Accumulate confusion matrix results from a single document evaluation.\n\n        This method handles the core accumulation logic, properly aggregating\n        both overall metrics and field-level metrics while maintaining correct\n        nested field paths.\n\n        Args:\n            cm_result: Confusion matrix result from compare_with method\n        \"\"\"\n        # Accumulate overall metrics\n        if \"overall\" in cm_result:\n            for metric_name, value in cm_result[\"overall\"].items():\n                if isinstance(value, (int, float)) and metric_name in [\n                    \"tp\",\n                    \"fp\",\n                    \"tn\",\n                    \"fn\",\n                    \"fd\",\n                    \"fa\",\n                ]:\n                    self._confusion_matrix[\"overall\"][metric_name] += value\n\n        # Accumulate field-level metrics with proper path handling\n        if \"fields\" in cm_result:\n            self._accumulate_field_metrics(cm_result[\"fields\"], \"\")\n\n    def _accumulate_field_metrics(\n        self, fields_dict: Dict[str, Any], path_prefix: str\n    ) -&gt; None:\n        \"\"\"\n        Recursively accumulate field-level metrics with proper nested path construction.\n\n        This method fixes the nested field aggregation bugs from the original implementation\n        by properly handling different field structure formats and maintaining correct\n        dotted notation paths for nested fields.\n\n        Args:\n            fields_dict: Dictionary containing field metrics to accumulate\n            path_prefix: Current path prefix for building nested field paths\n        \"\"\"\n        for field_name, field_data in fields_dict.items():\n            current_path = f\"{path_prefix}.{field_name}\" if path_prefix else field_name\n\n            if not isinstance(field_data, dict):\n                continue\n\n            # Handle field with direct confusion matrix metrics (simple leaf field)\n            direct_metrics = {\n                k: v\n                for k, v in field_data.items()\n                if k in [\"tp\", \"fp\", \"tn\", \"fn\", \"fd\", \"fa\"]\n                and isinstance(v, (int, float))\n            }\n            if direct_metrics:\n                self._accumulate_single_field_metrics(current_path, direct_metrics)\n\n            # Handle hierarchical field structure (object fields with overall + fields)\n            if \"overall\" in field_data:\n                # Accumulate the overall metrics for this field\n                self._accumulate_single_field_metrics(\n                    current_path, field_data[\"overall\"]\n                )\n\n            # Handle nested fields - check if there's a \"fields\" structure\n            if \"fields\" in field_data and isinstance(field_data[\"fields\"], dict):\n                # For each nested field, create the proper dotted path\n                for nested_field_name, nested_field_data in field_data[\n                    \"fields\"\n                ].items():\n                    nested_path = f\"{current_path}.{nested_field_name}\"\n\n                    if isinstance(nested_field_data, dict):\n                        # If nested field has \"overall\", use those metrics\n                        if \"overall\" in nested_field_data:\n                            self._accumulate_single_field_metrics(\n                                nested_path, nested_field_data[\"overall\"]\n                            )\n                        else:\n                            # Otherwise, look for direct metrics\n                            nested_metrics = {\n                                k: v\n                                for k, v in nested_field_data.items()\n                                if k in [\"tp\", \"fp\", \"tn\", \"fn\", \"fd\", \"fa\"]\n                                and isinstance(v, (int, float))\n                            }\n                            if nested_metrics:\n                                self._accumulate_single_field_metrics(\n                                    nested_path, nested_metrics\n                                )\n\n                        # Continue recursion if there are more nested fields\n                        if \"fields\" in nested_field_data:\n                            self._accumulate_field_metrics(\n                                nested_field_data[\"fields\"], nested_path\n                            )\n\n            # Handle list field structure with nested_fields\n            elif \"nested_fields\" in field_data:\n                # Accumulate list-level metrics\n                list_metrics = {\n                    k: v\n                    for k, v in field_data.items()\n                    if k in [\"tp\", \"fp\", \"tn\", \"fn\", \"fd\", \"fa\"]\n                    and isinstance(v, (int, float))\n                }\n                if list_metrics:\n                    self._accumulate_single_field_metrics(current_path, list_metrics)\n\n                # Accumulate nested field metrics from the list items\n                for nested_field_name, nested_metrics in field_data[\n                    \"nested_fields\"\n                ].items():\n                    nested_path = f\"{current_path}.{nested_field_name}\"\n                    self._accumulate_single_field_metrics(nested_path, nested_metrics)\n\n    def _accumulate_single_field_metrics(\n        self, field_path: str, metrics: Dict[str, Union[int, float]]\n    ) -&gt; None:\n        \"\"\"\n        Accumulate metrics for a single field path.\n\n        Args:\n            field_path: Dotted path to the field (e.g., 'transactions.date')\n            metrics: Dictionary of confusion matrix metrics to accumulate\n        \"\"\"\n        for metric_name, value in metrics.items():\n            if metric_name in [\"tp\", \"fp\", \"tn\", \"fn\", \"fd\", \"fa\"] and isinstance(\n                value, (int, float)\n            ):\n                self._confusion_matrix[\"fields\"][field_path][metric_name] += value\n\n    def _calculate_derived_metrics(\n        self, cm_dict: Dict[str, Union[int, float]]\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Calculate derived confusion matrix metrics (precision, recall, f1, accuracy).\n\n        This method replicates the derivation logic that was previously handled\n        by StructuredModelEvaluator.\n\n        Args:\n            cm_dict: Dictionary with basic confusion matrix counts\n\n        Returns:\n            Dictionary with derived metrics\n        \"\"\"\n        tp = cm_dict.get(\"tp\", 0)\n        fp = cm_dict.get(\"fp\", 0)\n        tn = cm_dict.get(\"tn\", 0)\n        fn = cm_dict.get(\"fn\", 0)\n\n        # Calculate derived metrics with safe division\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n        f1 = (\n            2 * (precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0.0\n        )\n        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) &gt; 0 else 0.0\n\n        return {\n            \"cm_precision\": precision,\n            \"cm_recall\": recall,\n            \"cm_f1\": f1,\n            \"cm_accuracy\": accuracy,\n        }\n\n    def _build_process_evaluation(self) -&gt; ProcessEvaluation:\n        \"\"\"\n        Build ProcessEvaluation from current accumulated state.\n\n        Returns:\n            ProcessEvaluation with computed metrics from accumulated state\n        \"\"\"\n        # Calculate derived metrics for overall results\n        overall_cm = dict(self._confusion_matrix[\"overall\"])\n        overall_derived = self._calculate_derived_metrics(overall_cm)\n        overall_metrics = {**overall_cm, **overall_derived}\n\n        # Calculate derived metrics for each field\n        field_metrics = {}\n        for field_path, field_cm in self._confusion_matrix[\"fields\"].items():\n            field_cm_dict = dict(field_cm)\n            field_derived = self._calculate_derived_metrics(field_cm_dict)\n            field_metrics[field_path] = {**field_cm_dict, **field_derived}\n\n        total_time = time.time() - self._start_time\n\n        return ProcessEvaluation(\n            metrics=overall_metrics,\n            field_metrics=field_metrics,\n            errors=list(self._errors),  # Copy to avoid external modification\n            total_time=total_time,\n            non_matches=list(self._non_matches) if self.document_non_matches else None,\n        )\n\n    def save_metrics(self, filepath: str) -&gt; None:\n        \"\"\"\n        Save current accumulated metrics to a JSON file.\n\n        Args:\n            filepath: Path where metrics will be saved as JSON\n        \"\"\"\n        process_eval = self._build_process_evaluation()\n\n        # Build comprehensive metrics dictionary\n        metrics_data = {\n            \"overall_metrics\": process_eval.metrics,\n            \"field_metrics\": process_eval.field_metrics,\n            \"evaluation_summary\": {\n                \"total_documents_processed\": self._processed_count,\n                \"total_evaluation_time\": process_eval.total_time,\n                \"documents_per_second\": self._processed_count / process_eval.total_time\n                if process_eval.total_time &gt; 0\n                else 0,\n                \"error_count\": len(process_eval.errors),\n                \"error_rate\": len(process_eval.errors) / self._processed_count\n                if self._processed_count &gt; 0\n                else 0,\n                \"target_schema\": self.target_schema.__name__,\n            },\n            \"errors\": process_eval.errors,\n            \"metadata\": {\n                \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S UTC\", time.gmtime()),\n                \"evaluator_config\": {\n                    \"verbose\": self.verbose,\n                    \"document_non_matches\": self.document_non_matches,\n                    \"elide_errors\": self.elide_errors,\n                    \"individual_results_jsonl\": self.individual_results_jsonl,\n                },\n            },\n        }\n\n        # Ensure directory exists\n        import os\n\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n        # Write to file\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(metrics_data, f, indent=2, default=str)\n\n        if self.verbose:\n            print(f\"Metrics saved to: {filepath}\")\n\n    def pretty_print_metrics(self) -&gt; None:\n        \"\"\"\n        Pretty print current accumulated metrics in a format similar to StructuredModel.\n\n        Displays overall metrics, field-level metrics, and evaluation summary\n        in a human-readable format.\n        \"\"\"\n        process_eval = self._build_process_evaluation()\n\n        # Header\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"BULK EVALUATION RESULTS - {self.target_schema.__name__}\")\n        print(\"=\" * 80)\n\n        # Overall metrics\n        overall_metrics = process_eval.metrics\n        print(\"\\nOVERALL METRICS:\")\n        print(\"-\" * 40)\n        print(f\"Documents Processed: {self._processed_count:,}\")\n        print(f\"Evaluation Time: {process_eval.total_time:.2f}s\")\n        print(\n            f\"Processing Rate: {self._processed_count / process_eval.total_time:.1f} docs/sec\"\n            if process_eval.total_time &gt; 0\n            else \"Processing Rate: N/A\"\n        )\n\n        # Confusion matrix\n        print(\"\\nCONFUSION MATRIX:\")\n        print(f\"  True Positives (TP):    {overall_metrics.get('tp', 0):,}\")\n        print(f\"  False Positives (FP):   {overall_metrics.get('fp', 0):,}\")\n        print(f\"  True Negatives (TN):    {overall_metrics.get('tn', 0):,}\")\n        print(f\"  False Negatives (FN):   {overall_metrics.get('fn', 0):,}\")\n        print(f\"  False Discovery (FD):   {overall_metrics.get('fd', 0):,}\")\n        print(f\"  False Alarm (FA):   {overall_metrics.get('fa', 0):,}\")\n\n        # Derived metrics\n        print(\"\\nDERIVED METRICS:\")\n        print(f\"  Precision:     {overall_metrics.get('cm_precision', 0.0):.4f}\")\n        print(f\"  Recall:        {overall_metrics.get('cm_recall', 0.0):.4f}\")\n        print(f\"  F1 Score:      {overall_metrics.get('cm_f1', 0.0):.4f}\")\n        print(f\"  Accuracy:      {overall_metrics.get('cm_accuracy', 0.0):.4f}\")\n\n        # Field-level metrics\n        if process_eval.field_metrics:\n            print(\"\\nFIELD-LEVEL METRICS:\")\n            print(\"-\" * 40)\n\n            # Sort fields by F1 score descending for better readability\n            sorted_fields = sorted(\n                process_eval.field_metrics.items(),\n                key=lambda x: x[1].get(\"cm_f1\", 0.0),\n                reverse=True,\n            )\n\n            for field_path, field_metrics in sorted_fields:\n                tp = field_metrics.get(\"tp\", 0)\n                fp = field_metrics.get(\"fp\", 0)\n                fn = field_metrics.get(\"fn\", 0)\n                precision = field_metrics.get(\"cm_precision\", 0.0)\n                recall = field_metrics.get(\"cm_recall\", 0.0)\n                f1 = field_metrics.get(\"cm_f1\", 0.0)\n\n                # Only show fields with some activity\n                if tp + fp + fn &gt; 0:\n                    print(\n                        f\"  {field_path:30} P: {precision:.3f} | R: {recall:.3f} | F1: {f1:.3f} | TP: {tp:,} | FP: {fp:,} | FN: {fn:,}\"\n                    )\n\n        # Error summary\n        if process_eval.errors:\n            print(\"\\nERROR SUMMARY:\")\n            print(\"-\" * 40)\n            print(f\"Total Errors: {len(process_eval.errors):,}\")\n            print(\n                f\"Error Rate: {len(process_eval.errors) / self._processed_count * 100:.2f}%\"\n                if self._processed_count &gt; 0\n                else \"Error Rate: N/A\"\n            )\n\n            # Group errors by type\n            error_types = {}\n            for error in process_eval.errors:\n                error_type = error.get(\"error_type\", \"Unknown\")\n                error_types[error_type] = error_types.get(error_type, 0) + 1\n\n            if error_types:\n                print(\"Error Types:\")\n                for error_type, count in sorted(\n                    error_types.items(), key=lambda x: x[1], reverse=True\n                ):\n                    print(f\"  {error_type}: {count:,}\")\n\n        # Configuration info\n        print(\"\\nCONFIGURATION:\")\n        print(\"-\" * 40)\n        print(f\"Target Schema: {self.target_schema.__name__}\")\n        print(f\"Document Non-matches: {'Yes' if self.document_non_matches else 'No'}\")\n        print(f\"Elide Errors: {'Yes' if self.elide_errors else 'No'}\")\n        if self.individual_results_jsonl:\n            print(f\"Individual Results JSONL: {self.individual_results_jsonl}\")\n\n        print(\"=\" * 80)\n\n    def get_state(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get serializable state for checkpointing and recovery.\n\n        Returns a dictionary containing all internal state that can be serialized\n        and later restored using load_state(). This enables checkpointing for\n        long-running evaluation jobs.\n\n        Returns:\n            Dictionary containing serializable evaluator state\n        \"\"\"\n        return {\n            \"confusion_matrix\": {\n                \"overall\": dict(self._confusion_matrix[\"overall\"]),\n                \"fields\": {\n                    path: dict(metrics)\n                    for path, metrics in self._confusion_matrix[\"fields\"].items()\n                },\n            },\n            \"errors\": list(self._errors),\n            \"processed_count\": self._processed_count,\n            \"start_time\": self._start_time,\n            # Configuration\n            \"target_schema\": self.target_schema.__name__,\n            \"elide_errors\": self.elide_errors,\n        }\n\n    def load_state(self, state: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Restore evaluator state from serialized data.\n\n        This method restores the internal state from data previously saved\n        with get_state(), enabling recovery from checkpoints.\n\n        Args:\n            state: State dictionary from get_state()\n        \"\"\"\n        # Validate state compatibility\n        if state.get(\"target_schema\") != self.target_schema.__name__:\n            raise ValueError(\n                f\"State schema {state.get('target_schema')} doesn't match evaluator schema {self.target_schema.__name__}\"\n            )\n\n        # Restore confusion matrix state\n        cm_state = state[\"confusion_matrix\"]\n        self._confusion_matrix = {\n            \"overall\": defaultdict(int, cm_state[\"overall\"]),\n            \"fields\": defaultdict(lambda: defaultdict(int)),\n        }\n\n        for field_path, field_metrics in cm_state[\"fields\"].items():\n            self._confusion_matrix[\"fields\"][field_path] = defaultdict(\n                int, field_metrics\n            )\n\n        # Restore other state\n        self._errors = list(state[\"errors\"])\n        self._processed_count = state[\"processed_count\"]\n        self._start_time = state[\"start_time\"]\n\n        if self.verbose:\n            print(f\"Loaded state: {self._processed_count} documents processed\")\n\n    def merge_state(self, other_state: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Merge results from another evaluator instance.\n\n        This method enables distributed processing by merging confusion matrix\n        counts from multiple evaluator instances that processed different\n        portions of a dataset.\n\n        Args:\n            other_state: State dictionary from another evaluator instance\n        \"\"\"\n        # Validate compatibility\n        if other_state.get(\"target_schema\") != self.target_schema.__name__:\n            raise ValueError(\n                f\"Cannot merge incompatible schemas: {other_state.get('target_schema')} vs {self.target_schema.__name__}\"\n            )\n\n        # Merge overall metrics\n        other_cm = other_state[\"confusion_matrix\"]\n        for metric, value in other_cm[\"overall\"].items():\n            self._confusion_matrix[\"overall\"][metric] += value\n\n        # Merge field-level metrics\n        for field_path, field_metrics in other_cm[\"fields\"].items():\n            for metric, value in field_metrics.items():\n                self._confusion_matrix[\"fields\"][field_path][metric] += value\n\n        # Merge errors and counts\n        self._errors.extend(other_state[\"errors\"])\n        self._processed_count += other_state[\"processed_count\"]\n\n        if self.verbose:\n            print(\n                f\"Merged state: now {self._processed_count} total documents processed\"\n            )\n\n    # Legacy compatibility methods\n\n    def evaluate_dataframe(self, df) -&gt; ProcessEvaluation:\n        \"\"\"\n        Legacy compatibility method for DataFrame-based evaluation.\n\n        This method provides backward compatibility with the original DataFrame-based\n        API while leveraging the new stateful processing internally.\n\n        Args:\n            df: DataFrame with columns for ground truth and predictions\n\n        Returns:\n            ProcessEvaluation with aggregated results\n        \"\"\"\n        # Reset state for clean evaluation\n        self.reset()\n\n        # Process each row\n        for idx, row in df.iterrows():\n            doc_id = row.get(\"doc_id\", f\"row_{idx}\")\n\n            try:\n                # Parse JSON data\n                gt_data = json.loads(row[\"expected\"])\n                pred_data = json.loads(row[\"predicted\"])\n\n                # Create StructuredModel instances\n                gt_model = self.target_schema(**gt_data)\n                pred_model = self.target_schema(**pred_data)\n\n                # Process using stateful update\n                self.update(gt_model, pred_model, doc_id)\n\n            except Exception as e:\n                if self.verbose:\n                    print(f\"Error processing row {idx}: {e}\")\n                continue\n\n        return self.compute()\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.__init__","title":"<code>__init__(target_schema, verbose=False, document_non_matches=True, elide_errors=False, individual_results_jsonl=None)</code>","text":"<p>Initialize the stateful bulk evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>target_schema</code> <code>Type[StructuredModel]</code> <p>StructuredModel class for validation and processing</p> required <code>verbose</code> <code>bool</code> <p>Whether to print detailed progress information</p> <code>False</code> <code>document_non_matches</code> <code>bool</code> <p>Whether to document detailed non-match information</p> <code>True</code> <code>elide_errors</code> <code>bool</code> <p>If True, skip documents with errors; if False, accumulate error metrics</p> <code>False</code> <code>individual_results_jsonl</code> <code>Optional[str]</code> <p>Optional path to JSONL file for appending individual comparison results</p> <code>None</code> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def __init__(\n    self,\n    target_schema: Type[StructuredModel],\n    verbose: bool = False,\n    document_non_matches: bool = True,\n    elide_errors: bool = False,\n    individual_results_jsonl: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the stateful bulk evaluator.\n\n    Args:\n        target_schema: StructuredModel class for validation and processing\n        verbose: Whether to print detailed progress information\n        document_non_matches: Whether to document detailed non-match information\n        elide_errors: If True, skip documents with errors; if False, accumulate error metrics\n        individual_results_jsonl: Optional path to JSONL file for appending individual comparison results\n    \"\"\"\n    self.target_schema = target_schema\n    self.verbose = verbose\n    self.document_non_matches = document_non_matches\n    self.elide_errors = elide_errors\n    self.individual_results_jsonl = individual_results_jsonl\n\n    # Initialize state\n    self.reset()\n\n    if self.verbose:\n        print(\n            f\"Initialized BulkStructuredModelEvaluator for {target_schema.__name__}\"\n        )\n        if self.individual_results_jsonl:\n            print(\n                f\"Individual results will be appended to: {self.individual_results_jsonl}\"\n            )\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.compute","title":"<code>compute()</code>","text":"<p>Calculate final aggregated metrics from accumulated state.</p> <p>This method performs the final computation of all derived metrics from the accumulated confusion matrix state, similar to PyTorch Lightning's training_epoch_end pattern.</p> <p>Returns:</p> Type Description <code>ProcessEvaluation</code> <p>ProcessEvaluation with final aggregated metrics</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def compute(self) -&gt; ProcessEvaluation:\n    \"\"\"\n    Calculate final aggregated metrics from accumulated state.\n\n    This method performs the final computation of all derived metrics from\n    the accumulated confusion matrix state, similar to PyTorch Lightning's\n    training_epoch_end pattern.\n\n    Returns:\n        ProcessEvaluation with final aggregated metrics\n    \"\"\"\n    result = self._build_process_evaluation()\n\n    if self.verbose:\n        total_time = time.time() - self._start_time\n        print(\n            f\"Final computation completed: {self._processed_count} documents in {total_time:.2f}s\"\n        )\n        print(f\"Overall accuracy: {result.metrics.get('cm_accuracy', 0.0):.3f}\")\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.evaluate_dataframe","title":"<code>evaluate_dataframe(df)</code>","text":"<p>Legacy compatibility method for DataFrame-based evaluation.</p> <p>This method provides backward compatibility with the original DataFrame-based API while leveraging the new stateful processing internally.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>DataFrame with columns for ground truth and predictions</p> required <p>Returns:</p> Type Description <code>ProcessEvaluation</code> <p>ProcessEvaluation with aggregated results</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def evaluate_dataframe(self, df) -&gt; ProcessEvaluation:\n    \"\"\"\n    Legacy compatibility method for DataFrame-based evaluation.\n\n    This method provides backward compatibility with the original DataFrame-based\n    API while leveraging the new stateful processing internally.\n\n    Args:\n        df: DataFrame with columns for ground truth and predictions\n\n    Returns:\n        ProcessEvaluation with aggregated results\n    \"\"\"\n    # Reset state for clean evaluation\n    self.reset()\n\n    # Process each row\n    for idx, row in df.iterrows():\n        doc_id = row.get(\"doc_id\", f\"row_{idx}\")\n\n        try:\n            # Parse JSON data\n            gt_data = json.loads(row[\"expected\"])\n            pred_data = json.loads(row[\"predicted\"])\n\n            # Create StructuredModel instances\n            gt_model = self.target_schema(**gt_data)\n            pred_model = self.target_schema(**pred_data)\n\n            # Process using stateful update\n            self.update(gt_model, pred_model, doc_id)\n\n        except Exception as e:\n            if self.verbose:\n                print(f\"Error processing row {idx}: {e}\")\n            continue\n\n    return self.compute()\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.get_current_metrics","title":"<code>get_current_metrics()</code>","text":"<p>Get current accumulated metrics without clearing state.</p> <p>This method allows monitoring evaluation progress by returning current metrics computed from accumulated state. Unlike compute(), this does not clear the internal state.</p> <p>Returns:</p> Type Description <code>ProcessEvaluation</code> <p>ProcessEvaluation with current accumulated metrics</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def get_current_metrics(self) -&gt; ProcessEvaluation:\n    \"\"\"\n    Get current accumulated metrics without clearing state.\n\n    This method allows monitoring evaluation progress by returning current\n    metrics computed from accumulated state. Unlike compute(), this does\n    not clear the internal state.\n\n    Returns:\n        ProcessEvaluation with current accumulated metrics\n    \"\"\"\n    return self._build_process_evaluation()\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.get_state","title":"<code>get_state()</code>","text":"<p>Get serializable state for checkpointing and recovery.</p> <p>Returns a dictionary containing all internal state that can be serialized and later restored using load_state(). This enables checkpointing for long-running evaluation jobs.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing serializable evaluator state</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def get_state(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get serializable state for checkpointing and recovery.\n\n    Returns a dictionary containing all internal state that can be serialized\n    and later restored using load_state(). This enables checkpointing for\n    long-running evaluation jobs.\n\n    Returns:\n        Dictionary containing serializable evaluator state\n    \"\"\"\n    return {\n        \"confusion_matrix\": {\n            \"overall\": dict(self._confusion_matrix[\"overall\"]),\n            \"fields\": {\n                path: dict(metrics)\n                for path, metrics in self._confusion_matrix[\"fields\"].items()\n            },\n        },\n        \"errors\": list(self._errors),\n        \"processed_count\": self._processed_count,\n        \"start_time\": self._start_time,\n        # Configuration\n        \"target_schema\": self.target_schema.__name__,\n        \"elide_errors\": self.elide_errors,\n    }\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.load_state","title":"<code>load_state(state)</code>","text":"<p>Restore evaluator state from serialized data.</p> <p>This method restores the internal state from data previously saved with get_state(), enabling recovery from checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Dict[str, Any]</code> <p>State dictionary from get_state()</p> required Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def load_state(self, state: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Restore evaluator state from serialized data.\n\n    This method restores the internal state from data previously saved\n    with get_state(), enabling recovery from checkpoints.\n\n    Args:\n        state: State dictionary from get_state()\n    \"\"\"\n    # Validate state compatibility\n    if state.get(\"target_schema\") != self.target_schema.__name__:\n        raise ValueError(\n            f\"State schema {state.get('target_schema')} doesn't match evaluator schema {self.target_schema.__name__}\"\n        )\n\n    # Restore confusion matrix state\n    cm_state = state[\"confusion_matrix\"]\n    self._confusion_matrix = {\n        \"overall\": defaultdict(int, cm_state[\"overall\"]),\n        \"fields\": defaultdict(lambda: defaultdict(int)),\n    }\n\n    for field_path, field_metrics in cm_state[\"fields\"].items():\n        self._confusion_matrix[\"fields\"][field_path] = defaultdict(\n            int, field_metrics\n        )\n\n    # Restore other state\n    self._errors = list(state[\"errors\"])\n    self._processed_count = state[\"processed_count\"]\n    self._start_time = state[\"start_time\"]\n\n    if self.verbose:\n        print(f\"Loaded state: {self._processed_count} documents processed\")\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.merge_state","title":"<code>merge_state(other_state)</code>","text":"<p>Merge results from another evaluator instance.</p> <p>This method enables distributed processing by merging confusion matrix counts from multiple evaluator instances that processed different portions of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>other_state</code> <code>Dict[str, Any]</code> <p>State dictionary from another evaluator instance</p> required Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def merge_state(self, other_state: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Merge results from another evaluator instance.\n\n    This method enables distributed processing by merging confusion matrix\n    counts from multiple evaluator instances that processed different\n    portions of a dataset.\n\n    Args:\n        other_state: State dictionary from another evaluator instance\n    \"\"\"\n    # Validate compatibility\n    if other_state.get(\"target_schema\") != self.target_schema.__name__:\n        raise ValueError(\n            f\"Cannot merge incompatible schemas: {other_state.get('target_schema')} vs {self.target_schema.__name__}\"\n        )\n\n    # Merge overall metrics\n    other_cm = other_state[\"confusion_matrix\"]\n    for metric, value in other_cm[\"overall\"].items():\n        self._confusion_matrix[\"overall\"][metric] += value\n\n    # Merge field-level metrics\n    for field_path, field_metrics in other_cm[\"fields\"].items():\n        for metric, value in field_metrics.items():\n            self._confusion_matrix[\"fields\"][field_path][metric] += value\n\n    # Merge errors and counts\n    self._errors.extend(other_state[\"errors\"])\n    self._processed_count += other_state[\"processed_count\"]\n\n    if self.verbose:\n        print(\n            f\"Merged state: now {self._processed_count} total documents processed\"\n        )\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.pretty_print_metrics","title":"<code>pretty_print_metrics()</code>","text":"<p>Pretty print current accumulated metrics in a format similar to StructuredModel.</p> <p>Displays overall metrics, field-level metrics, and evaluation summary in a human-readable format.</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def pretty_print_metrics(self) -&gt; None:\n    \"\"\"\n    Pretty print current accumulated metrics in a format similar to StructuredModel.\n\n    Displays overall metrics, field-level metrics, and evaluation summary\n    in a human-readable format.\n    \"\"\"\n    process_eval = self._build_process_evaluation()\n\n    # Header\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"BULK EVALUATION RESULTS - {self.target_schema.__name__}\")\n    print(\"=\" * 80)\n\n    # Overall metrics\n    overall_metrics = process_eval.metrics\n    print(\"\\nOVERALL METRICS:\")\n    print(\"-\" * 40)\n    print(f\"Documents Processed: {self._processed_count:,}\")\n    print(f\"Evaluation Time: {process_eval.total_time:.2f}s\")\n    print(\n        f\"Processing Rate: {self._processed_count / process_eval.total_time:.1f} docs/sec\"\n        if process_eval.total_time &gt; 0\n        else \"Processing Rate: N/A\"\n    )\n\n    # Confusion matrix\n    print(\"\\nCONFUSION MATRIX:\")\n    print(f\"  True Positives (TP):    {overall_metrics.get('tp', 0):,}\")\n    print(f\"  False Positives (FP):   {overall_metrics.get('fp', 0):,}\")\n    print(f\"  True Negatives (TN):    {overall_metrics.get('tn', 0):,}\")\n    print(f\"  False Negatives (FN):   {overall_metrics.get('fn', 0):,}\")\n    print(f\"  False Discovery (FD):   {overall_metrics.get('fd', 0):,}\")\n    print(f\"  False Alarm (FA):   {overall_metrics.get('fa', 0):,}\")\n\n    # Derived metrics\n    print(\"\\nDERIVED METRICS:\")\n    print(f\"  Precision:     {overall_metrics.get('cm_precision', 0.0):.4f}\")\n    print(f\"  Recall:        {overall_metrics.get('cm_recall', 0.0):.4f}\")\n    print(f\"  F1 Score:      {overall_metrics.get('cm_f1', 0.0):.4f}\")\n    print(f\"  Accuracy:      {overall_metrics.get('cm_accuracy', 0.0):.4f}\")\n\n    # Field-level metrics\n    if process_eval.field_metrics:\n        print(\"\\nFIELD-LEVEL METRICS:\")\n        print(\"-\" * 40)\n\n        # Sort fields by F1 score descending for better readability\n        sorted_fields = sorted(\n            process_eval.field_metrics.items(),\n            key=lambda x: x[1].get(\"cm_f1\", 0.0),\n            reverse=True,\n        )\n\n        for field_path, field_metrics in sorted_fields:\n            tp = field_metrics.get(\"tp\", 0)\n            fp = field_metrics.get(\"fp\", 0)\n            fn = field_metrics.get(\"fn\", 0)\n            precision = field_metrics.get(\"cm_precision\", 0.0)\n            recall = field_metrics.get(\"cm_recall\", 0.0)\n            f1 = field_metrics.get(\"cm_f1\", 0.0)\n\n            # Only show fields with some activity\n            if tp + fp + fn &gt; 0:\n                print(\n                    f\"  {field_path:30} P: {precision:.3f} | R: {recall:.3f} | F1: {f1:.3f} | TP: {tp:,} | FP: {fp:,} | FN: {fn:,}\"\n                )\n\n    # Error summary\n    if process_eval.errors:\n        print(\"\\nERROR SUMMARY:\")\n        print(\"-\" * 40)\n        print(f\"Total Errors: {len(process_eval.errors):,}\")\n        print(\n            f\"Error Rate: {len(process_eval.errors) / self._processed_count * 100:.2f}%\"\n            if self._processed_count &gt; 0\n            else \"Error Rate: N/A\"\n        )\n\n        # Group errors by type\n        error_types = {}\n        for error in process_eval.errors:\n            error_type = error.get(\"error_type\", \"Unknown\")\n            error_types[error_type] = error_types.get(error_type, 0) + 1\n\n        if error_types:\n            print(\"Error Types:\")\n            for error_type, count in sorted(\n                error_types.items(), key=lambda x: x[1], reverse=True\n            ):\n                print(f\"  {error_type}: {count:,}\")\n\n    # Configuration info\n    print(\"\\nCONFIGURATION:\")\n    print(\"-\" * 40)\n    print(f\"Target Schema: {self.target_schema.__name__}\")\n    print(f\"Document Non-matches: {'Yes' if self.document_non_matches else 'No'}\")\n    print(f\"Elide Errors: {'Yes' if self.elide_errors else 'No'}\")\n    if self.individual_results_jsonl:\n        print(f\"Individual Results JSONL: {self.individual_results_jsonl}\")\n\n    print(\"=\" * 80)\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.reset","title":"<code>reset()</code>","text":"<p>Clear all accumulated state and start fresh evaluation.</p> <p>This method resets all internal counters, metrics, and error tracking to initial state, enabling reuse of the same evaluator instance for multiple evaluation runs.</p> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Clear all accumulated state and start fresh evaluation.\n\n    This method resets all internal counters, metrics, and error tracking\n    to initial state, enabling reuse of the same evaluator instance for\n    multiple evaluation runs.\n    \"\"\"\n    # Accumulated confusion matrix state using nested defaultdicts\n    self._confusion_matrix = {\n        \"overall\": defaultdict(int),\n        \"fields\": defaultdict(lambda: defaultdict(int)),\n    }\n\n    # Non-match tracking (when document_non_matches=True)\n    self._non_matches = []\n\n    # Error tracking\n    self._errors = []\n\n    # Processing statistics\n    self._processed_count = 0\n    self._start_time = time.time()\n\n    if self.verbose:\n        print(\"Reset evaluator state\")\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.save_metrics","title":"<code>save_metrics(filepath)</code>","text":"<p>Save current accumulated metrics to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where metrics will be saved as JSON</p> required Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def save_metrics(self, filepath: str) -&gt; None:\n    \"\"\"\n    Save current accumulated metrics to a JSON file.\n\n    Args:\n        filepath: Path where metrics will be saved as JSON\n    \"\"\"\n    process_eval = self._build_process_evaluation()\n\n    # Build comprehensive metrics dictionary\n    metrics_data = {\n        \"overall_metrics\": process_eval.metrics,\n        \"field_metrics\": process_eval.field_metrics,\n        \"evaluation_summary\": {\n            \"total_documents_processed\": self._processed_count,\n            \"total_evaluation_time\": process_eval.total_time,\n            \"documents_per_second\": self._processed_count / process_eval.total_time\n            if process_eval.total_time &gt; 0\n            else 0,\n            \"error_count\": len(process_eval.errors),\n            \"error_rate\": len(process_eval.errors) / self._processed_count\n            if self._processed_count &gt; 0\n            else 0,\n            \"target_schema\": self.target_schema.__name__,\n        },\n        \"errors\": process_eval.errors,\n        \"metadata\": {\n            \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S UTC\", time.gmtime()),\n            \"evaluator_config\": {\n                \"verbose\": self.verbose,\n                \"document_non_matches\": self.document_non_matches,\n                \"elide_errors\": self.elide_errors,\n                \"individual_results_jsonl\": self.individual_results_jsonl,\n            },\n        },\n    }\n\n    # Ensure directory exists\n    import os\n\n    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n\n    # Write to file\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metrics_data, f, indent=2, default=str)\n\n    if self.verbose:\n        print(f\"Metrics saved to: {filepath}\")\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.update","title":"<code>update(gt_model, pred_model, doc_id=None)</code>","text":"<p>Process a single document pair and accumulate the results in internal state.</p> <p>This is the core method for stateful evaluation, inspired by PyTorch Lightning's training_step pattern. Each call processes one document pair and updates the internal confusion matrix counters.</p> <p>Parameters:</p> Name Type Description Default <code>gt_model</code> <code>StructuredModel</code> <p>Ground truth StructuredModel instance</p> required <code>pred_model</code> <code>StructuredModel</code> <p>Predicted StructuredModel instance</p> required <code>doc_id</code> <code>Optional[str]</code> <p>Optional document identifier for error tracking</p> <code>None</code> Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def update(\n    self,\n    gt_model: StructuredModel,\n    pred_model: StructuredModel,\n    doc_id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Process a single document pair and accumulate the results in internal state.\n\n    This is the core method for stateful evaluation, inspired by PyTorch Lightning's\n    training_step pattern. Each call processes one document pair and updates\n    the internal confusion matrix counters.\n\n    Args:\n        gt_model: Ground truth StructuredModel instance\n        pred_model: Predicted StructuredModel instance\n        doc_id: Optional document identifier for error tracking\n    \"\"\"\n    if doc_id is None:\n        doc_id = f\"doc_{self._processed_count}\"\n\n    try:\n        # Use compare_with method directly on the StructuredModel\n        # Pass document_non_matches to achieve parity with compare_with method\n        comparison_result = gt_model.compare_with(\n            pred_model,\n            include_confusion_matrix=True,\n            document_non_matches=self.document_non_matches,\n        )\n\n        # Collect non-matches if enabled\n        if self.document_non_matches and \"non_matches\" in comparison_result:\n            # Add doc_id to each non-match for bulk tracking\n            for non_match in comparison_result[\"non_matches\"]:\n                non_match_with_doc = non_match.copy()\n                non_match_with_doc[\"doc_id\"] = doc_id\n                self._non_matches.append(non_match_with_doc)\n\n        # Simple JSONL append of raw comparison result (before any processing)\n        if self.individual_results_jsonl:\n            record = {\"doc_id\": doc_id, \"comparison_result\": comparison_result}\n            with open(self.individual_results_jsonl, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(record) + \"\\n\")\n\n        # Accumulate the results into our state (this flattens for aggregation)\n        self._accumulate_confusion_matrix(comparison_result[\"confusion_matrix\"])\n\n        self._processed_count += 1\n\n        if self.verbose and self._processed_count % 1000 == 0:\n            elapsed = time.time() - self._start_time\n            print(f\"Processed {self._processed_count} documents ({elapsed:.2f}s)\")\n\n    except Exception as e:\n        error_record = {\n            \"doc_id\": doc_id,\n            \"error\": str(e),\n            \"error_type\": type(e).__name__,\n        }\n\n        if not self.elide_errors:\n            self._errors.append(error_record)\n\n            # For errors, add a \"failed\" classification to overall metrics\n            # This represents complete failure to process the document\n            self._confusion_matrix[\"overall\"][\"fn\"] += 1\n\n        if self.verbose:\n            print(f\"Error processing document {doc_id}: {str(e)}\")\n</code></pre>"},{"location":"SDK-Docs/evaluator/#stickler.structured_object_evaluator.bulk_structured_model_evaluator.BulkStructuredModelEvaluator.update_batch","title":"<code>update_batch(batch_data)</code>","text":"<p>Process multiple document pairs efficiently in a batch.</p> <p>This method provides efficient batch processing by calling update() multiple times with optional garbage collection for memory management.</p> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>List[Tuple[StructuredModel, StructuredModel, Optional[str]]]</code> <p>List of tuples containing (gt_model, pred_model, doc_id)</p> required Source code in <code>stickler/structured_object_evaluator/bulk_structured_model_evaluator.py</code> <pre><code>def update_batch(\n    self, batch_data: List[Tuple[StructuredModel, StructuredModel, Optional[str]]]\n) -&gt; None:\n    \"\"\"\n    Process multiple document pairs efficiently in a batch.\n\n    This method provides efficient batch processing by calling update()\n    multiple times with optional garbage collection for memory management.\n\n    Args:\n        batch_data: List of tuples containing (gt_model, pred_model, doc_id)\n    \"\"\"\n    batch_start = self._processed_count\n\n    for gt_model, pred_model, doc_id in batch_data:\n        self.update(gt_model, pred_model, doc_id)\n\n    # Garbage collection for large batches\n    if len(batch_data) &gt;= 1000:\n        gc.collect()\n\n    if self.verbose:\n        batch_size = self._processed_count - batch_start\n        print(f\"Processed batch of {batch_size} documents\")\n</code></pre>"},{"location":"SDK-Docs/models/","title":"Models","text":""},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models","title":"<code>stickler.structured_object_evaluator.models</code>","text":"<p>Models for structured object evaluation.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model","title":"<code>stickler.structured_object_evaluator.models.structured_model</code>","text":"<p>Structured model comparison using Pydantic models.</p> <p>This module provides the StructuredModel class for defining structured data models with comparison configuration and evaluation capabilities.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel","title":"<code>stickler.structured_object_evaluator.models.structured_model.StructuredModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for models with structured comparison capabilities.</p> <p>This class extends Pydantic's BaseModel with the ability to compare instances using configurable comparison metrics for each field. It supports: - Field-level comparison configuration - Nested model comparison - Integration with ANLS* comparators - JSON schema generation with comparison metadata - Unordered list comparison using Hungarian matching - Retention of extra fields not defined in the model</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>class StructuredModel(BaseModel):\n    \"\"\"Base class for models with structured comparison capabilities.\n\n    This class extends Pydantic's BaseModel with the ability to compare\n    instances using configurable comparison metrics for each field.\n    It supports:\n    - Field-level comparison configuration\n    - Nested model comparison\n    - Integration with ANLS* comparators\n    - JSON schema generation with comparison metadata\n    - Unordered list comparison using Hungarian matching\n    - Retention of extra fields not defined in the model\n    \"\"\"\n\n    # Default match threshold - can be overridden in subclasses\n    match_threshold: ClassVar[float] = 0.7\n\n    extra_fields: Dict[str, Any] = Field(default_factory=dict, exclude=True)\n\n    model_config = {\n        \"arbitrary_types_allowed\": True,\n        \"extra\": \"allow\",  # Allow extra fields to be stored in extra_fields\n    }\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Validate field configurations when a StructuredModel subclass is defined.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Validate field configurations using class annotations since model_fields isn't populated yet\n        if hasattr(cls, \"__annotations__\"):\n            for field_name, field_type in cls.__annotations__.items():\n                if field_name == \"extra_fields\":\n                    continue\n\n                # Get the field default value if it exists\n                field_default = getattr(cls, field_name, None)\n\n                # Since ComparableField is now always a function that returns a Field,\n                # we need to check if field_default has comparison metadata\n                if hasattr(field_default, \"json_schema_extra\") and callable(\n                    field_default.json_schema_extra\n                ):\n                    # Check for comparison metadata\n                    temp_schema = {}\n                    field_default.json_schema_extra(temp_schema)\n                    if \"x-comparison\" in temp_schema:\n                        # This field was created with ComparableField function - validate constraints\n                        if cls._is_list_of_structured_model_type(field_type):\n                            comparison_config = temp_schema[\"x-comparison\"]\n\n                            # Threshold validation - only flag if explicitly set to non-default value\n                            threshold = comparison_config.get(\"threshold\", 0.5)\n                            if threshold != 0.5:  # Default threshold value\n                                raise ValueError(\n                                    f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                    f\"'threshold' parameter in ComparableField. Hungarian matching uses each \"\n                                    f\"StructuredModel's 'match_threshold' class attribute instead. \"\n                                    f\"Set 'match_threshold = {threshold}' on the list element class.\"\n                                )\n\n                            # Comparator validation - only flag if explicitly set to non-default type\n                            comparator_type = comparison_config.get(\n                                \"comparator_type\", \"LevenshteinComparator\"\n                            )\n                            if (\n                                comparator_type != \"LevenshteinComparator\"\n                            ):  # Default comparator type\n                                raise ValueError(\n                                    f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                    f\"'comparator' parameter in ComparableField. Object comparison uses each \"\n                                    f\"StructuredModel's individual field comparators instead.\"\n                                )\n\n    @classmethod\n    def _is_list_of_structured_model_type(cls, field_type) -&gt; bool:\n        \"\"\"Check if a field type annotation represents List[StructuredModel].\n\n        Args:\n            field_type: The field type annotation\n\n        Returns:\n            True if the field is a List[StructuredModel] type\n        \"\"\"\n        # Handle direct imports and typing constructs\n        origin = get_origin(field_type)\n        if origin is list or origin is List:\n            args = get_args(field_type)\n            if args:\n                element_type = args[0]\n                # Check if element type is a StructuredModel subclass\n                try:\n                    return inspect.isclass(element_type) and issubclass(\n                        element_type, StructuredModel\n                    )\n                except (TypeError, AttributeError):\n                    return False\n\n        # Handle Union types (like Optional[List[StructuredModel]])\n        elif origin is Union:\n            args = get_args(field_type)\n            for arg in args:\n                if cls._is_list_of_structured_model_type(arg):\n                    return True\n\n        return False\n\n    @classmethod\n    def from_json(cls, json_data: Dict[str, Any]) -&gt; \"StructuredModel\":\n        \"\"\"Create a StructuredModel instance from JSON data.\n\n        This method handles missing fields gracefully and stores extra fields\n        in the extra_fields attribute.\n\n        Args:\n            json_data: Dictionary containing the JSON data\n\n        Returns:\n            StructuredModel instance created from the JSON data\n        \"\"\"\n        return ConfigurationHelper.from_json(cls, json_data)\n\n    @classmethod\n    def model_from_json(cls, config: Dict[str, Any]) -&gt; Type[\"StructuredModel\"]:\n        \"\"\"Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().\n\n        This method leverages Pydantic's native dynamic model creation capabilities to ensure\n        full compatibility with all Pydantic features while adding structured comparison\n        functionality through inherited StructuredModel methods.\n\n        The generated model inherits all StructuredModel capabilities:\n        - compare_with() method for detailed comparisons\n        - Field-level comparison configuration\n        - Hungarian algorithm for list matching\n        - Confusion matrix generation\n        - JSON schema with comparison metadata\n\n        Args:\n            config: JSON configuration with fields, comparators, and model settings.\n                   Required keys:\n                   - fields: Dict mapping field names to field configurations\n                   Optional keys:\n                   - model_name: Name for the generated class (default: \"DynamicModel\")\n                   - match_threshold: Overall matching threshold (default: 0.7)\n\n                   Field configuration format:\n                   {\n                       \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required\n                       \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional\n                       \"threshold\": 0.8,  # Optional, default 0.5\n                       \"weight\": 2.0,     # Optional, default 1.0\n                       \"required\": true,  # Optional, default false\n                       \"default\": \"value\", # Optional\n                       \"description\": \"Field description\",  # Optional\n                       \"alias\": \"field_alias\",  # Optional\n                       \"examples\": [\"example1\", \"example2\"]  # Optional\n                   }\n\n        Returns:\n            A fully functional StructuredModel subclass created with create_model()\n\n        Raises:\n            ValueError: If configuration is invalid or contains unsupported types/comparators\n            KeyError: If required configuration keys are missing\n\n        Examples:\n            &gt;&gt;&gt; config = {\n            ...     \"model_name\": \"Product\",\n            ...     \"match_threshold\": 0.8,\n            ...     \"fields\": {\n            ...         \"name\": {\n            ...             \"type\": \"str\",\n            ...             \"comparator\": \"LevenshteinComparator\",\n            ...             \"threshold\": 0.8,\n            ...             \"weight\": 2.0,\n            ...             \"required\": True\n            ...         },\n            ...         \"price\": {\n            ...             \"type\": \"float\",\n            ...             \"comparator\": \"NumericComparator\",\n            ...             \"default\": 0.0\n            ...         }\n            ...     }\n            ... }\n            &gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n            &gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\n            True\n            &gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n            &gt;&gt;&gt; product.name\n            'Widget'\n            &gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n            &gt;&gt;&gt; result[\"overall_score\"]\n            1.0\n        \"\"\"\n        from pydantic import create_model\n        from .field_converter import convert_fields_config, validate_fields_config\n\n        # Validate configuration structure\n        if not isinstance(config, dict):\n            raise ValueError(\"Configuration must be a dictionary\")\n\n        if \"fields\" not in config:\n            raise ValueError(\"Configuration must contain 'fields' key\")\n\n        fields_config = config[\"fields\"]\n        if not isinstance(fields_config, dict) or len(fields_config) == 0:\n            raise ValueError(\"'fields' must be a non-empty dictionary\")\n\n        # Validate all field configurations before proceeding (including nested schema validation)\n        try:\n            from .field_converter import get_global_converter\n\n            converter = get_global_converter()\n\n            # First validate basic field configurations\n            validate_fields_config(fields_config)\n\n            # Then validate nested schema rules\n            for field_name, field_config in fields_config.items():\n                converter.validate_nested_field_schema(field_name, field_config)\n\n        except ValueError as e:\n            raise ValueError(f\"Invalid field configuration: {e}\")\n\n        # Extract model configuration\n        model_name = config.get(\"model_name\", \"DynamicModel\")\n        match_threshold = config.get(\"match_threshold\", 0.7)\n\n        # Validate model name\n        if not isinstance(model_name, str) or not model_name.isidentifier():\n            raise ValueError(\n                f\"model_name must be a valid Python identifier, got: {model_name}\"\n            )\n\n        # Validate match threshold\n        if not isinstance(match_threshold, (int, float)) or not (\n            0.0 &lt;= match_threshold &lt;= 1.0\n        ):\n            raise ValueError(\n                f\"match_threshold must be a number between 0.0 and 1.0, got: {match_threshold}\"\n            )\n\n        # Convert field configurations to Pydantic field definitions\n        try:\n            field_definitions = convert_fields_config(fields_config)\n        except ValueError as e:\n            raise ValueError(f\"Error converting field configurations: {e}\")\n\n        # Create the dynamic model extending StructuredModel\n        try:\n            DynamicClass = create_model(\n                model_name,\n                __base__=cls,  # Extend StructuredModel\n                **field_definitions,\n            )\n        except Exception as e:\n            raise ValueError(f\"Error creating dynamic model: {e}\")\n\n        # Set class-level attributes\n        DynamicClass.match_threshold = match_threshold\n\n        # Add configuration metadata for debugging/introspection\n        DynamicClass._model_config = config\n\n        return DynamicClass\n\n    @classmethod\n    def _is_structured_field_type(cls, field_info) -&gt; bool:\n        \"\"\"Check if a field represents a structured type that needs special handling.\n\n        Args:\n            field_info: Pydantic field info object\n\n        Returns:\n            True if the field is a List[StructuredModel] or StructuredModel type\n        \"\"\"\n        return ConfigurationHelper.is_structured_field_type(field_info)\n\n    @classmethod\n    def _get_comparison_info(cls, field_name: str) -&gt; ComparableField:\n        \"\"\"Extract comparison info from a field.\n\n        Args:\n            field_name: Name of the field to get comparison info for\n\n        Returns:\n            ComparableField object with comparison configuration\n        \"\"\"\n        return ConfigurationHelper.get_comparison_info(cls, field_name)\n\n    # Remove legacy ComparableField handling since ComparableField is now always a function\n    # that returns proper Pydantic Fields\n    pass\n\n    # No special __init__ needed since ComparableField is now always a function\n    # that returns proper Pydantic Fields\n    pass\n\n    @classmethod\n    def _is_aggregate_field(cls, field_name: str) -&gt; bool:\n        \"\"\"Check if field is marked for confusion matrix aggregation.\n\n        Args:\n            field_name: Name of the field to check\n\n        Returns:\n            True if the field is marked for aggregation, False otherwise\n        \"\"\"\n        return ConfigurationHelper.is_aggregate_field(cls, field_name)\n\n    def _is_truly_null(self, val: Any) -&gt; bool:\n        \"\"\"Check if a value is truly null (None).\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None, False otherwise\n        \"\"\"\n        return val is None\n\n    def _should_use_hierarchical_structure(self, val: Any, field_name: str) -&gt; bool:\n        \"\"\"Check if a list value should maintain hierarchical structure.\n\n        For lists, we need to check if they should maintain hierarchical structure\n        based on their field type configuration.\n\n        Args:\n            val: Value to check (typically a list)\n            field_name: Name of the field being evaluated\n\n        Returns:\n            True if the value should use hierarchical structure, False otherwise\n        \"\"\"\n        if isinstance(val, list):\n            # Check if this field is configured as List[StructuredModel]\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                return True\n        return False\n\n    def _is_effectively_null_for_lists(self, val: Any) -&gt; bool:\n        \"\"\"Check if a list value is effectively null (None or empty list).\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None or an empty list, False otherwise\n        \"\"\"\n        return val is None or (isinstance(val, list) and len(val) == 0)\n\n    def _is_effectively_null_for_primitives(self, val: Any) -&gt; bool:\n        \"\"\"Check if a primitive value is effectively null.\n\n        Treats empty strings and None as equivalent for string fields.\n\n        Args:\n            val: Value to check\n\n        Returns:\n            True if the value is None or an empty string, False otherwise\n        \"\"\"\n        return val is None or (isinstance(val, str) and val == \"\")\n\n    def _is_list_field(self, field_name: str) -&gt; bool:\n        \"\"\"Check if a field is ANY list type.\n\n        Args:\n            field_name: Name of the field to check\n\n        Returns:\n            True if the field is a list type (List[str], List[StructuredModel], etc.)\n        \"\"\"\n        field_info = self.__class__.model_fields.get(field_name)\n        if not field_info:\n            return False\n\n        field_type = field_info.annotation\n        # Handle Optional types and direct List types\n        if hasattr(field_type, \"__origin__\"):\n            origin = field_type.__origin__\n            if origin is list or origin is List:\n                return True\n            elif origin is Union:  # Optional[List[...]] case\n                args = field_type.__args__\n                for arg in args:\n                    if hasattr(arg, \"__origin__\") and (\n                        arg.__origin__ is list or arg.__origin__ is List\n                    ):\n                        return True\n        return False\n\n    def _handle_list_field_dispatch(\n        self, gt_val: Any, pred_val: Any, weight: float\n    ) -&gt; dict:\n        \"\"\"Handle list field comparison using match statements.\n\n        Args:\n            gt_val: Ground truth list value\n            pred_val: Predicted list value\n            weight: Field weight for scoring\n\n        Returns:\n            Comparison result dictionary\n        \"\"\"\n        gt_effectively_null = self._is_effectively_null_for_lists(gt_val)\n        pred_effectively_null = self._is_effectively_null_for_lists(pred_val)\n\n        match (gt_effectively_null, pred_effectively_null):\n            case (True, True):\n                # Both None or empty lists \u2192 True Negative\n                return {\n                    \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                    \"fields\": {},\n                    \"raw_similarity_score\": 1.0,\n                    \"similarity_score\": 1.0,\n                    \"threshold_applied_score\": 1.0,\n                    \"weight\": weight,\n                }\n            case (True, False):\n                # GT=None/empty, Pred=populated list \u2192 False Alarm\n                pred_list = pred_val if isinstance(pred_val, list) else []\n                fa_count = (\n                    len(pred_list) if pred_list else 1\n                )  # At least 1 FA for the field itself\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": fa_count,\n                        \"fd\": 0,\n                        \"fp\": fa_count,\n                        \"tn\": 0,\n                        \"fn\": 0,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case (False, True):\n                # GT=populated list, Pred=None/empty \u2192 False Negative\n                gt_list = gt_val if isinstance(gt_val, list) else []\n                fn_count = (\n                    len(gt_list) if gt_list else 1\n                )  # At least 1 FN for the field itself\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": 0,\n                        \"fd\": 0,\n                        \"fp\": 0,\n                        \"tn\": 0,\n                        \"fn\": fn_count,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case _:\n                # Both non-null and non-empty, return None to continue processing\n                return None\n\n    def _create_true_negative_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a true negative result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            True negative result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n            \"fields\": {},\n            \"raw_similarity_score\": 1.0,\n            \"similarity_score\": 1.0,\n            \"threshold_applied_score\": 1.0,\n            \"weight\": weight,\n        }\n\n    def _create_false_alarm_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a false alarm result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            False alarm result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 1, \"fd\": 0, \"fp\": 1, \"tn\": 0, \"fn\": 0},\n            \"fields\": {},\n            \"raw_similarity_score\": 0.0,\n            \"similarity_score\": 0.0,\n            \"threshold_applied_score\": 0.0,\n            \"weight\": weight,\n        }\n\n    def _create_false_negative_result(self, weight: float) -&gt; dict:\n        \"\"\"Create a false negative result.\n\n        Args:\n            weight: Field weight for scoring\n\n        Returns:\n            False negative result dictionary\n        \"\"\"\n        return {\n            \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 1},\n            \"fields\": {},\n            \"raw_similarity_score\": 0.0,\n            \"similarity_score\": 0.0,\n            \"threshold_applied_score\": 0.0,\n            \"weight\": weight,\n        }\n\n    def _handle_struct_list_empty_cases(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        weight: float,\n    ) -&gt; dict:\n        \"\"\"Handle empty list cases with beautiful match statements.\n\n        Args:\n            gt_list: Ground truth list (may be None)\n            pred_list: Predicted list (may be None)\n            weight: Field weight for scoring\n\n        Returns:\n            Result dictionary if early exit needed, None if should continue processing\n        \"\"\"\n        # Normalize None to empty lists for consistent handling\n        gt_len = len(gt_list or [])\n        pred_len = len(pred_list or [])\n\n        match (gt_len, pred_len):\n            case (0, 0):\n                # Both empty lists \u2192 True Negative\n                return {\n                    \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                    \"fields\": {},\n                    \"raw_similarity_score\": 1.0,\n                    \"similarity_score\": 1.0,\n                    \"threshold_applied_score\": 1.0,\n                    \"weight\": weight,\n                }\n            case (0, pred_len):\n                # GT empty, pred has items \u2192 False Alarms\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": pred_len,\n                        \"fd\": 0,\n                        \"fp\": pred_len,\n                        \"tn\": 0,\n                        \"fn\": 0,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case (gt_len, 0):\n                # GT has items, pred empty \u2192 False Negatives\n                return {\n                    \"overall\": {\n                        \"tp\": 0,\n                        \"fa\": 0,\n                        \"fd\": 0,\n                        \"fp\": 0,\n                        \"tn\": 0,\n                        \"fn\": gt_len,\n                    },\n                    \"fields\": {},\n                    \"raw_similarity_score\": 0.0,\n                    \"similarity_score\": 0.0,\n                    \"threshold_applied_score\": 0.0,\n                    \"weight\": weight,\n                }\n            case _:\n                # Both non-empty, continue processing\n                return None\n\n    def _calculate_object_level_metrics(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        match_threshold: float,\n    ) -&gt; tuple:\n        \"\"\"Calculate object-level metrics using Hungarian matching.\n\n        Args:\n            gt_list: Ground truth list\n            pred_list: Predicted list\n            match_threshold: Threshold for considering objects as matches\n\n        Returns:\n            Tuple of (object_metrics_dict, matched_pairs, matched_gt_indices, matched_pred_indices)\n        \"\"\"\n        # Use Hungarian matching for OBJECT-LEVEL counts - OPTIMIZED: Single call gets all info\n        hungarian_helper = HungarianHelper()\n        hungarian_info = hungarian_helper.get_complete_matching_info(gt_list, pred_list)\n        matched_pairs = hungarian_info[\"matched_pairs\"]\n\n        # Count OBJECTS, not individual fields\n        tp_objects = 0  # Objects with similarity &gt;= match_threshold\n        fd_objects = 0  # Objects with similarity &lt; match_threshold\n        for gt_idx, pred_idx, similarity in matched_pairs:\n            if similarity &gt;= match_threshold:\n                tp_objects += 1\n            else:\n                fd_objects += 1\n\n        # Count unmatched objects\n        matched_gt_indices = {idx for idx, _, _ in matched_pairs}\n        matched_pred_indices = {idx for _, idx, _ in matched_pairs}\n        fn_objects = len(gt_list) - len(matched_gt_indices)  # Unmatched GT objects\n        fa_objects = len(pred_list) - len(\n            matched_pred_indices\n        )  # Unmatched pred objects\n\n        # Build list-level metrics counting OBJECTS (not fields)\n        object_level_metrics = {\n            \"tp\": tp_objects,\n            \"fa\": fa_objects,\n            \"fd\": fd_objects,\n            \"fp\": fa_objects + fd_objects,  # Total false positives\n            \"tn\": 0,  # No true negatives at object level for non-empty lists\n            \"fn\": fn_objects,\n        }\n\n        return (\n            object_level_metrics,\n            matched_pairs,\n            matched_gt_indices,\n            matched_pred_indices,\n        )\n\n    def _calculate_struct_list_similarity(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        info: \"ComparableField\",\n    ) -&gt; float:\n        \"\"\"Calculate raw similarity score for structured list.\n\n        Args:\n            gt_list: Ground truth list\n            pred_list: Predicted list\n            info: Field comparison info\n\n        Returns:\n            Raw similarity score between 0.0 and 1.0\n        \"\"\"\n        if len(pred_list) &gt; 0:\n            match_result = self._compare_unordered_lists(\n                gt_list, pred_list, info.comparator, info.threshold\n            )\n            return match_result.get(\"overall_score\", 0.0)\n        else:\n            return 0.0\n\n    # Necessary/sufficient field methods removed - no longer used\n\n    def _compare_unordered_lists(\n        self,\n        list1: List[Any],\n        list2: List[Any],\n        comparator: BaseComparator,\n        threshold: float,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compare two lists as unordered collections using Hungarian matching.\n\n        Args:\n            list1: First list\n            list2: Second list\n            comparator: Comparator to use for item comparison\n            threshold: Minimum score to consider a match\n\n        Returns:\n            Dictionary with confusion matrix metrics including:\n            - tp: True positives (matches &gt;= threshold)\n            - fd: False discoveries (matches &lt; threshold)\n            - fa: False alarms (unmatched prediction items)\n            - fn: False negatives (unmatched ground truth items)\n            - fp: Total false positives (fd + fa)\n            - overall_score: Similarity score for backward compatibility\n        \"\"\"\n        return ComparisonHelper.compare_unordered_lists(\n            list1, list2, comparator, threshold\n        )\n\n    def compare_field(self, field_name: str, other_value: Any) -&gt; float:\n        \"\"\"Compare a single field with a value using the configured comparator.\n\n        Args:\n            field_name: Name of the field to compare\n            other_value: Value to compare with\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        # Get our field value\n        my_value = getattr(self, field_name)\n\n        # If both values are StructuredModel instances, use recursive compare_with\n        if isinstance(my_value, StructuredModel) and isinstance(\n            other_value, StructuredModel\n        ):\n            # Use compare_with for rich comparison\n            comparison_result = my_value.compare_with(\n                other_value,\n                include_confusion_matrix=False,\n                document_non_matches=False,\n                evaluator_format=False,\n                recall_with_fd=False,\n            )\n            # Apply field-level threshold if configured\n            info = self._get_comparison_info(field_name)\n            raw_score = comparison_result[\"overall_score\"]\n            return (\n                raw_score\n                if raw_score &gt;= info.threshold or not info.clip_under_threshold\n                else 0.0\n            )\n\n        # CRITICAL FIX: For lists, don't clip under threshold for partial matches\n        if isinstance(my_value, list) and isinstance(other_value, list):\n            # Get field info\n            info = self._get_comparison_info(field_name)\n\n            # Use the raw comparison result without threshold clipping for lists\n            result = ComparisonHelper.compare_unordered_lists(\n                my_value, other_value, info.comparator, info.threshold\n            )\n\n            # Return the overall score directly (don't clip based on threshold for lists)\n            return result[\"overall_score\"]\n\n        # For other fields, use existing logic\n        return ComparisonHelper.compare_field_with_threshold(\n            self, field_name, other_value\n        )\n\n    def compare_field_raw(self, field_name: str, other_value: Any) -&gt; float:\n        \"\"\"Compare a single field with a value WITHOUT applying thresholds.\n\n        This version is used by the compare method to get raw similarity scores.\n\n        Args:\n            field_name: Name of the field to compare\n            other_value: Value to compare with\n\n        Returns:\n            Raw similarity score between 0.0 and 1.0 without threshold filtering\n        \"\"\"\n        # Get our field value\n        my_value = getattr(self, field_name)\n\n        # If both values are StructuredModel instances, use recursive compare_with\n        if isinstance(my_value, StructuredModel) and isinstance(\n            other_value, StructuredModel\n        ):\n            # Use compare_with for rich comparison, but extract the raw score\n            comparison_result = my_value.compare_with(\n                other_value,\n                include_confusion_matrix=False,\n                document_non_matches=False,\n                evaluator_format=False,\n                recall_with_fd=False,\n            )\n            return comparison_result[\"overall_score\"]\n\n        # For non-StructuredModel fields, use existing logic\n        return ComparisonHelper.compare_field_raw(self, field_name, other_value)\n\n    def compare_recursive(self, other: \"StructuredModel\") -&gt; dict:\n        \"\"\"The ONE clean recursive function that handles everything.\n\n        Enhanced to capture BOTH confusion matrix metrics AND similarity scores\n        in a single traversal to eliminate double traversal inefficiency.\n\n        Args:\n            other: Another instance of the same model to compare with\n\n        Returns:\n            Dictionary with clean hierarchical structure:\n            - overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched\n            - fields: Recursive structure for each field with scores\n            - non_matches: List of non-matching items\n        \"\"\"\n        result = {\n            \"overall\": {\n                \"tp\": 0,\n                \"fa\": 0,\n                \"fd\": 0,\n                \"fp\": 0,\n                \"tn\": 0,\n                \"fn\": 0,\n                \"similarity_score\": 0.0,\n                \"all_fields_matched\": False,\n            },\n            \"fields\": {},\n            \"non_matches\": [],\n        }\n\n        # Score percolation variables\n        total_score = 0.0\n        total_weight = 0.0\n        threshold_matched_fields = set()\n\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            gt_val = getattr(self, field_name)\n            pred_val = getattr(other, field_name, None)\n\n            # Enhanced dispatch returns both metrics AND scores\n            field_result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n\n            result[\"fields\"][field_name] = field_result\n\n            # Simple aggregation to overall metrics\n            self._aggregate_to_overall(field_result, result[\"overall\"])\n\n            # Score percolation - aggregate scores upward\n            if \"similarity_score\" in field_result and \"weight\" in field_result:\n                weight = field_result[\"weight\"]\n                threshold_applied_score = field_result[\"threshold_applied_score\"]\n                total_score += threshold_applied_score * weight\n                total_weight += weight\n\n                # Track threshold-matched fields\n                info = self._get_comparison_info(field_name)\n                if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n                    threshold_matched_fields.add(field_name)\n\n        # CRITICAL FIX: Handle hallucinated fields (extra fields) as False Alarms\n        extra_fields_fa = self._count_extra_fields_as_false_alarms(other)\n        result[\"overall\"][\"fa\"] += extra_fields_fa\n        result[\"overall\"][\"fp\"] += extra_fields_fa\n\n        # Calculate overall similarity score from percolated scores\n        if total_weight &gt; 0:\n            result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n\n        # Determine all_fields_matched\n        model_fields_for_comparison = set(self.__class__.model_fields.keys()) - {\n            \"extra_fields\"\n        }\n        result[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(\n            model_fields_for_comparison\n        )\n\n        return result\n\n    def _dispatch_field_comparison(\n        self, field_name: str, gt_val: Any, pred_val: Any\n    ) -&gt; dict:\n        \"\"\"Enhanced case-based dispatch using match statements for clean logic flow.\"\"\"\n\n        # Get field configuration for scoring\n        info = self._get_comparison_info(field_name)\n        weight = info.weight\n        threshold = info.threshold\n\n        # Check if this field is ANY list type (including Optional[List[str]], Optional[List[StructuredModel]], etc.)\n        is_list_field = self._is_list_field(field_name)\n\n        # Get null states and hierarchical needs\n        gt_is_null = self._is_truly_null(gt_val)\n        pred_is_null = self._is_truly_null(pred_val)\n        gt_needs_hierarchy = self._should_use_hierarchical_structure(gt_val, field_name)\n        pred_needs_hierarchy = self._should_use_hierarchical_structure(\n            pred_val, field_name\n        )\n\n        # Handle list fields with match statements\n        if is_list_field:\n            list_result = self._handle_list_field_dispatch(gt_val, pred_val, weight)\n            if list_result is not None:\n                return list_result\n            # If None returned, continue to regular type-based dispatch\n\n        # Handle non-hierarchical primitive null cases with match statements\n        if not (gt_needs_hierarchy or pred_needs_hierarchy):\n            gt_effectively_null_prim = self._is_effectively_null_for_primitives(gt_val)\n            pred_effectively_null_prim = self._is_effectively_null_for_primitives(\n                pred_val\n            )\n\n            match (gt_effectively_null_prim, pred_effectively_null_prim):\n                case (True, True):\n                    return self._create_true_negative_result(weight)\n                case (True, False):\n                    return self._create_false_alarm_result(weight)\n                case (False, True):\n                    return self._create_false_negative_result(weight)\n                case _:\n                    # Both non-null, continue to type-based dispatch\n                    pass\n\n        # Type-based dispatch\n        if isinstance(gt_val, (str, int, float)) and isinstance(\n            pred_val, (str, int, float)\n        ):\n            return self._compare_primitive_with_scores(gt_val, pred_val, field_name)\n        elif isinstance(gt_val, list) and isinstance(pred_val, list):\n            # Check if this should be structured list\n            if gt_val and isinstance(gt_val[0], StructuredModel):\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(gt_val, list) and len(gt_val) == 0:\n            # Handle empty GT list - check if it should be structured\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                # Empty structured list - should still return hierarchical structure\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(pred_val, list) and len(pred_val) == 0:\n            # Handle empty pred list - check if it should be structured\n            field_info = self.__class__.model_fields.get(field_name)\n            if field_info and self._is_structured_field_type(field_info):\n                # Empty structured list - should still return hierarchical structure\n                return self._compare_struct_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n            else:\n                return self._compare_primitive_list_with_scores(\n                    gt_val, pred_val, field_name\n                )\n        elif isinstance(gt_val, StructuredModel) and isinstance(\n            pred_val, StructuredModel\n        ):\n            # CRITICAL FIX: For StructuredModel fields, object-level metrics should be based on\n            # object similarity, not rollup of nested field metrics\n\n            # Get object-level similarity score\n            raw_score = gt_val.compare(pred_val)  # Overall object similarity\n\n            # Apply object-level binary classification based on threshold\n            if raw_score &gt;= threshold:\n                # Object matches threshold -&gt; True Positive\n                object_metrics = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n                threshold_applied_score = raw_score\n            else:\n                # Object below threshold -&gt; False Discovery\n                object_metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n                threshold_applied_score = (\n                    0.0 if info.clip_under_threshold else raw_score\n                )\n\n            # Still generate nested field details for debugging, but don't roll them up\n            nested_details = gt_val.compare_recursive(pred_val)[\"fields\"]\n\n            # Return structure with object-level metrics and nested field details kept separate\n            return {\n                \"overall\": {\n                    **object_metrics,\n                    \"similarity_score\": raw_score,\n                    \"all_fields_matched\": raw_score &gt;= threshold,\n                },\n                \"fields\": nested_details,  # Nested details available for debugging\n                \"raw_similarity_score\": raw_score,\n                \"similarity_score\": raw_score,\n                \"threshold_applied_score\": threshold_applied_score,\n                \"weight\": weight,\n                \"non_matches\": [],  # Add empty non_matches for consistency\n            }\n        else:\n            # Mismatched types\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0},\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n\n    def _compare_primitive_with_scores(\n        self, gt_val: Any, pred_val: Any, field_name: str\n    ) -&gt; dict:\n        \"\"\"Enhanced primitive comparison that returns both metrics AND scores.\"\"\"\n        info = self.__class__._get_comparison_info(field_name)\n        raw_similarity = info.comparator.compare(gt_val, pred_val)\n        weight = info.weight\n        threshold = info.threshold\n\n        # For binary classification metrics, always use threshold\n        if raw_similarity &gt;= threshold:\n            metrics = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n            threshold_applied_score = raw_similarity\n        else:\n            metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n            # For score calculation, respect clip_under_threshold setting\n            threshold_applied_score = (\n                0.0 if info.clip_under_threshold else raw_similarity\n            )\n\n        # UNIFIED STRUCTURE: Always use 'overall' for metrics\n        # 'fields' key omitted for primitive leaf nodes (semantic meaning: not a parent container)\n        return {\n            \"overall\": metrics,\n            \"raw_similarity_score\": raw_similarity,\n            \"similarity_score\": raw_similarity,\n            \"threshold_applied_score\": threshold_applied_score,\n            \"weight\": weight,\n        }\n\n    def _compare_primitive_list_with_scores(\n        self, gt_list: List[Any], pred_list: List[Any], field_name: str\n    ) -&gt; dict:\n        \"\"\"Enhanced primitive list comparison that returns both metrics AND scores with hierarchical structure.\n\n        DESIGN DECISION: Universal Hierarchical Structure\n        ===============================================\n        This method returns a hierarchical structure {\"overall\": {...}, \"fields\": {...}} even for\n        primitive lists (List[str], List[int], etc.) to maintain API consistency across all field types.\n\n        Why this approach:\n        - CONSISTENCY: All list fields use the same access pattern: cm[\"fields\"][name][\"overall\"]\n        - TEST COMPATIBILITY: Multiple test files expect this pattern for both primitive and structured lists\n        - PREDICTABLE API: Consumers don't need to check field type before accessing metrics\n\n        Trade-offs:\n        - Creates vestigial \"fields\": {} objects for primitive lists that will never be populated\n        - Slightly more verbose structure than necessary for leaf nodes\n        - Architecturally less pure than type-based structure (primitives flat, structured hierarchical)\n\n        Alternative considered but rejected:\n        - Type-based structure where List[primitive] \u2192 flat, List[StructuredModel] \u2192 hierarchical\n        - Would require updating multiple test files and consumer code to handle mixed access patterns\n        - More architecturally pure but breaks backward compatibility\n\n        Future consideration: If we ever refactor the entire confusion matrix API, we could move to\n        type-based structure where the presence of \"fields\" key indicates structured vs primitive.\n        \"\"\"\n        # Get field configuration\n        info = self.__class__._get_comparison_info(field_name)\n        weight = info.weight\n        threshold = info.threshold\n\n        # CRITICAL FIX: Handle None values before checking length\n        # Convert None to empty list for consistent handling\n        if gt_list is None:\n            gt_list = []\n        if pred_list is None:\n            pred_list = []\n\n        # Handle empty/null list cases first - FIXED: Empty lists should be TN=1\n        if len(gt_list) == 0 and len(pred_list) == 0:\n            # Both empty lists should be TN=1\n            return {\n                \"overall\": {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0},\n                \"fields\": {},  # Empty for primitive lists\n                \"raw_similarity_score\": 1.0,  # Perfect match\n                \"similarity_score\": 1.0,\n                \"threshold_applied_score\": 1.0,\n                \"weight\": weight,\n            }\n        elif len(gt_list) == 0:\n            # GT empty, pred has items \u2192 False Alarms\n            return {\n                \"overall\": {\n                    \"tp\": 0,\n                    \"fa\": len(pred_list),\n                    \"fd\": 0,\n                    \"fp\": len(pred_list),\n                    \"tn\": 0,\n                    \"fn\": 0,\n                },\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n        elif len(pred_list) == 0:\n            # GT has items, pred empty \u2192 False Negatives\n            return {\n                \"overall\": {\n                    \"tp\": 0,\n                    \"fa\": 0,\n                    \"fd\": 0,\n                    \"fp\": 0,\n                    \"tn\": 0,\n                    \"fn\": len(gt_list),\n                },\n                \"fields\": {},\n                \"raw_similarity_score\": 0.0,\n                \"similarity_score\": 0.0,\n                \"threshold_applied_score\": 0.0,\n                \"weight\": weight,\n            }\n\n        # For primitive lists, use the comparison logic from _compare_unordered_lists\n        # which properly handles the threshold-based matching\n        comparator = info.comparator\n        match_result = self._compare_unordered_lists(\n            gt_list, pred_list, comparator, threshold\n        )\n\n        # Extract the counts from the match result\n        tp = match_result.get(\"tp\", 0)\n        fd = match_result.get(\"fd\", 0)\n        fa = match_result.get(\"fa\", 0)\n        fn = match_result.get(\"fn\", 0)\n\n        # Use the overall_score from the match result for raw similarity\n        raw_similarity = match_result.get(\"overall_score\", 0.0)\n\n        # CRITICAL FIX: For lists, we NEVER clip under threshold - partial matches are important\n        threshold_applied_score = raw_similarity  # Always use raw score for lists\n\n        # Return hierarchical structure expected by tests\n        return {\n            \"overall\": {\"tp\": tp, \"fa\": fa, \"fd\": fd, \"fp\": fa + fd, \"tn\": 0, \"fn\": fn},\n            \"fields\": {},  # Empty for primitive lists - no nested structure\n            \"raw_similarity_score\": raw_similarity,\n            \"similarity_score\": raw_similarity,\n            \"threshold_applied_score\": threshold_applied_score,\n            \"weight\": weight,\n        }\n\n    def _compare_struct_list_with_scores(\n        self,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        field_name: str,\n    ) -&gt; dict:\n        \"\"\"Enhanced structural list comparison that returns both metrics AND scores.\n\n        PHASE 2: Delegates to StructuredListComparator while maintaining identical behavior.\n        \"\"\"\n        # Import here to avoid circular imports\n        from .structured_list_comparator import StructuredListComparator\n\n        # Create comparator and delegate\n        comparator = StructuredListComparator(self)\n        return comparator.compare_struct_list_with_scores(\n            gt_list, pred_list, field_name\n        )\n\n    def _count_extra_fields_as_false_alarms(self, other: \"StructuredModel\") -&gt; int:\n        \"\"\"Count hallucinated fields (extra fields) in the prediction as False Alarms.\n\n        Args:\n            other: The predicted StructuredModel instance to check for extra fields\n\n        Returns:\n            Number of hallucinated fields that should count as False Alarms\n        \"\"\"\n        fa_count = 0\n\n        # Check if the other model has extra fields (hallucinated content)\n        if hasattr(other, \"__pydantic_extra__\"):\n            # Count each extra field as one False Alarm\n            fa_count += len(other.__pydantic_extra__)\n\n        # Also recursively check nested StructuredModel objects for extra fields\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            gt_val = getattr(self, field_name, None)\n            pred_val = getattr(other, field_name, None)\n\n            # Check nested StructuredModel objects\n            if isinstance(gt_val, StructuredModel) and isinstance(\n                pred_val, StructuredModel\n            ):\n                fa_count += gt_val._count_extra_fields_as_false_alarms(pred_val)\n\n            # Check lists of StructuredModel objects\n            elif (\n                isinstance(gt_val, list)\n                and isinstance(pred_val, list)\n                and gt_val\n                and isinstance(gt_val[0], StructuredModel)\n                and pred_val\n                and isinstance(pred_val[0], StructuredModel)\n            ):\n                # For lists, we need to match them up properly using Hungarian matching - OPTIMIZED: Single call gets all info\n                # to avoid double-counting in cases where the list comparison already\n                # handles unmatched items as FA. For now, let's recursively check each item.\n                hungarian_helper = HungarianHelper()\n                hungarian_info = hungarian_helper.get_complete_matching_info(\n                    gt_val, pred_val\n                )\n                matched_pairs = hungarian_info[\"matched_pairs\"]\n\n                # Count extra fields in matched pairs\n                for gt_idx, pred_idx, similarity in matched_pairs:\n                    if gt_idx &lt; len(gt_val) and pred_idx &lt; len(pred_val):\n                        gt_item = gt_val[gt_idx]\n                        pred_item = pred_val[pred_idx]\n                        fa_count += gt_item._count_extra_fields_as_false_alarms(\n                            pred_item\n                        )\n\n                # For unmatched prediction items, count their extra fields too\n                matched_pred_indices = {pred_idx for _, pred_idx, _ in matched_pairs}\n                for pred_idx, pred_item in enumerate(pred_val):\n                    if pred_idx not in matched_pred_indices and isinstance(\n                        pred_item, StructuredModel\n                    ):\n                        # For unmatched items, we need a dummy GT to compare against\n                        if gt_val:  # Use first GT item as template\n                            dummy_gt = gt_val[0]\n                            fa_count += dummy_gt._count_extra_fields_as_false_alarms(\n                                pred_item\n                            )\n                        else:\n                            # If no GT items, count all extra fields in this pred item\n                            if hasattr(pred_item, \"__pydantic_extra__\"):\n                                fa_count += len(pred_item.__pydantic_extra__)\n\n        return fa_count\n\n    def _aggregate_to_overall(self, field_result: dict, overall: dict) -&gt; None:\n        \"\"\"Simple aggregation to overall metrics.\"\"\"\n        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n            if isinstance(field_result, dict):\n                if metric in field_result:\n                    overall[metric] += field_result[metric]\n                elif \"overall\" in field_result and metric in field_result[\"overall\"]:\n                    overall[metric] += field_result[\"overall\"][metric]\n\n    def _calculate_aggregate_metrics(self, result: dict) -&gt; dict:\n        \"\"\"Calculate aggregate metrics for all nodes in the result tree.\n\n        CRITICAL FIX: Enhanced deep nesting traversal to handle arbitrary nesting depth.\n        The aggregate field contains the sum of all primitive field confusion matrices\n        below that node in the tree. This provides universal field-level granularity.\n\n        Args:\n            result: Result from compare_recursive with hierarchical structure\n\n        Returns:\n            Modified result with 'aggregate' fields added at each level\n        \"\"\"\n        if not isinstance(result, dict):\n            return result\n\n        # Make a copy to avoid modifying the original\n        result_copy = result.copy()\n\n        # Calculate aggregate for this node\n        aggregate_metrics = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n\n        # Recursively process 'fields' first to get child aggregates\n        if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n            fields_copy = {}\n            for field_name, field_result in result_copy[\"fields\"].items():\n                if isinstance(field_result, dict):\n                    # Recursively calculate aggregate for child field\n                    processed_field = self._calculate_aggregate_metrics(field_result)\n                    fields_copy[field_name] = processed_field\n\n                    # CRITICAL FIX: Sum child's aggregate metrics to parent\n                    if \"aggregate\" in processed_field and self._has_basic_metrics(\n                        processed_field[\"aggregate\"]\n                    ):\n                        child_aggregate = processed_field[\"aggregate\"]\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            aggregate_metrics[metric] += child_aggregate.get(metric, 0)\n                else:\n                    # Non-dict field - keep as is\n                    fields_copy[field_name] = field_result\n            result_copy[\"fields\"] = fields_copy\n\n        # CRITICAL FIX: Enhanced leaf node detection for deep nesting\n        # Handle both empty fields dict and missing fields key as leaf indicators\n        is_leaf_node = (\n            \"fields\" not in result_copy\n            or not result_copy[\"fields\"]\n            or (\n                isinstance(result_copy[\"fields\"], dict)\n                and len(result_copy[\"fields\"]) == 0\n            )\n        )\n\n        if is_leaf_node:\n            # Check if this is a leaf node with basic metrics (either in \"overall\" or directly)\n            if \"overall\" in result_copy and self._has_basic_metrics(\n                result_copy[\"overall\"]\n            ):\n                # Hierarchical leaf node: aggregate = overall metrics\n                overall = result_copy[\"overall\"]\n                for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                    aggregate_metrics[metric] = overall.get(metric, 0)\n            elif self._has_basic_metrics(result_copy):\n                # CRITICAL FIX: Legacy primitive leaf node - wrap in \"overall\" structure\n                # This preserves Universal Aggregate Field structure compliance\n                legacy_metrics = {}\n                for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                    legacy_metrics[metric] = result_copy.get(metric, 0)\n                    aggregate_metrics[metric] = result_copy.get(metric, 0)\n\n                # Wrap legacy structure in \"overall\" key to maintain consistency\n                if not \"overall\" in result_copy:\n                    # Move all basic metrics to \"overall\" key\n                    result_copy[\"overall\"] = legacy_metrics\n                    # Remove basic metrics from top level to avoid duplication\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        if metric in result_copy:\n                            del result_copy[metric]\n                    # Preserve other keys like derived, raw_similarity_score, etc.\n\n        # CRITICAL FIX: Always sum child field metrics if no child aggregates were found\n        # This handles the deep nesting case where leaf nodes have overall metrics but empty fields\n        if (\n            aggregate_metrics[\"tp\"] == 0\n            and aggregate_metrics[\"fa\"] == 0\n            and aggregate_metrics[\"fd\"] == 0\n            and aggregate_metrics[\"fp\"] == 0\n            and aggregate_metrics[\"tn\"] == 0\n            and aggregate_metrics[\"fn\"] == 0\n        ):\n            # Check if we have fields with overall metrics that we can sum\n            if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n                for field_name, field_result in result_copy[\"fields\"].items():\n                    if isinstance(field_result, dict):\n                        # ENHANCED: Check for both direct metrics and overall metrics\n                        if \"overall\" in field_result and self._has_basic_metrics(\n                            field_result[\"overall\"]\n                        ):\n                            field_overall = field_result[\"overall\"]\n                            for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                                aggregate_metrics[metric] += field_overall.get(\n                                    metric, 0\n                                )\n                        elif self._has_basic_metrics(field_result):\n                            # Direct metrics (legacy format)\n                            for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                                aggregate_metrics[metric] += field_result.get(metric, 0)\n\n        # Add aggregate as a sibling of 'overall' and 'fields'\n        result_copy[\"aggregate\"] = aggregate_metrics\n\n        return result_copy\n\n    def _add_derived_metrics_to_result(self, result: dict) -&gt; dict:\n        \"\"\"Walk through result and add 'derived' fields with F1, precision, recall, accuracy.\n\n        Args:\n            result: Result from compare_recursive with basic TP, FP, FN, etc. metrics\n\n        Returns:\n            Modified result with 'derived' fields added at each level\n        \"\"\"\n        if not isinstance(result, dict):\n            return result\n\n        # Make a copy to avoid modifying the original\n        result_copy = result.copy()\n\n        # Add derived metrics to 'overall' if it exists and has basic metrics\n        if \"overall\" in result_copy and isinstance(result_copy[\"overall\"], dict):\n            overall = result_copy[\"overall\"]\n            if self._has_basic_metrics(overall):\n                metrics_helper = MetricsHelper()\n                overall[\"derived\"] = metrics_helper.calculate_derived_metrics(overall)\n\n                # Also add derived metrics to aggregate if it exists\n                if \"aggregate\" in overall and self._has_basic_metrics(\n                    overall[\"aggregate\"]\n                ):\n                    overall[\"aggregate\"][\"derived\"] = (\n                        metrics_helper.calculate_derived_metrics(overall[\"aggregate\"])\n                    )\n\n        # Add derived metrics to top-level aggregate if it exists\n        if \"aggregate\" in result_copy and self._has_basic_metrics(\n            result_copy[\"aggregate\"]\n        ):\n            metrics_helper = MetricsHelper()\n            result_copy[\"aggregate\"][\"derived\"] = (\n                metrics_helper.calculate_derived_metrics(result_copy[\"aggregate\"])\n            )\n\n        # Recursively process 'fields' if it exists\n        if \"fields\" in result_copy and isinstance(result_copy[\"fields\"], dict):\n            fields_copy = {}\n            for field_name, field_result in result_copy[\"fields\"].items():\n                if isinstance(field_result, dict):\n                    # Check if this is a hierarchical field (has overall/fields) or a unified structure field\n                    if \"overall\" in field_result and \"fields\" in field_result:\n                        # Hierarchical field - process recursively\n                        fields_copy[field_name] = self._add_derived_metrics_to_result(\n                            field_result\n                        )\n                    elif \"overall\" in field_result and self._has_basic_metrics(\n                        field_result[\"overall\"]\n                    ):\n                        # Unified structure field - add derived metrics to overall\n                        field_copy = field_result.copy()\n                        metrics_helper = MetricsHelper()\n                        field_copy[\"overall\"][\"derived\"] = (\n                            metrics_helper.calculate_derived_metrics(\n                                field_result[\"overall\"]\n                            )\n                        )\n\n                        # Also add derived metrics to aggregate if it exists\n                        if \"aggregate\" in field_copy and self._has_basic_metrics(\n                            field_copy[\"aggregate\"]\n                        ):\n                            field_copy[\"aggregate\"][\"derived\"] = (\n                                metrics_helper.calculate_derived_metrics(\n                                    field_copy[\"aggregate\"]\n                                )\n                            )\n\n                        fields_copy[field_name] = field_copy\n                    elif self._has_basic_metrics(field_result):\n                        # CRITICAL FIX: Legacy leaf field with basic metrics - wrap in \"overall\" structure\n                        field_copy = field_result.copy()\n                        metrics_helper = MetricsHelper()\n\n                        # Extract basic metrics and wrap in \"overall\" structure\n                        legacy_metrics = {}\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            if metric in field_copy:\n                                legacy_metrics[metric] = field_copy[metric]\n                                del field_copy[metric]  # Remove from top level\n\n                        # Add derived metrics to the legacy metrics\n                        legacy_metrics[\"derived\"] = (\n                            metrics_helper.calculate_derived_metrics(legacy_metrics)\n                        )\n\n                        # Wrap in \"overall\" structure\n                        field_copy[\"overall\"] = legacy_metrics\n\n                        fields_copy[field_name] = field_copy\n                    else:\n                        # Other structure - keep as is\n                        fields_copy[field_name] = field_result\n                else:\n                    # Non-dict field - keep as is\n                    fields_copy[field_name] = field_result\n            result_copy[\"fields\"] = fields_copy\n\n        return result_copy\n\n    def _has_basic_metrics(self, metrics_dict: dict) -&gt; bool:\n        \"\"\"Check if a dictionary has basic confusion matrix metrics.\n\n        Args:\n            metrics_dict: Dictionary to check\n\n        Returns:\n            True if it has the basic metrics (tp, fp, fn, etc.)\n        \"\"\"\n        basic_metrics = [\"tp\", \"fp\", \"fn\", \"tn\", \"fa\", \"fd\"]\n        return all(metric in metrics_dict for metric in basic_metrics)\n\n    def _classify_field_for_confusion_matrix(\n        self, field_name: str, other_value: Any, threshold: float = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Classify a field comparison according to the confusion matrix rules.\n\n        Args:\n            field_name: Name of the field being compared\n            other_value: Value to compare with\n            threshold: Threshold for matching (uses field's threshold if None)\n\n        Returns:\n            Dictionary with TP, FP, TN, FN, FD counts and derived metrics\n        \"\"\"\n        # Get field values\n        gt_value = getattr(self, field_name)\n        pred_value = other_value\n\n        # Get field configuration\n        info = self.__class__._get_comparison_info(field_name)\n        if threshold is None:\n            threshold = info.threshold\n        comparator = info.comparator\n\n        # Determine if values are null\n        gt_is_null = FieldHelper.is_null_value(gt_value)\n        pred_is_null = FieldHelper.is_null_value(pred_value)\n\n        # Calculate similarity if both aren't null\n        similarity = None\n        if not gt_is_null and not pred_is_null:\n            if isinstance(gt_value, StructuredModel) and isinstance(\n                pred_value, StructuredModel\n            ):\n                comparison = gt_value.compare_with(pred_value)\n                similarity = comparison[\"overall_score\"]\n            else:\n                # Use the field's configured comparator for primitive comparison\n                similarity = comparator.compare(gt_value, pred_value)\n            values_match = similarity &gt;= threshold\n        else:\n            values_match = False\n\n        # Apply confusion matrix classification\n        if gt_is_null and pred_is_null:\n            # TN: Both null\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0}\n        elif gt_is_null and not pred_is_null:\n            # FA: GT null, prediction non-null (False Alarm)\n            result = {\"tp\": 0, \"fa\": 1, \"fd\": 0, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n        elif not gt_is_null and pred_is_null:\n            # FN: GT non-null, prediction null\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 1}\n        elif values_match:\n            # TP: Both non-null and match\n            result = {\"tp\": 1, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n        else:\n            # FD: Both non-null but don't match (False Discovery)\n            result = {\"tp\": 0, \"fa\": 0, \"fd\": 1, \"fp\": 1, \"tn\": 0, \"fn\": 0}\n\n        # Add derived metrics\n        metrics_helper = MetricsHelper()\n        result[\"derived\"] = metrics_helper.calculate_derived_metrics(result)\n        # Don't include similarity_score in the result as tests don't expect it\n\n        return result\n\n    def _calculate_list_confusion_matrix(\n        self, field_name: str, other_list: List[Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Calculate confusion matrix for a list field, including nested field metrics.\n\n        Args:\n            field_name: Name of the list field being compared\n            other_list: Predicted list to compare with\n\n        Returns:\n            Dictionary with:\n            - Top-level TP, FP, TN, FN, FD, FA counts and derived metrics for the list field\n            - nested_fields: Dict with metrics for individual fields within list items (e.g., \"transactions.date\")\n            - non_matches: List of individual object-level non-matches for detailed analysis\n        \"\"\"\n        gt_list = getattr(self, field_name)\n        pred_list = other_list\n\n        # Initialize result structure\n        result = {\n            \"tp\": 0,\n            \"fa\": 0,\n            \"fd\": 0,\n            \"fp\": 0,\n            \"tn\": 0,\n            \"fn\": 0,\n            \"nested_fields\": {},  # Store nested field metrics here\n            \"non_matches\": [],  # Store individual object-level non-matches here\n        }\n\n        # Handle null cases first\n        if FieldHelper.is_null_value(gt_list) and FieldHelper.is_null_value(pred_list):\n            result.update({\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0})\n        elif FieldHelper.is_null_value(gt_list):\n            result.update(\n                {\n                    \"tp\": 0,\n                    \"fa\": len(pred_list),\n                    \"fd\": 0,\n                    \"fp\": len(pred_list),\n                    \"tn\": 0,\n                    \"fn\": 0,\n                }\n            )\n            # Add non-matches for each FA item using NonMatchesHelper\n            non_matches_helper = NonMatchesHelper()\n            result[\"non_matches\"] = non_matches_helper.add_non_matches_for_null_cases(\n                field_name, gt_list, pred_list\n            )\n        elif FieldHelper.is_null_value(pred_list):\n            result.update(\n                {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": len(gt_list)}\n            )\n            # Add non-matches for each FN item using NonMatchesHelper\n            non_matches_helper = NonMatchesHelper()\n            result[\"non_matches\"] = non_matches_helper.add_non_matches_for_null_cases(\n                field_name, gt_list, pred_list\n            )\n        else:\n            # Use existing comparison logic for list-level metrics\n            info = self.__class__._get_comparison_info(field_name)\n            comparator = info.comparator\n            threshold = info.threshold\n\n            # Reuse existing Hungarian matching logic\n            match_result = self._compare_unordered_lists(\n                gt_list, pred_list, comparator, threshold\n            )\n\n            # Use the detailed confusion matrix results directly from Hungarian matcher\n            result.update(\n                {\n                    \"tp\": match_result[\"tp\"],\n                    \"fa\": match_result[\n                        \"fa\"\n                    ],  # False alarms (unmatched prediction items)\n                    \"fd\": match_result[\n                        \"fd\"\n                    ],  # False discoveries (matches below threshold)\n                    \"fp\": match_result[\"fp\"],  # Total false positives (fa + fd)\n                    \"tn\": 0,\n                    \"fn\": match_result[\"fn\"],  # False negatives (unmatched GT items)\n                }\n            )\n\n            # Collect individual object-level non-matches using NonMatchesHelper\n            if gt_list and isinstance(gt_list[0], StructuredModel):\n                non_matches_helper = NonMatchesHelper()\n                non_matches = non_matches_helper.collect_list_non_matches(\n                    field_name, gt_list, pred_list\n                )\n                result[\"non_matches\"] = non_matches\n\n            # If list contains StructuredModel objects, calculate nested field metrics\n            if gt_list and isinstance(gt_list[0], StructuredModel):\n                nested_metrics = self._calculate_nested_field_metrics(\n                    field_name, gt_list, pred_list, threshold\n                )\n                result[\"nested_fields\"] = nested_metrics\n\n        # For List[StructuredModel], we should NOT aggregate nested fields to list level\n        # List level metrics represent object-level matches from Hungarian algorithm\n        # Nested field metrics represent field-level matches within those objects\n        # They are separate concerns and should not be aggregated\n\n        # Only aggregate if this is explicitly marked as an aggregate field AND it's not a list\n        is_aggregate = self.__class__._is_aggregate_field(field_name)\n        if is_aggregate and not isinstance(gt_list, list):\n            # Initialize top-level confusion matrix values to 0\n            result[\"tp\"] = 0\n            result[\"fa\"] = 0\n            result[\"fd\"] = 0\n            result[\"fp\"] = 0\n            result[\"tn\"] = 0\n            result[\"fn\"] = 0\n            # Sum up the confusion matrix values from nested fields\n            for field, field_metrics in result[\"nested_fields\"].items():\n                result[\"tp\"] += field_metrics[\"tp\"]\n                result[\"fa\"] += field_metrics[\"fa\"]\n                result[\"fd\"] += field_metrics[\"fd\"]\n                result[\"fp\"] += field_metrics[\"fp\"]\n                result[\"tn\"] += field_metrics[\"tn\"]\n                result[\"fn\"] += field_metrics[\"fn\"]\n\n        # Add derived metrics\n        metrics_helper = MetricsHelper()\n        result[\"derived\"] = metrics_helper.calculate_derived_metrics(result)\n\n        return result\n\n    def _calculate_nested_field_metrics(\n        self,\n        list_field_name: str,\n        gt_list: List[\"StructuredModel\"],\n        pred_list: List[\"StructuredModel\"],\n        threshold: float,\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Calculate confusion matrix metrics for individual fields within list items.\n\n        THRESHOLD-GATED RECURSION: Only perform recursive field analysis for object pairs\n        with similarity &gt;= StructuredModel.match_threshold. Poor matches and unmatched\n        items are treated as atomic units.\n\n        Args:\n            list_field_name: Name of the parent list field (e.g., \"transactions\")\n            gt_list: Ground truth list of StructuredModel objects\n            pred_list: Predicted list of StructuredModel objects\n            threshold: Matching threshold (not used for threshold-gating)\n\n        Returns:\n            Dictionary mapping nested field paths to their confusion matrix metrics\n            E.g., {\"transactions.date\": {...}, \"transactions.description\": {...}}\n        \"\"\"\n        nested_metrics = {}\n\n        if not gt_list or not isinstance(gt_list[0], StructuredModel):\n            return nested_metrics\n\n        # Get the model class from the first item\n        model_class = gt_list[0].__class__\n\n        # CRITICAL FIX: Use field's threshold, not class's match_threshold\n        # Get the field info from the parent object to use the correct threshold\n        parent_field_info = self.__class__._get_comparison_info(list_field_name)\n        match_threshold = parent_field_info.threshold\n\n        # For each field in the nested model\n        for field_name in model_class.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            nested_field_path = f\"{list_field_name}.{field_name}\"\n\n            # Initialize aggregated counts for this nested field\n            total_tp = total_fa = total_fd = total_fp = total_tn = total_fn = 0\n\n            # Use HungarianHelper for Hungarian matching operations - OPTIMIZED: Single call gets all info\n            hungarian_helper = HungarianHelper()\n\n            # Use HungarianHelper to get optimal assignments with similarity scores\n            assignments = []\n            matched_pairs_with_scores = []\n            if gt_list and pred_list:\n                hungarian_info = hungarian_helper.get_complete_matching_info(\n                    gt_list, pred_list\n                )\n                matched_pairs_with_scores = hungarian_info[\"matched_pairs\"]\n                # Extract (gt_idx, pred_idx) pairs from the matched_pairs\n                assignments = [(i, j) for i, j, score in matched_pairs_with_scores]\n\n            # THRESHOLD-GATED RECURSION: Only process pairs that meet the match_threshold\n            for gt_idx, pred_idx, similarity_score in matched_pairs_with_scores:\n                if gt_idx &lt; len(gt_list) and pred_idx &lt; len(pred_list):\n                    gt_item = gt_list[gt_idx]\n                    pred_item = pred_list[pred_idx]\n\n                    # Handle floating point precision issues\n                    is_above_threshold = (\n                        similarity_score &gt;= match_threshold\n                        or abs(similarity_score - match_threshold) &lt; 1e-10\n                    )\n\n                    # Only perform recursive field analysis if similarity meets threshold\n                    if is_above_threshold:\n                        # Get field values\n                        gt_value = getattr(gt_item, field_name, None)\n                        pred_value = getattr(pred_item, field_name, None)\n\n                        # Check if this field is a List[StructuredModel] that needs recursive processing\n                        if (\n                            isinstance(gt_value, list)\n                            and isinstance(pred_value, list)\n                            and gt_value\n                            and isinstance(gt_value[0], StructuredModel)\n                        ):\n                            # Handle List[StructuredModel] recursively\n                            list_classification = (\n                                gt_item._calculate_list_confusion_matrix(\n                                    field_name, pred_value\n                                )\n                            )\n\n                            # Aggregate the list-level counts\n                            total_tp += list_classification[\"tp\"]\n                            total_fa += list_classification[\"fa\"]\n                            total_fd += list_classification[\"fd\"]\n                            total_fp += list_classification[\"fp\"]\n                            total_tn += list_classification[\"tn\"]\n                            total_fn += list_classification[\"fn\"]\n\n                            # IMPORTANT: Also collect the deeper nested field metrics\n                            if \"nested_fields\" in list_classification:\n                                for (\n                                    deeper_field_path,\n                                    deeper_metrics,\n                                ) in list_classification[\"nested_fields\"].items():\n                                    # Create the full path: e.g., \"products.attributes.name\"\n                                    full_deeper_path = (\n                                        f\"{list_field_name}.{deeper_field_path}\"\n                                    )\n\n                                    # Initialize or aggregate into the deeper nested metrics\n                                    if full_deeper_path not in nested_metrics:\n                                        nested_metrics[full_deeper_path] = {\n                                            \"tp\": 0,\n                                            \"fa\": 0,\n                                            \"fd\": 0,\n                                            \"fp\": 0,\n                                            \"tn\": 0,\n                                            \"fn\": 0,\n                                        }\n\n                                    nested_metrics[full_deeper_path][\"tp\"] += (\n                                        deeper_metrics[\"tp\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fa\"] += (\n                                        deeper_metrics[\"fa\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fd\"] += (\n                                        deeper_metrics[\"fd\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fp\"] += (\n                                        deeper_metrics[\"fp\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"tn\"] += (\n                                        deeper_metrics[\"tn\"]\n                                    )\n                                    nested_metrics[full_deeper_path][\"fn\"] += (\n                                        deeper_metrics[\"fn\"]\n                                    )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            field_classification = (\n                                gt_item._classify_field_for_confusion_matrix(\n                                    field_name,\n                                    pred_value,\n                                    None,  # Use field's own threshold\n                                )\n                            )\n\n                            # Aggregate counts\n                            total_tp += field_classification[\"tp\"]\n                            total_fa += field_classification[\"fa\"]\n                            total_fd += field_classification[\"fd\"]\n                            total_fp += field_classification[\"fp\"]\n                            total_tn += field_classification[\"tn\"]\n                            total_fn += field_classification[\"fn\"]\n                    else:\n                        # Skip recursive analysis for pairs below threshold\n                        # These will be handled as FD at the object level\n                        pass\n\n            # Handle unmatched ground truth items (false negatives)\n            matched_gt_indices = set(idx for idx, _ in assignments)\n            for gt_idx, gt_item in enumerate(gt_list):\n                if gt_idx not in matched_gt_indices:\n                    gt_value = getattr(gt_item, field_name, None)\n                    if not FieldHelper.is_null_value(gt_value):\n                        # Check if this is a List[StructuredModel] that needs deeper processing for FN\n                        if (\n                            isinstance(gt_value, list)\n                            and gt_value\n                            and isinstance(gt_value[0], StructuredModel)\n                        ):\n                            # For List[StructuredModel], count each item in the list as a separate FN\n                            # and handle deeper nested fields\n                            total_fn += len(gt_value)  # Each list item is a separate FN\n\n                            # Also handle deeper nested fields for unmatched items\n                            dummy_empty_list = []  # Empty list for comparison\n                            list_classification = (\n                                gt_item._calculate_list_confusion_matrix(\n                                    field_name, dummy_empty_list\n                                )\n                            )\n                            if \"nested_fields\" in list_classification:\n                                for (\n                                    deeper_field_path,\n                                    deeper_metrics,\n                                ) in list_classification[\"nested_fields\"].items():\n                                    full_deeper_path = (\n                                        f\"{list_field_name}.{deeper_field_path}\"\n                                    )\n                                    if full_deeper_path not in nested_metrics:\n                                        nested_metrics[full_deeper_path] = {\n                                            \"tp\": 0,\n                                            \"fa\": 0,\n                                            \"fd\": 0,\n                                            \"fp\": 0,\n                                            \"tn\": 0,\n                                            \"fn\": 0,\n                                        }\n                                    nested_metrics[full_deeper_path][\"fn\"] += (\n                                        deeper_metrics[\"fn\"]\n                                    )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            total_fn += 1\n\n            # Handle unmatched prediction items (false alarms)\n            matched_pred_indices = set(idx for _, idx in assignments)\n            for pred_idx, pred_item in enumerate(pred_list):\n                if pred_idx not in matched_pred_indices:\n                    pred_value = getattr(pred_item, field_name, None)\n                    if not FieldHelper.is_null_value(pred_value):\n                        # Check if this is a List[StructuredModel] that needs deeper processing for FA\n                        if (\n                            isinstance(pred_value, list)\n                            and pred_value\n                            and isinstance(pred_value[0], StructuredModel)\n                        ):\n                            # For List[StructuredModel], count each item in the list as a separate FA\n                            # and handle deeper nested fields\n                            total_fa += len(\n                                pred_value\n                            )  # Each list item is a separate FA\n                            total_fp += len(\n                                pred_value\n                            )  # Each list item is also a separate FP\n\n                            # Also handle deeper nested fields for unmatched items\n                            dummy_empty_list = []  # Empty list for comparison\n                            # We need to create a dummy GT item for comparison to get the structure\n                            if gt_list:  # Use structure from an existing GT item\n                                dummy_gt_item = gt_list[0]\n                                list_classification = (\n                                    dummy_gt_item._calculate_list_confusion_matrix(\n                                        field_name, pred_value\n                                    )\n                                )\n                                if \"nested_fields\" in list_classification:\n                                    for (\n                                        deeper_field_path,\n                                        deeper_metrics,\n                                    ) in list_classification[\"nested_fields\"].items():\n                                        full_deeper_path = (\n                                            f\"{list_field_name}.{deeper_field_path}\"\n                                        )\n                                        if full_deeper_path not in nested_metrics:\n                                            nested_metrics[full_deeper_path] = {\n                                                \"tp\": 0,\n                                                \"fa\": 0,\n                                                \"fd\": 0,\n                                                \"fp\": 0,\n                                                \"tn\": 0,\n                                                \"fn\": 0,\n                                            }\n                                        nested_metrics[full_deeper_path][\"fa\"] += (\n                                            deeper_metrics[\"fa\"]\n                                        )\n                                        nested_metrics[full_deeper_path][\"fp\"] += (\n                                            deeper_metrics[\"fp\"]\n                                        )\n                        else:\n                            # Handle primitive fields or single StructuredModel fields\n                            total_fa += 1\n                            total_fp += 1\n\n            # Store the aggregated metrics for this nested field\n            nested_metrics[nested_field_path] = {\n                \"tp\": total_tp,\n                \"fa\": total_fa,\n                \"fd\": total_fd,\n                \"fp\": total_fp,\n                \"tn\": total_tn,\n                \"fn\": total_fn,\n                \"derived\": MetricsHelper().calculate_derived_metrics(\n                    {\n                        \"tp\": total_tp,\n                        \"fa\": total_fa,\n                        \"fd\": total_fd,\n                        \"fp\": total_fp,\n                        \"tn\": total_tn,\n                        \"fn\": total_fn,\n                    }\n                ),\n            }\n\n        # Add derived metrics for all deeper nested fields that were collected\n        for deeper_path, deeper_metrics in nested_metrics.items():\n            if deeper_path != nested_field_path and \"derived\" not in deeper_metrics:\n                deeper_metrics[\"derived\"] = MetricsHelper().calculate_derived_metrics(\n                    {\n                        \"tp\": deeper_metrics[\"tp\"],\n                        \"fa\": deeper_metrics[\"fa\"],\n                        \"fd\": deeper_metrics[\"fd\"],\n                        \"fp\": deeper_metrics[\"fp\"],\n                        \"tn\": deeper_metrics[\"tn\"],\n                        \"fn\": deeper_metrics[\"fn\"],\n                    }\n                )\n\n        return nested_metrics\n\n    def _calculate_single_nested_field_metrics(\n        self,\n        parent_field_name: str,\n        gt_nested: \"StructuredModel\",\n        pred_nested: \"StructuredModel\",\n        parent_is_aggregate: bool = False,\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Calculate confusion matrix metrics for fields within a single nested StructuredModel.\n\n        Args:\n            parent_field_name: Name of the parent field (e.g., \"address\")\n            gt_nested: Ground truth nested StructuredModel\n            pred_nested: Predicted nested StructuredModel\n            parent_is_aggregate: Whether the parent field should aggregate child metrics\n\n        Returns:\n            Dictionary mapping nested field paths to their confusion matrix metrics\n            E.g., {\"address.street\": {...}, \"address.city\": {...}}\n        \"\"\"\n        nested_metrics = {}\n\n        if not isinstance(gt_nested, StructuredModel) or not isinstance(\n            pred_nested, StructuredModel\n        ):\n            # Handle case where one of the fields is a list of StructuredModel objects\n            if (\n                not isinstance(gt_nested, list)\n                or not gt_nested\n                or not isinstance(gt_nested[0], StructuredModel)\n            ):\n                return nested_metrics\n            return nested_metrics\n\n        # Initialize aggregation metrics for parent field if it's an aggregated field\n        parent_metrics = (\n            {\"tp\": 0, \"fa\": 0, \"fd\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n            if parent_is_aggregate\n            else None\n        )\n\n        # Track which fields are aggregate fields themselves to avoid double counting\n        child_aggregate_fields = set()\n\n        # For each field in the nested model\n        for field_name in gt_nested.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            nested_field_path = f\"{parent_field_name}.{field_name}\"\n\n            # Check if this nested field is itself an aggregate field\n            is_child_aggregate = False\n            if hasattr(gt_nested.__class__, \"_is_aggregate_field\"):\n                is_child_aggregate = gt_nested.__class__._is_aggregate_field(field_name)\n                if is_child_aggregate:\n                    child_aggregate_fields.add(field_name)\n\n            # Get the field value from the prediction\n            pred_value = getattr(pred_nested, field_name, None)\n            gt_value = getattr(gt_nested, field_name)\n\n            # Handle lists of StructuredModel objects\n            if (\n                isinstance(gt_value, list)\n                and isinstance(pred_value, list)\n                and gt_value\n                and isinstance(gt_value[0], StructuredModel)\n            ):\n                # Use the list comparison logic for lists of StructuredModel objects\n                list_metrics = gt_nested._calculate_list_confusion_matrix(\n                    field_name, pred_value\n                )\n\n                # Store the metrics for this nested field\n                nested_metrics[nested_field_path] = {\n                    key: value\n                    for key, value in list_metrics.items()\n                    if key != \"nested_fields\"\n                }\n\n                # Add nested field metrics if available\n                if \"nested_fields\" in list_metrics:\n                    for sub_field, sub_metrics in list_metrics[\"nested_fields\"].items():\n                        full_path = f\"{nested_field_path}.{sub_field.split('.')[-1]}\"\n                        nested_metrics[full_path] = sub_metrics\n            else:\n                # Classify this field comparison\n                field_classification = gt_nested._classify_field_for_confusion_matrix(\n                    field_name, pred_value\n                )\n\n                # Store the metrics for this nested field\n                nested_metrics[nested_field_path] = field_classification\n\n                # Recursively calculate metrics for deeper nesting\n                deeper_metrics = self._calculate_single_nested_field_metrics(\n                    nested_field_path, gt_value, pred_value, is_child_aggregate\n                )\n                nested_metrics.update(deeper_metrics)\n\n                # If this is an aggregate child field, we need to use its aggregated metrics\n                # instead of the direct field comparison metrics\n                if is_child_aggregate and nested_field_path in deeper_metrics:\n                    # For an aggregate child field, we replace its direct metrics with\n                    # the aggregation of its children's metrics\n                    nested_metrics[nested_field_path] = deeper_metrics[\n                        nested_field_path\n                    ]\n\n            # For parent aggregation, we need to be careful not to double count metrics\n            if parent_is_aggregate:\n                if is_child_aggregate:\n                    # If child is an aggregate, use its aggregated metrics for parent\n                    if nested_field_path in deeper_metrics:\n                        child_agg_metrics = deeper_metrics[nested_field_path]\n                        for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                            parent_metrics[metric] += child_agg_metrics.get(metric, 0)\n                else:\n                    # If child is not an aggregate, use its direct field metrics\n                    field_metrics = nested_metrics[nested_field_path]\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        parent_metrics[metric] += field_metrics.get(metric, 0)\n\n        # If parent is an aggregated field, add the aggregated metrics to the result\n        if parent_is_aggregate:\n            # Don't include metrics from child aggregate fields in the parent's metrics\n            # as they've already been counted through their own aggregation\n            for field_name in child_aggregate_fields:\n                nested_field_path = f\"{parent_field_name}.{field_name}\"\n                if nested_field_path in nested_metrics:\n                    # Don't double count these metrics in the parent\n                    field_metrics = nested_metrics[nested_field_path]\n                    # Subtract these metrics from parent_metrics to avoid double counting\n                    for metric in [\"tp\", \"fa\", \"fd\", \"fp\", \"tn\", \"fn\"]:\n                        parent_metrics[metric] -= field_metrics.get(metric, 0)\n\n            nested_metrics[parent_field_name] = parent_metrics\n            # Add derived metrics\n            nested_metrics[parent_field_name][\"derived\"] = (\n                MetricsHelper().calculate_derived_metrics(parent_metrics)\n            )\n\n        return nested_metrics\n\n    def _collect_enhanced_non_matches(\n        self, recursive_result: dict, other: \"StructuredModel\"\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Collect enhanced non-matches with object-level granularity.\n\n        Args:\n            recursive_result: Result from compare_recursive containing field comparison details\n            other: The predicted StructuredModel instance\n\n        Returns:\n            List of non-match dictionaries with enhanced object-level information\n        \"\"\"\n        all_non_matches = []\n\n        # Walk through the recursive result and collect non-matches\n        for field_name, field_result in recursive_result.get(\"fields\", {}).items():\n            gt_val = getattr(self, field_name)\n            pred_val = getattr(other, field_name, None)\n\n            # Check if this is a list field that should use object-level collection\n            if (\n                isinstance(gt_val, list)\n                and isinstance(pred_val, list)\n                and gt_val\n                and isinstance(gt_val[0], StructuredModel)\n            ):\n                # Use NonMatchesHelper for object-level collection\n                helper = NonMatchesHelper()\n                object_non_matches = helper.collect_list_non_matches(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(object_non_matches)\n\n            # Handle null list cases\n            elif (\n                (gt_val is None or (isinstance(gt_val, list) and len(gt_val) == 0))\n                and isinstance(pred_val, list)\n                and len(pred_val) &gt; 0\n            ):\n                # GT empty, pred has items \u2192 use helper for FA entries\n                helper = NonMatchesHelper()\n                null_non_matches = helper.add_non_matches_for_null_cases(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(null_non_matches)\n\n            elif (\n                isinstance(gt_val, list)\n                and len(gt_val) &gt; 0\n                and (\n                    pred_val is None\n                    or (isinstance(pred_val, list) and len(pred_val) == 0)\n                )\n            ):\n                # GT has items, pred empty \u2192 use helper for FN entries\n                helper = NonMatchesHelper()\n                null_non_matches = helper.add_non_matches_for_null_cases(\n                    field_name, gt_val, pred_val\n                )\n                all_non_matches.extend(null_non_matches)\n\n            else:\n                # Use existing field-level logic for non-list fields\n                # Extract metrics from field result to determine non-match type\n                if isinstance(field_result, dict) and \"overall\" in field_result:\n                    metrics = field_result[\"overall\"]\n                elif isinstance(field_result, dict):\n                    metrics = field_result\n                else:\n                    continue  # Skip if we can't extract metrics\n\n                # Create field-level non-match entries based on metrics (legacy format for backward compatibility)\n                if metrics.get(\"fa\", 0) &gt; 0:  # False Alarm\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_ALARM,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"details\": {\"reason\": \"unmatched prediction\"},\n                    }\n                    all_non_matches.append(entry)\n                elif metrics.get(\"fn\", 0) &gt; 0:  # False Negative\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_NEGATIVE,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"details\": {\"reason\": \"unmatched ground truth\"},\n                    }\n                    all_non_matches.append(entry)\n                elif metrics.get(\"fd\", 0) &gt; 0:  # False Discovery\n                    similarity = field_result.get(\"raw_similarity_score\")\n                    entry = {\n                        \"field_path\": field_name,\n                        \"non_match_type\": NonMatchType.FALSE_DISCOVERY,  # Use enum value\n                        \"ground_truth_value\": gt_val,\n                        \"prediction_value\": pred_val,\n                        \"similarity_score\": similarity,\n                        \"details\": {\"reason\": \"below threshold\"},\n                    }\n                    if similarity is not None:\n                        info = self._get_comparison_info(field_name)\n                        entry[\"details\"][\"reason\"] = (\n                            f\"below threshold ({similarity:.3f} &lt; {info.threshold})\"\n                        )\n                    all_non_matches.append(entry)\n\n                # ADDITIONAL: Handle nested StructuredModel objects for detailed non-match collection\n                if (\n                    isinstance(gt_val, StructuredModel)\n                    and isinstance(pred_val, StructuredModel)\n                    and \"fields\" in field_result\n                ):\n                    # Recursively collect non-matches from nested objects\n                    nested_non_matches = gt_val._collect_enhanced_non_matches(\n                        field_result, pred_val\n                    )\n                    # Prefix nested field paths with the parent field name\n                    for nested_nm in nested_non_matches:\n                        nested_nm[\"field_path\"] = (\n                            f\"{field_name}.{nested_nm['field_path']}\"\n                        )\n                        all_non_matches.append(nested_nm)\n\n        return all_non_matches\n\n    def _collect_non_matches(\n        self, other: \"StructuredModel\", base_path: str = \"\"\n    ) -&gt; List[NonMatchField]:\n        \"\"\"Collect non-matches for detailed analysis.\n\n        Args:\n            other: Other model to compare with\n            base_path: Base path for field naming (e.g., \"address\")\n\n        Returns:\n            List of NonMatchField objects documenting non-matches\n        \"\"\"\n        non_matches = []\n\n        # Handle null cases\n        if other is None:\n            non_matches.append(\n                NonMatchField(\n                    field_path=base_path or \"root\",\n                    non_match_type=NonMatchType.FALSE_NEGATIVE,\n                    ground_truth_value=self,\n                    prediction_value=None,\n                )\n            )\n            return non_matches\n\n        # Compare each field\n        for field_name in self.__class__.model_fields:\n            if field_name == \"extra_fields\":\n                continue\n\n            field_path = f\"{base_path}.{field_name}\" if base_path else field_name\n            gt_value = getattr(self, field_name)\n            pred_value = getattr(other, field_name, None)\n\n            # Use existing field classification logic\n            if isinstance(pred_value, list):\n                classification = self._calculate_list_confusion_matrix(\n                    field_name, pred_value\n                )\n            else:\n                classification = self._classify_field_for_confusion_matrix(\n                    field_name, pred_value\n                )\n\n            # Document non-matches based on classification\n            if classification[\"fa\"] &gt; 0:  # False Alarm\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_ALARM,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                        similarity_score=classification.get(\"similarity_score\"),\n                    )\n                )\n            elif classification[\"fn\"] &gt; 0:  # False Negative\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_NEGATIVE,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                    )\n                )\n            elif classification[\"fd\"] &gt; 0:  # False Discovery\n                non_matches.append(\n                    NonMatchField(\n                        field_path=field_path,\n                        non_match_type=NonMatchType.FALSE_DISCOVERY,\n                        ground_truth_value=gt_value,\n                        prediction_value=pred_value,\n                        similarity_score=classification.get(\"similarity_score\"),\n                    )\n                )\n\n            # Handle nested models recursively\n            if isinstance(gt_value, StructuredModel) and isinstance(\n                pred_value, StructuredModel\n            ):\n                nested_non_matches = gt_value._collect_non_matches(\n                    pred_value, field_path\n                )\n                non_matches.extend(nested_non_matches)\n\n        return non_matches\n\n    def compare(self, other: \"StructuredModel\") -&gt; float:\n        \"\"\"Compare this model with another and return a scalar similarity score.\n\n        Returns the overall weighted average score regardless of sufficient/necessary field matching.\n        This provides a more nuanced score for use in comparators.\n\n        Args:\n            other: Another instance of the same model to compare with\n\n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        # We'll calculate the overall weighted score directly instead of using compare_with\n        # This ensures that sufficient/necessary field rules don't cause a zero score\n        # when at least some fields match\n\n        total_score = 0.0\n        total_weight = 0.0\n\n        for field_name in self.__class__.model_fields:\n            # Skip the extra_fields attribute in comparison\n            if field_name == \"extra_fields\":\n                continue\n            if hasattr(other, field_name):\n                # Get field configuration\n                info = self.__class__._get_comparison_info(field_name)\n                # Use weight from ComparableField object\n                weight = info.weight\n\n                # Compare field values WITHOUT applying thresholds\n                field_score = self.compare_field_raw(\n                    field_name, getattr(other, field_name)\n                )\n\n                # Update total score\n                total_score += field_score * weight\n                total_weight += weight\n\n        # Calculate overall score\n        if total_weight &gt; 0:\n            return total_score / total_weight\n        else:\n            return 0.0\n\n    def compare_with(\n        self,\n        other: \"StructuredModel\",\n        include_confusion_matrix: bool = False,\n        document_non_matches: bool = False,\n        evaluator_format: bool = False,\n        recall_with_fd: bool = False,\n        add_derived_metrics: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compare this model with another instance using SINGLE TRAVERSAL optimization.\n\n        Args:\n            other: Another instance of the same model to compare with\n            include_confusion_matrix: Whether to include confusion matrix calculations\n            document_non_matches: Whether to document non-matches for analysis\n            evaluator_format: Whether to format results for the evaluator\n            recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                            If False, use traditional recall (TP/(TP+FN))\n            add_derived_metrics: Whether to add derived metrics to confusion matrix\n\n        Returns:\n            Dictionary with comparison results including:\n            - field_scores: Scores for each field\n            - overall_score: Weighted average score\n            - all_fields_matched: Whether all fields matched\n            - confusion_matrix: (optional) Confusion matrix data if requested\n            - non_matches: (optional) Non-match documentation if requested\n        \"\"\"\n        # SINGLE TRAVERSAL: Get everything in one pass\n        recursive_result = self.compare_recursive(other)\n\n        # Extract scoring information from recursive result\n        field_scores = {}\n        for field_name, field_result in recursive_result[\"fields\"].items():\n            if isinstance(field_result, dict):\n                # Use threshold_applied_score when available, which respects clip_under_threshold setting\n                if \"threshold_applied_score\" in field_result:\n                    field_scores[field_name] = field_result[\"threshold_applied_score\"]\n                # Fallback to raw_similarity_score if threshold_applied_score not available\n                elif \"raw_similarity_score\" in field_result:\n                    field_scores[field_name] = field_result[\"raw_similarity_score\"]\n\n        # Extract overall metrics\n        overall_result = recursive_result[\"overall\"]\n        overall_score = overall_result.get(\"similarity_score\", 0.0)\n        all_fields_matched = overall_result.get(\"all_fields_matched\", False)\n\n        # Build basic result structure\n        result = {\n            \"field_scores\": field_scores,\n            \"overall_score\": overall_score,\n            \"all_fields_matched\": all_fields_matched,\n        }\n\n        # Add optional features using already-computed recursive result\n        if include_confusion_matrix:\n            confusion_matrix = recursive_result\n\n            # Add universal aggregate metrics to all nodes\n            confusion_matrix = self._calculate_aggregate_metrics(confusion_matrix)\n\n            # Add derived metrics if requested\n            if add_derived_metrics:\n                confusion_matrix = self._add_derived_metrics_to_result(confusion_matrix)\n\n            result[\"confusion_matrix\"] = confusion_matrix\n\n        # Add optional non-match documentation\n        if document_non_matches:\n            # NEW: Collect enhanced object-level non-matches\n            non_matches = self._collect_enhanced_non_matches(recursive_result, other)\n            result[\"non_matches\"] = non_matches\n\n        # If evaluator_format is requested, transform the result\n        if evaluator_format:\n            return self._format_for_evaluator(result, other, recall_with_fd)\n\n        return result\n\n    def _convert_score_to_binary_metrics(\n        self, score: float, threshold: float = 0.5\n    ) -&gt; Dict[str, float]:\n        \"\"\"Convert similarity score to binary classification metrics using MetricsHelper.\n\n        Args:\n            score: Similarity score [0-1]\n            threshold: Threshold for considering a match\n\n        Returns:\n            Dictionary with TP, FP, FN, TN counts converted to metrics\n        \"\"\"\n        metrics_helper = MetricsHelper()\n        return metrics_helper.convert_score_to_binary_metrics(score, threshold)\n\n    def _format_for_evaluator(\n        self,\n        result: Dict[str, Any],\n        other: \"StructuredModel\",\n        recall_with_fd: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Format comparison results for evaluator compatibility.\n\n        Args:\n            result: Standard comparison result from compare_with\n            other: The other model being compared\n            recall_with_fd: Whether to include FD in recall denominator\n\n        Returns:\n            Dictionary in evaluator format with overall, fields, confusion_matrix\n        \"\"\"\n        return EvaluatorFormatHelper.format_for_evaluator(\n            self, result, other, recall_with_fd\n        )\n\n    def _calculate_list_item_metrics(\n        self,\n        field_name: str,\n        gt_list: List[Any],\n        pred_list: List[Any],\n        recall_with_fd: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Calculate metrics for individual items in a list field.\n\n        Args:\n            field_name: Name of the list field\n            gt_list: Ground truth list\n            pred_list: Prediction list\n            recall_with_fd: Whether to include FD in recall denominator\n\n        Returns:\n            List of metrics dictionaries for each matched item pair\n        \"\"\"\n        return EvaluatorFormatHelper.calculate_list_item_metrics(\n            field_name, gt_list, pred_list, recall_with_fd\n        )\n\n    @classmethod\n    def model_json_schema(cls, **kwargs):\n        \"\"\"Override to add model-level comparison metadata.\n\n        Extends the standard Pydantic JSON schema with comparison metadata\n        at the field level.\n\n        Args:\n            **kwargs: Arguments to pass to the parent method\n\n        Returns:\n            JSON schema with added comparison metadata\n        \"\"\"\n        schema = super().model_json_schema(**kwargs)\n\n        # Add comparison metadata to each field in the schema\n        for field_name, field_info in cls.model_fields.items():\n            if field_name == \"extra_fields\":\n                continue\n\n            # Get the schema property for this field\n            if field_name not in schema.get(\"properties\", {}):\n                continue\n\n            field_props = schema[\"properties\"][field_name]\n\n            # Since ComparableField is now always a function, check for json_schema_extra\n            if hasattr(field_info, \"json_schema_extra\") and callable(\n                field_info.json_schema_extra\n            ):\n                # Fallback: Check for json_schema_extra function\n                temp_schema = {}\n                field_info.json_schema_extra(temp_schema)\n\n                if \"x-comparison\" in temp_schema:\n                    # Copy the comparison metadata from the temp schema to the real schema\n                    field_props[\"x-comparison\"] = temp_schema[\"x-comparison\"]\n\n        return schema\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Validate field configurations when a StructuredModel subclass is defined.</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Validate field configurations when a StructuredModel subclass is defined.\"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # Validate field configurations using class annotations since model_fields isn't populated yet\n    if hasattr(cls, \"__annotations__\"):\n        for field_name, field_type in cls.__annotations__.items():\n            if field_name == \"extra_fields\":\n                continue\n\n            # Get the field default value if it exists\n            field_default = getattr(cls, field_name, None)\n\n            # Since ComparableField is now always a function that returns a Field,\n            # we need to check if field_default has comparison metadata\n            if hasattr(field_default, \"json_schema_extra\") and callable(\n                field_default.json_schema_extra\n            ):\n                # Check for comparison metadata\n                temp_schema = {}\n                field_default.json_schema_extra(temp_schema)\n                if \"x-comparison\" in temp_schema:\n                    # This field was created with ComparableField function - validate constraints\n                    if cls._is_list_of_structured_model_type(field_type):\n                        comparison_config = temp_schema[\"x-comparison\"]\n\n                        # Threshold validation - only flag if explicitly set to non-default value\n                        threshold = comparison_config.get(\"threshold\", 0.5)\n                        if threshold != 0.5:  # Default threshold value\n                            raise ValueError(\n                                f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                f\"'threshold' parameter in ComparableField. Hungarian matching uses each \"\n                                f\"StructuredModel's 'match_threshold' class attribute instead. \"\n                                f\"Set 'match_threshold = {threshold}' on the list element class.\"\n                            )\n\n                        # Comparator validation - only flag if explicitly set to non-default type\n                        comparator_type = comparison_config.get(\n                            \"comparator_type\", \"LevenshteinComparator\"\n                        )\n                        if (\n                            comparator_type != \"LevenshteinComparator\"\n                        ):  # Default comparator type\n                            raise ValueError(\n                                f\"Field '{field_name}' is a List[StructuredModel] and cannot have a \"\n                                f\"'comparator' parameter in ComparableField. Object comparison uses each \"\n                                f\"StructuredModel's individual field comparators instead.\"\n                            )\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare","title":"<code>compare(other)</code>","text":"<p>Compare this model with another and return a scalar similarity score.</p> <p>Returns the overall weighted average score regardless of sufficient/necessary field matching. This provides a more nuanced score for use in comparators.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare(self, other: \"StructuredModel\") -&gt; float:\n    \"\"\"Compare this model with another and return a scalar similarity score.\n\n    Returns the overall weighted average score regardless of sufficient/necessary field matching.\n    This provides a more nuanced score for use in comparators.\n\n    Args:\n        other: Another instance of the same model to compare with\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    # We'll calculate the overall weighted score directly instead of using compare_with\n    # This ensures that sufficient/necessary field rules don't cause a zero score\n    # when at least some fields match\n\n    total_score = 0.0\n    total_weight = 0.0\n\n    for field_name in self.__class__.model_fields:\n        # Skip the extra_fields attribute in comparison\n        if field_name == \"extra_fields\":\n            continue\n        if hasattr(other, field_name):\n            # Get field configuration\n            info = self.__class__._get_comparison_info(field_name)\n            # Use weight from ComparableField object\n            weight = info.weight\n\n            # Compare field values WITHOUT applying thresholds\n            field_score = self.compare_field_raw(\n                field_name, getattr(other, field_name)\n            )\n\n            # Update total score\n            total_score += field_score * weight\n            total_weight += weight\n\n    # Calculate overall score\n    if total_weight &gt; 0:\n        return total_score / total_weight\n    else:\n        return 0.0\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_field","title":"<code>compare_field(field_name, other_value)</code>","text":"<p>Compare a single field with a value using the configured comparator.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to compare</p> required <code>other_value</code> <code>Any</code> <p>Value to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_field(self, field_name: str, other_value: Any) -&gt; float:\n    \"\"\"Compare a single field with a value using the configured comparator.\n\n    Args:\n        field_name: Name of the field to compare\n        other_value: Value to compare with\n\n    Returns:\n        Similarity score between 0.0 and 1.0\n    \"\"\"\n    # Get our field value\n    my_value = getattr(self, field_name)\n\n    # If both values are StructuredModel instances, use recursive compare_with\n    if isinstance(my_value, StructuredModel) and isinstance(\n        other_value, StructuredModel\n    ):\n        # Use compare_with for rich comparison\n        comparison_result = my_value.compare_with(\n            other_value,\n            include_confusion_matrix=False,\n            document_non_matches=False,\n            evaluator_format=False,\n            recall_with_fd=False,\n        )\n        # Apply field-level threshold if configured\n        info = self._get_comparison_info(field_name)\n        raw_score = comparison_result[\"overall_score\"]\n        return (\n            raw_score\n            if raw_score &gt;= info.threshold or not info.clip_under_threshold\n            else 0.0\n        )\n\n    # CRITICAL FIX: For lists, don't clip under threshold for partial matches\n    if isinstance(my_value, list) and isinstance(other_value, list):\n        # Get field info\n        info = self._get_comparison_info(field_name)\n\n        # Use the raw comparison result without threshold clipping for lists\n        result = ComparisonHelper.compare_unordered_lists(\n            my_value, other_value, info.comparator, info.threshold\n        )\n\n        # Return the overall score directly (don't clip based on threshold for lists)\n        return result[\"overall_score\"]\n\n    # For other fields, use existing logic\n    return ComparisonHelper.compare_field_with_threshold(\n        self, field_name, other_value\n    )\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_field_raw","title":"<code>compare_field_raw(field_name, other_value)</code>","text":"<p>Compare a single field with a value WITHOUT applying thresholds.</p> <p>This version is used by the compare method to get raw similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to compare</p> required <code>other_value</code> <code>Any</code> <p>Value to compare with</p> required <p>Returns:</p> Type Description <code>float</code> <p>Raw similarity score between 0.0 and 1.0 without threshold filtering</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_field_raw(self, field_name: str, other_value: Any) -&gt; float:\n    \"\"\"Compare a single field with a value WITHOUT applying thresholds.\n\n    This version is used by the compare method to get raw similarity scores.\n\n    Args:\n        field_name: Name of the field to compare\n        other_value: Value to compare with\n\n    Returns:\n        Raw similarity score between 0.0 and 1.0 without threshold filtering\n    \"\"\"\n    # Get our field value\n    my_value = getattr(self, field_name)\n\n    # If both values are StructuredModel instances, use recursive compare_with\n    if isinstance(my_value, StructuredModel) and isinstance(\n        other_value, StructuredModel\n    ):\n        # Use compare_with for rich comparison, but extract the raw score\n        comparison_result = my_value.compare_with(\n            other_value,\n            include_confusion_matrix=False,\n            document_non_matches=False,\n            evaluator_format=False,\n            recall_with_fd=False,\n        )\n        return comparison_result[\"overall_score\"]\n\n    # For non-StructuredModel fields, use existing logic\n    return ComparisonHelper.compare_field_raw(self, field_name, other_value)\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_recursive","title":"<code>compare_recursive(other)</code>","text":"<p>The ONE clean recursive function that handles everything.</p> <p>Enhanced to capture BOTH confusion matrix metrics AND similarity scores in a single traversal to eliminate double traversal inefficiency.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with clean hierarchical structure:</p> <code>dict</code> <ul> <li>overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched</li> </ul> <code>dict</code> <ul> <li>fields: Recursive structure for each field with scores</li> </ul> <code>dict</code> <ul> <li>non_matches: List of non-matching items</li> </ul> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_recursive(self, other: \"StructuredModel\") -&gt; dict:\n    \"\"\"The ONE clean recursive function that handles everything.\n\n    Enhanced to capture BOTH confusion matrix metrics AND similarity scores\n    in a single traversal to eliminate double traversal inefficiency.\n\n    Args:\n        other: Another instance of the same model to compare with\n\n    Returns:\n        Dictionary with clean hierarchical structure:\n        - overall: TP, FP, TN, FN, FD, FA counts + similarity_score + all_fields_matched\n        - fields: Recursive structure for each field with scores\n        - non_matches: List of non-matching items\n    \"\"\"\n    result = {\n        \"overall\": {\n            \"tp\": 0,\n            \"fa\": 0,\n            \"fd\": 0,\n            \"fp\": 0,\n            \"tn\": 0,\n            \"fn\": 0,\n            \"similarity_score\": 0.0,\n            \"all_fields_matched\": False,\n        },\n        \"fields\": {},\n        \"non_matches\": [],\n    }\n\n    # Score percolation variables\n    total_score = 0.0\n    total_weight = 0.0\n    threshold_matched_fields = set()\n\n    for field_name in self.__class__.model_fields:\n        if field_name == \"extra_fields\":\n            continue\n\n        gt_val = getattr(self, field_name)\n        pred_val = getattr(other, field_name, None)\n\n        # Enhanced dispatch returns both metrics AND scores\n        field_result = self._dispatch_field_comparison(field_name, gt_val, pred_val)\n\n        result[\"fields\"][field_name] = field_result\n\n        # Simple aggregation to overall metrics\n        self._aggregate_to_overall(field_result, result[\"overall\"])\n\n        # Score percolation - aggregate scores upward\n        if \"similarity_score\" in field_result and \"weight\" in field_result:\n            weight = field_result[\"weight\"]\n            threshold_applied_score = field_result[\"threshold_applied_score\"]\n            total_score += threshold_applied_score * weight\n            total_weight += weight\n\n            # Track threshold-matched fields\n            info = self._get_comparison_info(field_name)\n            if field_result[\"raw_similarity_score\"] &gt;= info.threshold:\n                threshold_matched_fields.add(field_name)\n\n    # CRITICAL FIX: Handle hallucinated fields (extra fields) as False Alarms\n    extra_fields_fa = self._count_extra_fields_as_false_alarms(other)\n    result[\"overall\"][\"fa\"] += extra_fields_fa\n    result[\"overall\"][\"fp\"] += extra_fields_fa\n\n    # Calculate overall similarity score from percolated scores\n    if total_weight &gt; 0:\n        result[\"overall\"][\"similarity_score\"] = total_score / total_weight\n\n    # Determine all_fields_matched\n    model_fields_for_comparison = set(self.__class__.model_fields.keys()) - {\n        \"extra_fields\"\n    }\n    result[\"overall\"][\"all_fields_matched\"] = len(threshold_matched_fields) == len(\n        model_fields_for_comparison\n    )\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.compare_with","title":"<code>compare_with(other, include_confusion_matrix=False, document_non_matches=False, evaluator_format=False, recall_with_fd=False, add_derived_metrics=True)</code>","text":"<p>Compare this model with another instance using SINGLE TRAVERSAL optimization.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StructuredModel</code> <p>Another instance of the same model to compare with</p> required <code>include_confusion_matrix</code> <code>bool</code> <p>Whether to include confusion matrix calculations</p> <code>False</code> <code>document_non_matches</code> <code>bool</code> <p>Whether to document non-matches for analysis</p> <code>False</code> <code>evaluator_format</code> <code>bool</code> <p>Whether to format results for the evaluator</p> <code>False</code> <code>recall_with_fd</code> <code>bool</code> <p>If True, include FD in recall denominator (TP/(TP+FN+FD))             If False, use traditional recall (TP/(TP+FN))</p> <code>False</code> <code>add_derived_metrics</code> <code>bool</code> <p>Whether to add derived metrics to confusion matrix</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with comparison results including:</p> <code>Dict[str, Any]</code> <ul> <li>field_scores: Scores for each field</li> </ul> <code>Dict[str, Any]</code> <ul> <li>overall_score: Weighted average score</li> </ul> <code>Dict[str, Any]</code> <ul> <li>all_fields_matched: Whether all fields matched</li> </ul> <code>Dict[str, Any]</code> <ul> <li>confusion_matrix: (optional) Confusion matrix data if requested</li> </ul> <code>Dict[str, Any]</code> <ul> <li>non_matches: (optional) Non-match documentation if requested</li> </ul> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>def compare_with(\n    self,\n    other: \"StructuredModel\",\n    include_confusion_matrix: bool = False,\n    document_non_matches: bool = False,\n    evaluator_format: bool = False,\n    recall_with_fd: bool = False,\n    add_derived_metrics: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare this model with another instance using SINGLE TRAVERSAL optimization.\n\n    Args:\n        other: Another instance of the same model to compare with\n        include_confusion_matrix: Whether to include confusion matrix calculations\n        document_non_matches: Whether to document non-matches for analysis\n        evaluator_format: Whether to format results for the evaluator\n        recall_with_fd: If True, include FD in recall denominator (TP/(TP+FN+FD))\n                        If False, use traditional recall (TP/(TP+FN))\n        add_derived_metrics: Whether to add derived metrics to confusion matrix\n\n    Returns:\n        Dictionary with comparison results including:\n        - field_scores: Scores for each field\n        - overall_score: Weighted average score\n        - all_fields_matched: Whether all fields matched\n        - confusion_matrix: (optional) Confusion matrix data if requested\n        - non_matches: (optional) Non-match documentation if requested\n    \"\"\"\n    # SINGLE TRAVERSAL: Get everything in one pass\n    recursive_result = self.compare_recursive(other)\n\n    # Extract scoring information from recursive result\n    field_scores = {}\n    for field_name, field_result in recursive_result[\"fields\"].items():\n        if isinstance(field_result, dict):\n            # Use threshold_applied_score when available, which respects clip_under_threshold setting\n            if \"threshold_applied_score\" in field_result:\n                field_scores[field_name] = field_result[\"threshold_applied_score\"]\n            # Fallback to raw_similarity_score if threshold_applied_score not available\n            elif \"raw_similarity_score\" in field_result:\n                field_scores[field_name] = field_result[\"raw_similarity_score\"]\n\n    # Extract overall metrics\n    overall_result = recursive_result[\"overall\"]\n    overall_score = overall_result.get(\"similarity_score\", 0.0)\n    all_fields_matched = overall_result.get(\"all_fields_matched\", False)\n\n    # Build basic result structure\n    result = {\n        \"field_scores\": field_scores,\n        \"overall_score\": overall_score,\n        \"all_fields_matched\": all_fields_matched,\n    }\n\n    # Add optional features using already-computed recursive result\n    if include_confusion_matrix:\n        confusion_matrix = recursive_result\n\n        # Add universal aggregate metrics to all nodes\n        confusion_matrix = self._calculate_aggregate_metrics(confusion_matrix)\n\n        # Add derived metrics if requested\n        if add_derived_metrics:\n            confusion_matrix = self._add_derived_metrics_to_result(confusion_matrix)\n\n        result[\"confusion_matrix\"] = confusion_matrix\n\n    # Add optional non-match documentation\n    if document_non_matches:\n        # NEW: Collect enhanced object-level non-matches\n        non_matches = self._collect_enhanced_non_matches(recursive_result, other)\n        result[\"non_matches\"] = non_matches\n\n    # If evaluator_format is requested, transform the result\n    if evaluator_format:\n        return self._format_for_evaluator(result, other, recall_with_fd)\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.from_json","title":"<code>from_json(json_data)</code>  <code>classmethod</code>","text":"<p>Create a StructuredModel instance from JSON data.</p> <p>This method handles missing fields gracefully and stores extra fields in the extra_fields attribute.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>Dict[str, Any]</code> <p>Dictionary containing the JSON data</p> required <p>Returns:</p> Type Description <code>StructuredModel</code> <p>StructuredModel instance created from the JSON data</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef from_json(cls, json_data: Dict[str, Any]) -&gt; \"StructuredModel\":\n    \"\"\"Create a StructuredModel instance from JSON data.\n\n    This method handles missing fields gracefully and stores extra fields\n    in the extra_fields attribute.\n\n    Args:\n        json_data: Dictionary containing the JSON data\n\n    Returns:\n        StructuredModel instance created from the JSON data\n    \"\"\"\n    return ConfigurationHelper.from_json(cls, json_data)\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.model_from_json","title":"<code>model_from_json(config)</code>  <code>classmethod</code>","text":"<p>Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().</p> <p>This method leverages Pydantic's native dynamic model creation capabilities to ensure full compatibility with all Pydantic features while adding structured comparison functionality through inherited StructuredModel methods.</p> <p>The generated model inherits all StructuredModel capabilities: - compare_with() method for detailed comparisons - Field-level comparison configuration - Hungarian algorithm for list matching - Confusion matrix generation - JSON schema with comparison metadata</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>JSON configuration with fields, comparators, and model settings.    Required keys:    - fields: Dict mapping field names to field configurations    Optional keys:    - model_name: Name for the generated class (default: \"DynamicModel\")    - match_threshold: Overall matching threshold (default: 0.7)</p> <p>Field configuration format:    {        \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required        \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional        \"threshold\": 0.8,  # Optional, default 0.5        \"weight\": 2.0,     # Optional, default 1.0        \"required\": true,  # Optional, default false        \"default\": \"value\", # Optional        \"description\": \"Field description\",  # Optional        \"alias\": \"field_alias\",  # Optional        \"examples\": [\"example1\", \"example2\"]  # Optional    }</p> required <p>Returns:</p> Type Description <code>Type[StructuredModel]</code> <p>A fully functional StructuredModel subclass created with create_model()</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid or contains unsupported types/comparators</p> <code>KeyError</code> <p>If required configuration keys are missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"model_name\": \"Product\",\n...     \"match_threshold\": 0.8,\n...     \"fields\": {\n...         \"name\": {\n...             \"type\": \"str\",\n...             \"comparator\": \"LevenshteinComparator\",\n...             \"threshold\": 0.8,\n...             \"weight\": 2.0,\n...             \"required\": True\n...         },\n...         \"price\": {\n...             \"type\": \"float\",\n...             \"comparator\": \"NumericComparator\",\n...             \"default\": 0.0\n...         }\n...     }\n... }\n&gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n&gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\nTrue\n&gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n&gt;&gt;&gt; product.name\n'Widget'\n&gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n&gt;&gt;&gt; result[\"overall_score\"]\n1.0\n</code></pre> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef model_from_json(cls, config: Dict[str, Any]) -&gt; Type[\"StructuredModel\"]:\n    \"\"\"Create a StructuredModel subclass from JSON configuration using Pydantic's create_model().\n\n    This method leverages Pydantic's native dynamic model creation capabilities to ensure\n    full compatibility with all Pydantic features while adding structured comparison\n    functionality through inherited StructuredModel methods.\n\n    The generated model inherits all StructuredModel capabilities:\n    - compare_with() method for detailed comparisons\n    - Field-level comparison configuration\n    - Hungarian algorithm for list matching\n    - Confusion matrix generation\n    - JSON schema with comparison metadata\n\n    Args:\n        config: JSON configuration with fields, comparators, and model settings.\n               Required keys:\n               - fields: Dict mapping field names to field configurations\n               Optional keys:\n               - model_name: Name for the generated class (default: \"DynamicModel\")\n               - match_threshold: Overall matching threshold (default: 0.7)\n\n               Field configuration format:\n               {\n                   \"type\": \"str|int|float|bool|List[str]|etc.\",  # Required\n                   \"comparator\": \"LevenshteinComparator|ExactComparator|etc.\",  # Optional\n                   \"threshold\": 0.8,  # Optional, default 0.5\n                   \"weight\": 2.0,     # Optional, default 1.0\n                   \"required\": true,  # Optional, default false\n                   \"default\": \"value\", # Optional\n                   \"description\": \"Field description\",  # Optional\n                   \"alias\": \"field_alias\",  # Optional\n                   \"examples\": [\"example1\", \"example2\"]  # Optional\n               }\n\n    Returns:\n        A fully functional StructuredModel subclass created with create_model()\n\n    Raises:\n        ValueError: If configuration is invalid or contains unsupported types/comparators\n        KeyError: If required configuration keys are missing\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"model_name\": \"Product\",\n        ...     \"match_threshold\": 0.8,\n        ...     \"fields\": {\n        ...         \"name\": {\n        ...             \"type\": \"str\",\n        ...             \"comparator\": \"LevenshteinComparator\",\n        ...             \"threshold\": 0.8,\n        ...             \"weight\": 2.0,\n        ...             \"required\": True\n        ...         },\n        ...         \"price\": {\n        ...             \"type\": \"float\",\n        ...             \"comparator\": \"NumericComparator\",\n        ...             \"default\": 0.0\n        ...         }\n        ...     }\n        ... }\n        &gt;&gt;&gt; ProductClass = StructuredModel.model_from_json(config)\n        &gt;&gt;&gt; isinstance(ProductClass.model_fields, dict)  # Full Pydantic compatibility\n        True\n        &gt;&gt;&gt; product = ProductClass(name=\"Widget\", price=29.99)\n        &gt;&gt;&gt; product.name\n        'Widget'\n        &gt;&gt;&gt; result = product.compare_with(ProductClass(name=\"Widget\", price=29.99))\n        &gt;&gt;&gt; result[\"overall_score\"]\n        1.0\n    \"\"\"\n    from pydantic import create_model\n    from .field_converter import convert_fields_config, validate_fields_config\n\n    # Validate configuration structure\n    if not isinstance(config, dict):\n        raise ValueError(\"Configuration must be a dictionary\")\n\n    if \"fields\" not in config:\n        raise ValueError(\"Configuration must contain 'fields' key\")\n\n    fields_config = config[\"fields\"]\n    if not isinstance(fields_config, dict) or len(fields_config) == 0:\n        raise ValueError(\"'fields' must be a non-empty dictionary\")\n\n    # Validate all field configurations before proceeding (including nested schema validation)\n    try:\n        from .field_converter import get_global_converter\n\n        converter = get_global_converter()\n\n        # First validate basic field configurations\n        validate_fields_config(fields_config)\n\n        # Then validate nested schema rules\n        for field_name, field_config in fields_config.items():\n            converter.validate_nested_field_schema(field_name, field_config)\n\n    except ValueError as e:\n        raise ValueError(f\"Invalid field configuration: {e}\")\n\n    # Extract model configuration\n    model_name = config.get(\"model_name\", \"DynamicModel\")\n    match_threshold = config.get(\"match_threshold\", 0.7)\n\n    # Validate model name\n    if not isinstance(model_name, str) or not model_name.isidentifier():\n        raise ValueError(\n            f\"model_name must be a valid Python identifier, got: {model_name}\"\n        )\n\n    # Validate match threshold\n    if not isinstance(match_threshold, (int, float)) or not (\n        0.0 &lt;= match_threshold &lt;= 1.0\n    ):\n        raise ValueError(\n            f\"match_threshold must be a number between 0.0 and 1.0, got: {match_threshold}\"\n        )\n\n    # Convert field configurations to Pydantic field definitions\n    try:\n        field_definitions = convert_fields_config(fields_config)\n    except ValueError as e:\n        raise ValueError(f\"Error converting field configurations: {e}\")\n\n    # Create the dynamic model extending StructuredModel\n    try:\n        DynamicClass = create_model(\n            model_name,\n            __base__=cls,  # Extend StructuredModel\n            **field_definitions,\n        )\n    except Exception as e:\n        raise ValueError(f\"Error creating dynamic model: {e}\")\n\n    # Set class-level attributes\n    DynamicClass.match_threshold = match_threshold\n\n    # Add configuration metadata for debugging/introspection\n    DynamicClass._model_config = config\n\n    return DynamicClass\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.structured_model.StructuredModel.model_json_schema","title":"<code>model_json_schema(**kwargs)</code>  <code>classmethod</code>","text":"<p>Override to add model-level comparison metadata.</p> <p>Extends the standard Pydantic JSON schema with comparison metadata at the field level.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arguments to pass to the parent method</p> <code>{}</code> <p>Returns:</p> Type Description <p>JSON schema with added comparison metadata</p> Source code in <code>stickler/structured_object_evaluator/models/structured_model.py</code> <pre><code>@classmethod\ndef model_json_schema(cls, **kwargs):\n    \"\"\"Override to add model-level comparison metadata.\n\n    Extends the standard Pydantic JSON schema with comparison metadata\n    at the field level.\n\n    Args:\n        **kwargs: Arguments to pass to the parent method\n\n    Returns:\n        JSON schema with added comparison metadata\n    \"\"\"\n    schema = super().model_json_schema(**kwargs)\n\n    # Add comparison metadata to each field in the schema\n    for field_name, field_info in cls.model_fields.items():\n        if field_name == \"extra_fields\":\n            continue\n\n        # Get the schema property for this field\n        if field_name not in schema.get(\"properties\", {}):\n            continue\n\n        field_props = schema[\"properties\"][field_name]\n\n        # Since ComparableField is now always a function, check for json_schema_extra\n        if hasattr(field_info, \"json_schema_extra\") and callable(\n            field_info.json_schema_extra\n        ):\n            # Fallback: Check for json_schema_extra function\n            temp_schema = {}\n            field_info.json_schema_extra(temp_schema)\n\n            if \"x-comparison\" in temp_schema:\n                # Copy the comparison metadata from the temp schema to the real schema\n                field_props[\"x-comparison\"] = temp_schema[\"x-comparison\"]\n\n    return schema\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparable_field","title":"<code>stickler.structured_object_evaluator.models.comparable_field</code>","text":"<p>Field module for structured model evaluation.</p> <p>This module provides the ComparableField function for creating fields in structured models with comparison configuration parameters.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparable_field.ComparableField","title":"<code>stickler.structured_object_evaluator.models.comparable_field.ComparableField(comparator=None, threshold=0.5, weight=1.0, default=None, aggregate=False, clip_under_threshold=True, alias=None, description=None, examples=None, **field_kwargs)</code>","text":"<p>Create a Pydantic Field with comparison metadata.</p> <p>This function creates a proper Pydantic Field with embedded comparison configuration, enabling both comparison functionality and native Pydantic features like aliases.</p> <p>Parameters:</p> Name Type Description Default <code>comparator</code> <code>Optional[BaseComparator]</code> <p>Comparator to use for field comparison (default: LevenshteinComparator)</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity score to consider a match (default: 0.5)</p> <code>0.5</code> <code>weight</code> <code>float</code> <p>Weight of this field in overall score calculation (default: 1.0)</p> <code>1.0</code> <code>default</code> <code>Any</code> <p>Default value for the field (default: None)</p> <code>None</code> <code>aggregate</code> <code>bool</code> <p>DEPRECATED - This parameter is deprecated and will be removed in a future version.       Use the new universal 'aggregate' field in compare_with() output instead.</p> <code>False</code> <code>clip_under_threshold</code> <code>bool</code> <p>Whether to zero out scores below threshold (default: True)</p> <code>True</code> <code>alias</code> <code>Optional[str]</code> <p>Pydantic field alias for serialization (default: None)</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Field description for documentation (default: None)</p> <code>None</code> <code>examples</code> <code>Optional[list]</code> <p>Example values for the field (default: None)</p> <code>None</code> <code>**field_kwargs</code> <p>Additional Pydantic Field arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>Pydantic Field with embedded comparison metadata</p> Example <p>class MyModel(StructuredModel):     # Basic usage (no alias):     name: str = ComparableField(threshold=0.8)</p> <pre><code># With alias (new feature):\nemail: str = ComparableField(\n    threshold=0.9,\n    alias=\"email_address\",\n    description=\"User's email\",\n    examples=[\"user@example.com\"]\n)\n</code></pre> Source code in <code>stickler/structured_object_evaluator/models/comparable_field.py</code> <pre><code>def ComparableField(\n    comparator: Optional[BaseComparator] = None,\n    threshold: float = 0.5,\n    weight: float = 1.0,\n    default: Any = None,\n    aggregate: bool = False,\n    clip_under_threshold: bool = True,\n    # Pydantic Field parameters (all optional, just like Field)\n    alias: Optional[str] = None,\n    description: Optional[str] = None,\n    examples: Optional[list] = None,\n    **field_kwargs,\n):\n    \"\"\"Create a Pydantic Field with comparison metadata.\n\n    This function creates a proper Pydantic Field with embedded comparison configuration,\n    enabling both comparison functionality and native Pydantic features like aliases.\n\n    Args:\n        comparator: Comparator to use for field comparison (default: LevenshteinComparator)\n        threshold: Minimum similarity score to consider a match (default: 0.5)\n        weight: Weight of this field in overall score calculation (default: 1.0)\n        default: Default value for the field (default: None)\n        aggregate: DEPRECATED - This parameter is deprecated and will be removed in a future version.\n                  Use the new universal 'aggregate' field in compare_with() output instead.\n        clip_under_threshold: Whether to zero out scores below threshold (default: True)\n        alias: Pydantic field alias for serialization (default: None)\n        description: Field description for documentation (default: None)\n        examples: Example values for the field (default: None)\n        **field_kwargs: Additional Pydantic Field arguments\n\n    Returns:\n        Pydantic Field with embedded comparison metadata\n\n    Example:\n        class MyModel(StructuredModel):\n            # Basic usage (no alias):\n            name: str = ComparableField(threshold=0.8)\n\n            # With alias (new feature):\n            email: str = ComparableField(\n                threshold=0.9,\n                alias=\"email_address\",\n                description=\"User's email\",\n                examples=[\"user@example.com\"]\n            )\n    \"\"\"\n    # Issue deprecation warning if aggregate=True is used\n    if aggregate:\n        warnings.warn(\n            \"The 'aggregate' parameter in ComparableField is deprecated and will be removed \"\n            \"in a future version. All nodes now automatically include an 'aggregate' field \"\n            \"in the compare_with() output that sums primitive field metrics below that node.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    # Create the actual comparator instance\n    actual_comparator = comparator or LevenshteinComparator()\n\n    # Create serializable metadata for JSON schema compatibility\n    serializable_metadata = {\n        \"comparator_type\": actual_comparator.__class__.__name__,\n        \"comparator_name\": getattr(actual_comparator, \"name\", \"unknown\"),\n        \"comparator_config\": getattr(actual_comparator, \"config\", {}),\n        \"threshold\": threshold,\n        \"weight\": weight,\n        \"clip_under_threshold\": clip_under_threshold,\n        \"aggregate\": aggregate,\n    }\n\n    # Create json_schema_extra function that stores runtime data\n    def json_schema_extra_func(schema: Dict[str, Any]) -&gt; None:\n        schema[\"x-comparison\"] = serializable_metadata\n\n    # HYBRID APPROACH: Store runtime instances as function attributes\n    # This works around FieldInfo's __slots__ restriction\n    json_schema_extra_func._comparator_instance = actual_comparator\n    json_schema_extra_func._threshold = threshold\n    json_schema_extra_func._weight = weight\n    json_schema_extra_func._clip_under_threshold = clip_under_threshold\n    json_schema_extra_func._aggregate = aggregate\n    json_schema_extra_func._comparison_metadata = serializable_metadata\n\n    # Merge with existing json_schema_extra if provided\n    existing_json_schema_extra = field_kwargs.get(\"json_schema_extra\", {})\n    if callable(existing_json_schema_extra):\n\n        def enhanced_json_schema_extra(schema: Dict[str, Any]) -&gt; None:\n            existing_json_schema_extra(schema)\n            json_schema_extra_func(schema)\n\n        # Copy our runtime data to the enhanced function\n        enhanced_json_schema_extra._comparator_instance = actual_comparator\n        enhanced_json_schema_extra._threshold = threshold\n        enhanced_json_schema_extra._weight = weight\n        enhanced_json_schema_extra._clip_under_threshold = clip_under_threshold\n        enhanced_json_schema_extra._aggregate = aggregate\n        enhanced_json_schema_extra._comparison_metadata = serializable_metadata\n        final_json_schema_extra = enhanced_json_schema_extra\n    elif isinstance(existing_json_schema_extra, dict):\n\n        def enhanced_json_schema_extra(schema: Dict[str, Any]) -&gt; None:\n            schema.update(existing_json_schema_extra)\n            json_schema_extra_func(schema)\n\n        # Copy our runtime data to the enhanced function\n        enhanced_json_schema_extra._comparator_instance = actual_comparator\n        enhanced_json_schema_extra._threshold = threshold\n        enhanced_json_schema_extra._weight = weight\n        enhanced_json_schema_extra._clip_under_threshold = clip_under_threshold\n        enhanced_json_schema_extra._aggregate = aggregate\n        enhanced_json_schema_extra._comparison_metadata = serializable_metadata\n        final_json_schema_extra = enhanced_json_schema_extra\n    else:\n        final_json_schema_extra = json_schema_extra_func\n\n    # Remove json_schema_extra from field_kwargs to avoid duplication\n    clean_field_kwargs = {\n        k: v for k, v in field_kwargs.items() if k != \"json_schema_extra\"\n    }\n\n    # Create the Field\n    field = Field(\n        default=default,\n        alias=alias,\n        description=description,\n        examples=examples,\n        json_schema_extra=final_json_schema_extra,\n        **clean_field_kwargs,\n    )\n\n    return field\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field","title":"<code>stickler.structured_object_evaluator.models.non_match_field</code>","text":"<p>Models for documenting non-matches in structured object evaluation.</p> <p>This module provides data models for documenting and tracking non-matches (false positives, false negatives, etc.) during structured object evaluation. It also includes utilities for filtering, exporting, and analyzing non-matches.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField","title":"<code>stickler.structured_object_evaluator.models.non_match_field.NonMatchField</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for documenting non-matches in structured object evaluation.</p> <p>This class stores detailed information about each non-match detected during the evaluation process, enabling more thorough analysis and debugging of evaluation results.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>class NonMatchField(BaseModel):\n    \"\"\"Model for documenting non-matches in structured object evaluation.\n\n    This class stores detailed information about each non-match detected\n    during the evaluation process, enabling more thorough analysis and\n    debugging of evaluation results.\n    \"\"\"\n\n    field_path: str = Field(\n        description=\"Dot-notation path to the field (e.g., 'address.city')\"\n    )\n    non_match_type: NonMatchType = Field(description=\"Type of non-match\")\n    ground_truth_value: Any = Field(description=\"Original ground truth value\")\n    prediction_value: Any = Field(description=\"Predicted value\")\n    similarity_score: Optional[float] = Field(\n        default=None, description=\"Similarity score if available\"\n    )\n    details: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Additional context or details\"\n    )\n    document_id: Optional[str] = Field(\n        default=None, description=\"ID of the document this non-match belongs to\"\n    )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the non-match document.\"\"\"\n        similarity_str = (\n            f\", similarity: {self.similarity_score:.4f}\"\n            if self.similarity_score is not None\n            else \"\"\n        )\n        doc_id_str = f\" (doc: {self.document_id})\" if self.document_id else \"\"\n        return (\n            f\"{self.non_match_type.value.upper()} at '{self.field_path}'{similarity_str}{doc_id_str}\\n\"\n            f\"  GT: {self.ground_truth_value}\\n\"\n            f\"  Pred: {self.prediction_value}\"\n        )\n\n    @staticmethod\n    def filter_by_type(\n        documents: List[\"NonMatchField\"], match_type: NonMatchType\n    ) -&gt; List[\"NonMatchField\"]:\n        \"\"\"\n        Filter non-match documents by their type.\n\n        Args:\n            documents: List of NonMatchField instances to filter\n            match_type: Type of non-match to filter for\n\n        Returns:\n            Filtered list of NonMatchField instances\n        \"\"\"\n        return [doc for doc in documents if doc.non_match_type == match_type]\n\n    @staticmethod\n    def export_to_dict(\n        documents: List[\"NonMatchField\"],\n    ) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Export a list of non-match documents to a dictionary for serialization.\n\n        Args:\n            documents: List of NonMatchField instances\n\n        Returns:\n            Dictionary with categorized non-matches\n        \"\"\"\n        result = {\"false_alarms\": [], \"false_discoveries\": [], \"false_negatives\": []}\n\n        for doc in documents:\n            # Create a simplified entry\n            entry = {\n                \"field_path\": doc.field_path,\n                \"ground_truth\": str(doc.ground_truth_value),\n                \"prediction\": str(doc.prediction_value),\n            }\n\n            if doc.similarity_score is not None:\n                entry[\"similarity_score\"] = doc.similarity_score\n\n            if doc.details:\n                entry[\"details\"] = doc.details\n\n            if doc.non_match_type == NonMatchType.FALSE_ALARM:\n                result[\"false_alarms\"].append(entry)\n            elif doc.non_match_type == NonMatchType.FALSE_DISCOVERY:\n                result[\"false_discoveries\"].append(entry)\n            elif doc.non_match_type == NonMatchType.FALSE_NEGATIVE:\n                result[\"false_negatives\"].append(entry)\n\n        return result\n\n    @staticmethod\n    def export_to_json(documents: List[\"NonMatchField\"], output_path: str):\n        \"\"\"\n        Export a list of non-match documents to a JSON file.\n\n        Args:\n            documents: List of NonMatchField instances\n            output_path: Path to save the JSON file\n        \"\"\"\n        # Create parent directories if needed\n        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n        # Export as dictionary\n        data = NonMatchField.export_to_dict(documents)\n\n        # Write to file\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2)\n\n    @staticmethod\n    def print_summary(documents: List[\"NonMatchField\"], detailed: bool = False):\n        \"\"\"\n        Print a summary of non-match documents.\n\n        Args:\n            documents: List of NonMatchField instances\n            detailed: Whether to print detailed information for each document\n        \"\"\"\n        # Count by type\n        false_alarms = NonMatchField.filter_by_type(documents, NonMatchType.FALSE_ALARM)\n        false_discoveries = NonMatchField.filter_by_type(\n            documents, NonMatchType.FALSE_DISCOVERY\n        )\n        false_negatives = NonMatchField.filter_by_type(\n            documents, NonMatchType.FALSE_NEGATIVE\n        )\n\n        # Print summary counts\n        print(f\"Non-matches summary:\")\n        print(f\"- False Alarms: {len(false_alarms)}\")\n        print(f\"- False Discoveries: {len(false_discoveries)}\")\n        print(f\"- False Negatives: {len(false_negatives)}\")\n\n        # Print details if requested\n        if detailed and documents:\n            print(\"\\nDetailed non-matches:\")\n            for i, doc in enumerate(documents):\n                print(f\"\\nNon-match #{i + 1}:\")\n                print(f\"- Type: {doc.non_match_type}\")\n                print(f\"- Field: {doc.field_path}\")\n                print(f\"- Ground truth: {doc.ground_truth_value}\")\n                print(f\"- Prediction: {doc.prediction_value}\")\n                if doc.similarity_score is not None:\n                    print(f\"- Similarity: {doc.similarity_score:.4f}\")\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the non-match document.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the non-match document.\"\"\"\n    similarity_str = (\n        f\", similarity: {self.similarity_score:.4f}\"\n        if self.similarity_score is not None\n        else \"\"\n    )\n    doc_id_str = f\" (doc: {self.document_id})\" if self.document_id else \"\"\n    return (\n        f\"{self.non_match_type.value.upper()} at '{self.field_path}'{similarity_str}{doc_id_str}\\n\"\n        f\"  GT: {self.ground_truth_value}\\n\"\n        f\"  Pred: {self.prediction_value}\"\n    )\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.export_to_dict","title":"<code>export_to_dict(documents)</code>  <code>staticmethod</code>","text":"<p>Export a list of non-match documents to a dictionary for serialization.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <p>Returns:</p> Type Description <code>Dict[str, List[Dict[str, Any]]]</code> <p>Dictionary with categorized non-matches</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef export_to_dict(\n    documents: List[\"NonMatchField\"],\n) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"\n    Export a list of non-match documents to a dictionary for serialization.\n\n    Args:\n        documents: List of NonMatchField instances\n\n    Returns:\n        Dictionary with categorized non-matches\n    \"\"\"\n    result = {\"false_alarms\": [], \"false_discoveries\": [], \"false_negatives\": []}\n\n    for doc in documents:\n        # Create a simplified entry\n        entry = {\n            \"field_path\": doc.field_path,\n            \"ground_truth\": str(doc.ground_truth_value),\n            \"prediction\": str(doc.prediction_value),\n        }\n\n        if doc.similarity_score is not None:\n            entry[\"similarity_score\"] = doc.similarity_score\n\n        if doc.details:\n            entry[\"details\"] = doc.details\n\n        if doc.non_match_type == NonMatchType.FALSE_ALARM:\n            result[\"false_alarms\"].append(entry)\n        elif doc.non_match_type == NonMatchType.FALSE_DISCOVERY:\n            result[\"false_discoveries\"].append(entry)\n        elif doc.non_match_type == NonMatchType.FALSE_NEGATIVE:\n            result[\"false_negatives\"].append(entry)\n\n    return result\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.export_to_json","title":"<code>export_to_json(documents, output_path)</code>  <code>staticmethod</code>","text":"<p>Export a list of non-match documents to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <code>output_path</code> <code>str</code> <p>Path to save the JSON file</p> required Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef export_to_json(documents: List[\"NonMatchField\"], output_path: str):\n    \"\"\"\n    Export a list of non-match documents to a JSON file.\n\n    Args:\n        documents: List of NonMatchField instances\n        output_path: Path to save the JSON file\n    \"\"\"\n    # Create parent directories if needed\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # Export as dictionary\n    data = NonMatchField.export_to_dict(documents)\n\n    # Write to file\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.filter_by_type","title":"<code>filter_by_type(documents, match_type)</code>  <code>staticmethod</code>","text":"<p>Filter non-match documents by their type.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances to filter</p> required <code>match_type</code> <code>NonMatchType</code> <p>Type of non-match to filter for</p> required <p>Returns:</p> Type Description <code>List[NonMatchField]</code> <p>Filtered list of NonMatchField instances</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef filter_by_type(\n    documents: List[\"NonMatchField\"], match_type: NonMatchType\n) -&gt; List[\"NonMatchField\"]:\n    \"\"\"\n    Filter non-match documents by their type.\n\n    Args:\n        documents: List of NonMatchField instances to filter\n        match_type: Type of non-match to filter for\n\n    Returns:\n        Filtered list of NonMatchField instances\n    \"\"\"\n    return [doc for doc in documents if doc.non_match_type == match_type]\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchField.print_summary","title":"<code>print_summary(documents, detailed=False)</code>  <code>staticmethod</code>","text":"<p>Print a summary of non-match documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[NonMatchField]</code> <p>List of NonMatchField instances</p> required <code>detailed</code> <code>bool</code> <p>Whether to print detailed information for each document</p> <code>False</code> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>@staticmethod\ndef print_summary(documents: List[\"NonMatchField\"], detailed: bool = False):\n    \"\"\"\n    Print a summary of non-match documents.\n\n    Args:\n        documents: List of NonMatchField instances\n        detailed: Whether to print detailed information for each document\n    \"\"\"\n    # Count by type\n    false_alarms = NonMatchField.filter_by_type(documents, NonMatchType.FALSE_ALARM)\n    false_discoveries = NonMatchField.filter_by_type(\n        documents, NonMatchType.FALSE_DISCOVERY\n    )\n    false_negatives = NonMatchField.filter_by_type(\n        documents, NonMatchType.FALSE_NEGATIVE\n    )\n\n    # Print summary counts\n    print(f\"Non-matches summary:\")\n    print(f\"- False Alarms: {len(false_alarms)}\")\n    print(f\"- False Discoveries: {len(false_discoveries)}\")\n    print(f\"- False Negatives: {len(false_negatives)}\")\n\n    # Print details if requested\n    if detailed and documents:\n        print(\"\\nDetailed non-matches:\")\n        for i, doc in enumerate(documents):\n            print(f\"\\nNon-match #{i + 1}:\")\n            print(f\"- Type: {doc.non_match_type}\")\n            print(f\"- Field: {doc.field_path}\")\n            print(f\"- Ground truth: {doc.ground_truth_value}\")\n            print(f\"- Prediction: {doc.prediction_value}\")\n            if doc.similarity_score is not None:\n                print(f\"- Similarity: {doc.similarity_score:.4f}\")\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.non_match_field.NonMatchType","title":"<code>stickler.structured_object_evaluator.models.non_match_field.NonMatchType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum defining the types of non-matches.</p> Source code in <code>stickler/structured_object_evaluator/models/non_match_field.py</code> <pre><code>class NonMatchType(str, Enum):\n    \"\"\"Enum defining the types of non-matches.\"\"\"\n\n    FALSE_ALARM = \"false_alarm\"  # GT null, prediction non-null\n    FALSE_DISCOVERY = \"false_discovery\"  # Both non-null but don't match\n    FALSE_NEGATIVE = \"false_negative\"  # GT non-null, prediction null\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.field","title":"<code>stickler.structured_object_evaluator.models.field</code>","text":"<p>Field module for structured model evaluation.</p> <p>This module contains the ComparableField class used to define fields in structured models with comparison configuration parameters.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.field.CustomField","title":"<code>stickler.structured_object_evaluator.models.field.CustomField</code>","text":"<p>               Bases: <code>FieldInfo</code></p> <p>Field with comparable properties for structured model evaluation.</p> <p>This extends pydantic's Field with additional attributes that control how the field is compared during evaluation.</p> <p>Attributes:</p> Name Type Description <code>comparator</code> <p>The comparator to use for this field</p> <code>threshold</code> <p>The threshold for determining if values match</p> <code>weight</code> <p>The weight of this field in the overall score</p> <code>description</code> <p>Human-readable description of the field</p> Source code in <code>stickler/structured_object_evaluator/models/field.py</code> <pre><code>class CustomField(FieldInfo):\n    \"\"\"\n    Field with comparable properties for structured model evaluation.\n\n    This extends pydantic's Field with additional attributes that control how the field\n    is compared during evaluation.\n\n    Attributes:\n        comparator: The comparator to use for this field\n        threshold: The threshold for determining if values match\n        weight: The weight of this field in the overall score\n        description: Human-readable description of the field\n    \"\"\"\n\n    def __init__(\n        self,\n        default: Any = ...,\n        *,\n        comparator: Optional[BaseComparator] = None,\n        threshold: float = DEFAULT_THRESHOLD,\n        weight: float = DEFAULT_WEIGHT,\n        description: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize comparable field.\n\n        Args:\n            default: Default value for the field\n            comparator: The comparator to use for this field\n            threshold: The threshold for determining if values match\n            weight: The weight of this field in the overall score\n            description: Human-readable description of the field\n            **kwargs: Additional field parameters\n        \"\"\"\n        # Fix: Pass all kwargs together with default as keyword args,\n        # since pydantic expects a specific format\n        kwargs[\"default\"] = default\n        super().__init__(**kwargs)\n\n        # Store comparison configuration\n        self.comparator = comparator or LevenshteinComparator()\n        self.threshold = threshold\n        self.weight = weight\n        self.description = description\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get field configuration.\n\n        Returns:\n            Dictionary with field configuration\n        \"\"\"\n        return {\n            \"comparator\": self.comparator,\n            \"threshold\": self.threshold,\n            \"weight\": self.weight,\n            \"description\": self.description,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation.\n\n        Returns:\n            String representation\n        \"\"\"\n        return (\n            f\"ComparableField(\"\n            f\"comparator={self.comparator}, \"\n            f\"threshold={self.threshold}, \"\n            f\"weight={self.weight})\"\n        )\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.field.CustomField.__init__","title":"<code>__init__(default=..., *, comparator=None, threshold=DEFAULT_THRESHOLD, weight=DEFAULT_WEIGHT, description=None, **kwargs)</code>","text":"<p>Initialize comparable field.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>Default value for the field</p> <code>...</code> <code>comparator</code> <code>Optional[BaseComparator]</code> <p>The comparator to use for this field</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The threshold for determining if values match</p> <code>DEFAULT_THRESHOLD</code> <code>weight</code> <code>float</code> <p>The weight of this field in the overall score</p> <code>DEFAULT_WEIGHT</code> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of the field</p> <code>None</code> <code>**kwargs</code> <p>Additional field parameters</p> <code>{}</code> Source code in <code>stickler/structured_object_evaluator/models/field.py</code> <pre><code>def __init__(\n    self,\n    default: Any = ...,\n    *,\n    comparator: Optional[BaseComparator] = None,\n    threshold: float = DEFAULT_THRESHOLD,\n    weight: float = DEFAULT_WEIGHT,\n    description: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize comparable field.\n\n    Args:\n        default: Default value for the field\n        comparator: The comparator to use for this field\n        threshold: The threshold for determining if values match\n        weight: The weight of this field in the overall score\n        description: Human-readable description of the field\n        **kwargs: Additional field parameters\n    \"\"\"\n    # Fix: Pass all kwargs together with default as keyword args,\n    # since pydantic expects a specific format\n    kwargs[\"default\"] = default\n    super().__init__(**kwargs)\n\n    # Store comparison configuration\n    self.comparator = comparator or LevenshteinComparator()\n    self.threshold = threshold\n    self.weight = weight\n    self.description = description\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.field.CustomField.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation</p> Source code in <code>stickler/structured_object_evaluator/models/field.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation.\n\n    Returns:\n        String representation\n    \"\"\"\n    return (\n        f\"ComparableField(\"\n        f\"comparator={self.comparator}, \"\n        f\"threshold={self.threshold}, \"\n        f\"weight={self.weight})\"\n    )\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.field.CustomField.get_config","title":"<code>get_config()</code>","text":"<p>Get field configuration.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with field configuration</p> Source code in <code>stickler/structured_object_evaluator/models/field.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get field configuration.\n\n    Returns:\n        Dictionary with field configuration\n    \"\"\"\n    return {\n        \"comparator\": self.comparator,\n        \"threshold\": self.threshold,\n        \"weight\": self.weight,\n        \"description\": self.description,\n    }\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info","title":"<code>stickler.structured_object_evaluator.models.comparison_info</code>","text":"<p>Comparison configuration for structured model fields.</p>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo","title":"<code>stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo</code>","text":"<p>Container for comparison configuration.</p> <p>This class holds the configuration for how a field should be compared, including which comparator to use, the threshold for considering a match, and the weight in scoring.</p> <p>Attributes:</p> Name Type Description <code>comparator</code> <p>The comparator to use for string similarity</p> <code>threshold</code> <p>Minimum score to consider a match (like ANLS threshold)</p> <code>weight</code> <p>Weight of this field in the overall score calculation</p> Source code in <code>stickler/structured_object_evaluator/models/comparison_info.py</code> <pre><code>class ComparisonInfo:\n    \"\"\"Container for comparison configuration.\n\n    This class holds the configuration for how a field should be compared,\n    including which comparator to use, the threshold for considering a match,\n    and the weight in scoring.\n\n    Attributes:\n        comparator: The comparator to use for string similarity\n        threshold: Minimum score to consider a match (like ANLS threshold)\n        weight: Weight of this field in the overall score calculation\n    \"\"\"\n\n    def __init__(\n        self,\n        comparator: Optional[BaseComparator] = None,\n        threshold: float = 0.5,\n        weight: float = 1.0,\n    ):\n        \"\"\"Initialize comparison configuration.\n\n        Args:\n            comparator: Comparator to use (default: LevenshteinComparator)\n            threshold: Minimum similarity score to consider a match (default: 0.5)\n            weight: Weight of this field in the overall score (default: 1.0)\n        \"\"\"\n        self.comparator = comparator or LevenshteinComparator()\n        self.threshold = threshold\n        self.weight = weight\n\n    def compare(self, value1: Any, value2: Any) -&gt; float:\n        \"\"\"Compare two values and return a similarity score between 0 and 1.\n\n        Args:\n            value1: First value to compare\n            value2: Second value to compare\n\n        Returns:\n            Similarity score between 0.0 and 1.0, with 0.0 if below threshold\n        \"\"\"\n        # Handle None values\n        if value1 is None or value2 is None:\n            return 1.0 if value1 == value2 else 0.0\n\n        # Use the comparator to calculate similarity\n        similarity = self.comparator.compare(value1, value2)\n\n        # Apply threshold (if below threshold, return 0)\n        return 0.0 if similarity &lt; self.threshold else similarity\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation.\"\"\"\n        return f\"ComparisonInfo(comparator={self.comparator}, threshold={self.threshold}, weight={self.weight})\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to a serializable dictionary for JSON schema.\"\"\"\n        return {\n            \"comparator_type\": self.comparator.__class__.__name__,\n            \"comparator_name\": getattr(self.comparator, \"name\", \"unknown\"),\n            \"comparator_config\": getattr(self.comparator, \"config\", {}),\n            \"threshold\": self.threshold,\n            \"weight\": self.weight,\n        }\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo.__init__","title":"<code>__init__(comparator=None, threshold=0.5, weight=1.0)</code>","text":"<p>Initialize comparison configuration.</p> <p>Parameters:</p> Name Type Description Default <code>comparator</code> <code>Optional[BaseComparator]</code> <p>Comparator to use (default: LevenshteinComparator)</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity score to consider a match (default: 0.5)</p> <code>0.5</code> <code>weight</code> <code>float</code> <p>Weight of this field in the overall score (default: 1.0)</p> <code>1.0</code> Source code in <code>stickler/structured_object_evaluator/models/comparison_info.py</code> <pre><code>def __init__(\n    self,\n    comparator: Optional[BaseComparator] = None,\n    threshold: float = 0.5,\n    weight: float = 1.0,\n):\n    \"\"\"Initialize comparison configuration.\n\n    Args:\n        comparator: Comparator to use (default: LevenshteinComparator)\n        threshold: Minimum similarity score to consider a match (default: 0.5)\n        weight: Weight of this field in the overall score (default: 1.0)\n    \"\"\"\n    self.comparator = comparator or LevenshteinComparator()\n    self.threshold = threshold\n    self.weight = weight\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation.</p> Source code in <code>stickler/structured_object_evaluator/models/comparison_info.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return f\"ComparisonInfo(comparator={self.comparator}, threshold={self.threshold}, weight={self.weight})\"\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo.compare","title":"<code>compare(value1, value2)</code>","text":"<p>Compare two values and return a similarity score between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>value1</code> <code>Any</code> <p>First value to compare</p> required <code>value2</code> <code>Any</code> <p>Second value to compare</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0, with 0.0 if below threshold</p> Source code in <code>stickler/structured_object_evaluator/models/comparison_info.py</code> <pre><code>def compare(self, value1: Any, value2: Any) -&gt; float:\n    \"\"\"Compare two values and return a similarity score between 0 and 1.\n\n    Args:\n        value1: First value to compare\n        value2: Second value to compare\n\n    Returns:\n        Similarity score between 0.0 and 1.0, with 0.0 if below threshold\n    \"\"\"\n    # Handle None values\n    if value1 is None or value2 is None:\n        return 1.0 if value1 == value2 else 0.0\n\n    # Use the comparator to calculate similarity\n    similarity = self.comparator.compare(value1, value2)\n\n    # Apply threshold (if below threshold, return 0)\n    return 0.0 if similarity &lt; self.threshold else similarity\n</code></pre>"},{"location":"SDK-Docs/models/#stickler.structured_object_evaluator.models.comparison_info.ComparisonInfo.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to a serializable dictionary for JSON schema.</p> Source code in <code>stickler/structured_object_evaluator/models/comparison_info.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to a serializable dictionary for JSON schema.\"\"\"\n    return {\n        \"comparator_type\": self.comparator.__class__.__name__,\n        \"comparator_name\": getattr(self.comparator, \"name\", \"unknown\"),\n        \"comparator_config\": getattr(self.comparator, \"config\", {}),\n        \"threshold\": self.threshold,\n        \"weight\": self.weight,\n    }\n</code></pre>"},{"location":"SDK-Docs/utils/","title":"Utils","text":""},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils","title":"<code>stickler.structured_object_evaluator.utils</code>","text":"<p>Utility functions for structured object evaluation.</p>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.anls_score","title":"<code>stickler.structured_object_evaluator.utils.anls_score</code>","text":"<p>ANLS score calculation for structured objects.</p>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.anls_score.compare_structured_models","title":"<code>stickler.structured_object_evaluator.utils.anls_score.compare_structured_models(gt, pred)</code>","text":"<p>Compare a ground truth model with a prediction.</p> <p>This function wraps the compare_with method of StructuredModel for a more explicit API.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>StructuredModel</code> <p>Ground truth model</p> required <code>pred</code> <code>StructuredModel</code> <p>Prediction model</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Comparison result dictionary</p> Source code in <code>stickler/structured_object_evaluator/utils/anls_score.py</code> <pre><code>def compare_structured_models(\n    gt: StructuredModel, pred: StructuredModel\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare a ground truth model with a prediction.\n\n    This function wraps the compare_with method of StructuredModel for\n    a more explicit API.\n\n    Args:\n        gt: Ground truth model\n        pred: Prediction model\n\n    Returns:\n        Comparison result dictionary\n    \"\"\"\n    return gt.compare_with(pred)\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.anls_score.anls_score","title":"<code>stickler.structured_object_evaluator.utils.anls_score.anls_score(gt, pred, return_gt=False, return_key_scores=False)</code>","text":"<p>Calculate ANLS* score between two objects.</p> <p>This function provides a simple API for getting an ANLS* score between any two objects, similar to the original anls_score function.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>Any</code> <p>Ground truth object</p> required <code>pred</code> <code>Any</code> <p>Prediction object</p> required <code>return_gt</code> <code>bool</code> <p>Whether to return the closest ground truth</p> <code>False</code> <code>return_key_scores</code> <code>bool</code> <p>Whether to return detailed key scores</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>Either just the overall score (float), or a tuple with the score and</p> <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>closest ground truth, or a tuple with the score, closest ground truth,</p> <code>Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]</code> <p>and key scores.</p> Source code in <code>stickler/structured_object_evaluator/utils/anls_score.py</code> <pre><code>def anls_score(\n    gt: Any, pred: Any, return_gt: bool = False, return_key_scores: bool = False\n) -&gt; Union[float, Tuple[float, Any], Tuple[float, Any, Dict[str, Any]]]:\n    \"\"\"Calculate ANLS* score between two objects.\n\n    This function provides a simple API for getting an ANLS* score\n    between any two objects, similar to the original anls_score function.\n\n    Args:\n        gt: Ground truth object\n        pred: Prediction object\n        return_gt: Whether to return the closest ground truth\n        return_key_scores: Whether to return detailed key scores\n\n    Returns:\n        Either just the overall score (float), or a tuple with the score and\n        closest ground truth, or a tuple with the score, closest ground truth,\n        and key scores.\n    \"\"\"\n    import warnings\n    from ..trees.base import ANLSTree\n\n    # Store original gt object for possible return\n    original_gt = gt\n\n    # Handle classical QA dataset compatibility\n    gt_is_list_str = isinstance(gt, list) and all(isinstance(x, str) for x in gt)\n    pred_is_str = isinstance(pred, str)\n    if gt_is_list_str and pred_is_str:\n        warnings.warn(\n            \"Treating ground truth as a list of options. This is a compatibility mode for ST-VQA-like datasets.\"\n        )\n        gt = tuple(gt)\n\n    # Create trees from the objects\n    gt_tree = ANLSTree.make_tree(gt, is_gt=True)\n    pred_tree = ANLSTree.make_tree(pred, is_gt=False)\n\n    # Calculate ANLS score\n    score, closest_gt, key_scores = gt_tree.anls(pred_tree)\n\n    # Determine what to return for gt (smart detection)\n    gt_to_return = original_gt if hasattr(original_gt, \"model_dump\") else closest_gt\n\n    # Return the requested information\n    if return_gt and return_key_scores:\n        from .key_scores import construct_nested_dict\n\n        key_scores_dict = construct_nested_dict(key_scores)\n        return score, gt_to_return, key_scores_dict\n    elif return_gt:\n        return score, gt_to_return\n    elif return_key_scores:\n        from .key_scores import construct_nested_dict\n\n        key_scores_dict = construct_nested_dict(key_scores)\n        return score, key_scores_dict\n    else:\n        return score\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.compare_json","title":"<code>stickler.structured_object_evaluator.utils.compare_json</code>","text":""},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.compare_json.compare_json","title":"<code>stickler.structured_object_evaluator.utils.compare_json.compare_json(gt_json, pred_json, model_cls)</code>","text":"<p>Compare JSON objects using a StructuredModel.</p> <p>This function is a utility for comparing raw JSON objects using a StructuredModel class. It handles missing fields and extra fields gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>gt_json</code> <code>Dict[str, Any]</code> <p>Ground truth JSON</p> required <code>pred_json</code> <code>Dict[str, Any]</code> <p>Prediction JSON</p> required <code>model_cls</code> <code>Type[StructuredModel]</code> <p>StructuredModel class to use for comparison</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with comparison results</p> Source code in <code>stickler/structured_object_evaluator/utils/compare_json.py</code> <pre><code>def compare_json(\n    gt_json: Dict[str, Any], pred_json: Dict[str, Any], model_cls: Type[StructuredModel]\n) -&gt; Dict[str, Any]:\n    \"\"\"Compare JSON objects using a StructuredModel.\n\n    This function is a utility for comparing raw JSON objects using a\n    StructuredModel class. It handles missing fields and extra fields gracefully.\n\n    Args:\n        gt_json: Ground truth JSON\n        pred_json: Prediction JSON\n        model_cls: StructuredModel class to use for comparison\n\n    Returns:\n        Dictionary with comparison results\n    \"\"\"\n    try:\n        # Try to convert both JSONs to structured models\n        gt_model = model_cls.from_json(gt_json)\n        pred_model = model_cls.from_json(pred_json)\n\n        # Compare the models\n        return gt_model.compare_with(pred_model)\n    except Exception as e:\n        # Return error details if conversion fails\n        return {\n            \"error\": str(e),\n            \"overall_score\": 0.0,\n            \"field_scores\": {},\n            \"all_fields_matched\": False,\n        }\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.key_scores","title":"<code>stickler.structured_object_evaluator.utils.key_scores</code>","text":"<p>Utility functions for handling key scores in structured object evaluation.</p>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.key_scores.ScoreNode","title":"<code>stickler.structured_object_evaluator.utils.key_scores.ScoreNode</code>  <code>dataclass</code>","text":"<p>Node in a score tree representing scores for hierarchical structures.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of this node (key in the hierarchy).</p> <code>score</code> <code>Optional[float]</code> <p>The score for this node, or None if this is an intermediate node.</p> <code>children</code> <code>Dict[str, Any]</code> <p>A dictionary mapping child keys to their ScoreNode objects.</p> Source code in <code>stickler/structured_object_evaluator/utils/key_scores.py</code> <pre><code>@dataclass\nclass ScoreNode:\n    \"\"\"Node in a score tree representing scores for hierarchical structures.\n\n    Attributes:\n        name: The name of this node (key in the hierarchy).\n        score: The score for this node, or None if this is an intermediate node.\n        children: A dictionary mapping child keys to their ScoreNode objects.\n    \"\"\"\n\n    name: str = \"\"\n    score: Optional[float] = None\n    children: Dict[str, Any] = field(default_factory=dict)\n\n    # For backward compatibility\n    @property\n    def anls_score(self):\n        \"\"\"Alias for score to maintain backward compatibility.\"\"\"\n        return self.score\n\n    @anls_score.setter\n    def anls_score(self, value):\n        \"\"\"Setter for anls_score that updates the score attribute.\"\"\"\n        self.score = value\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.key_scores.ScoreNode.anls_score","title":"<code>anls_score</code>  <code>property</code> <code>writable</code>","text":"<p>Alias for score to maintain backward compatibility.</p>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.key_scores.construct_nested_dict","title":"<code>stickler.structured_object_evaluator.utils.key_scores.construct_nested_dict(list_of_dicts)</code>","text":"<p>Construct a nested dictionary from a list of dictionaries with nested keys.</p> <p>This function transforms a flat list of dictionaries with tuple keys into a hierarchical structure of ScoreNode objects. This is useful for representing and analyzing scores for nested data structures like dictionaries and lists.</p> <p>Note: If there are duplicates of keys in the list of dictionaries, the last value will be used.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_dicts</code> <code>List[Dict[Tuple[str, ...], float]]</code> <p>A list of dictionaries with nested keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, ScoreNode]</code> <p>A nested dictionary of ScoreNode objects.</p> Example <p>list_of_dicts = [         {(\"a\",): 3},         {(\"a\", \"b\", \"c\"): 1},         {(\"a\", \"b\", \"d\"): 2},         {(\"a\", \"c\", \"e\"): 3},     ], construct_nested_dict(list_of_dicts)     {         \"a\": ScoreNode(             anls_score=3,             children={                 \"b\": ScoreNode(                     children={                         \"c\": ScoreNode(anls_score=1),                         \"d\": ScoreNode(anls_score=2),                     }                 ),                 \"c\": ScoreNode(children={\"e\": ScoreNode(anls_score=3)}),             },         )     },</p> Source code in <code>stickler/structured_object_evaluator/utils/key_scores.py</code> <pre><code>def construct_nested_dict(\n    list_of_dicts: List[Dict[Tuple[str, ...], float]],\n) -&gt; Dict[str, ScoreNode]:\n    \"\"\"Construct a nested dictionary from a list of dictionaries with nested keys.\n\n    This function transforms a flat list of dictionaries with tuple keys into a\n    hierarchical structure of ScoreNode objects. This is useful for representing\n    and analyzing scores for nested data structures like dictionaries and lists.\n\n    Note: If there are duplicates of keys in the list of dictionaries, the last value will be used.\n\n    Args:\n        list_of_dicts: A list of dictionaries with nested keys.\n\n    Returns:\n        A nested dictionary of ScoreNode objects.\n\n    Example:\n        &gt;&gt;&gt; list_of_dicts = [\n                {(\"a\",): 3},\n                {(\"a\", \"b\", \"c\"): 1},\n                {(\"a\", \"b\", \"d\"): 2},\n                {(\"a\", \"c\", \"e\"): 3},\n            ],\n        &gt;&gt;&gt; construct_nested_dict(list_of_dicts)\n            {\n                \"a\": ScoreNode(\n                    anls_score=3,\n                    children={\n                        \"b\": ScoreNode(\n                            children={\n                                \"c\": ScoreNode(anls_score=1),\n                                \"d\": ScoreNode(anls_score=2),\n                            }\n                        ),\n                        \"c\": ScoreNode(children={\"e\": ScoreNode(anls_score=3)}),\n                    },\n                )\n            },\n    \"\"\"\n    nested_dict: Dict[str, ScoreNode] = {}\n\n    if len(list_of_dicts) == 0:\n        return nested_dict\n\n    for entry in list_of_dicts:\n        for key_tuple, value in entry.items():\n            current_dict: Dict[str, ScoreNode] = nested_dict\n            # Traverse and build nested dict, except for last entry\n            for key in key_tuple[:-1]:\n                if key not in current_dict:\n                    current_dict[key] = ScoreNode(name=key)\n                current_dict = current_dict[key].children\n\n            # Set the value for the final key\n            final_key = key_tuple[-1]\n            if final_key not in current_dict:\n                current_dict[final_key] = ScoreNode(name=final_key)\n            current_dict[final_key].score = value\n\n    return nested_dict\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.key_scores.merge_and_calculate_mean","title":"<code>stickler.structured_object_evaluator.utils.key_scores.merge_and_calculate_mean(list_of_dicts)</code>","text":"<p>Merge a list of dictionaries and calculate the mean value for each key.</p> <p>This function takes a list of dictionaries where keys are tuples of strings and values are floats. It combines the dictionaries and calculates the mean value for each unique key across all the dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_dicts</code> <code>List[Dict[Tuple[str, ...], float]]</code> <p>A list of dictionaries with tuple keys and float values.</p> required <p>Returns:</p> Type Description <code>List[Dict[Tuple[str, ...], float]]</code> <p>A list of dictionaries, each containing a single key-value pair where</p> <code>List[Dict[Tuple[str, ...], float]]</code> <p>values are the mean of the original values for the corresponding key.</p> Example <p>list_of_dicts = [         {('a', 'b'): 10.0, ('c', 'd'): 20.0},         {('a', 'b'): 30.0, ('e', 'f'): 40.0}     ] merge_and_calculate_mean(list_of_dicts)     [{('a', 'b'): 20.0}, {('c', 'd'): 20.0}, {('e', 'f'): 40.0}]</p> Source code in <code>stickler/structured_object_evaluator/utils/key_scores.py</code> <pre><code>def merge_and_calculate_mean(\n    list_of_dicts: List[Dict[Tuple[str, ...], float]],\n) -&gt; List[Dict[Tuple[str, ...], float]]:\n    \"\"\"\n    Merge a list of dictionaries and calculate the mean value for each key.\n\n    This function takes a list of dictionaries where keys are tuples of strings and\n    values are floats. It combines the dictionaries and calculates the mean value\n    for each unique key across all the dictionaries.\n\n    Args:\n        list_of_dicts: A list of dictionaries with tuple keys and float values.\n\n    Returns:\n        A list of dictionaries, each containing a single key-value pair where\n        values are the mean of the original values for the corresponding key.\n\n    Example:\n        &gt;&gt;&gt; list_of_dicts = [\n                {('a', 'b'): 10.0, ('c', 'd'): 20.0},\n                {('a', 'b'): 30.0, ('e', 'f'): 40.0}\n            ]\n        &gt;&gt;&gt; merge_and_calculate_mean(list_of_dicts)\n            [{('a', 'b'): 20.0}, {('c', 'd'): 20.0}, {('e', 'f'): 40.0}]\n    \"\"\"\n    combined_scores: Dict[Tuple[str, ...], float] = {}\n    count_dict: Dict[Tuple[str, ...], int] = {}\n\n    # Combine scores for the same keys\n    for d in list_of_dicts:\n        for k, v in d.items():\n            if k not in combined_scores:\n                combined_scores[k] = 0\n                count_dict[k] = 0\n            combined_scores[k] += v\n            count_dict[k] += 1\n\n    # Calculate the mean for each key\n    for k in combined_scores.keys():\n        combined_scores[k] /= count_dict[k]\n\n    # Convert back to a list of dictionaries\n    list_combined_scores = [{k: v} for k, v in combined_scores.items()]\n\n    return list_combined_scores\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.pretty_print","title":"<code>stickler.structured_object_evaluator.utils.pretty_print</code>","text":"<p>Pretty print utilities for StructuredModelEvaluator results.</p> <p>This module provides functions for displaying confusion matrix metrics in a more readable and visually appealing format.</p>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.pretty_print.print_confusion_matrix","title":"<code>stickler.structured_object_evaluator.utils.pretty_print.print_confusion_matrix(results, field_filter=None, sort_by='name', show_details=True, use_color=True, output_file=None, nested_detail='standard')</code>","text":"<p>Pretty print confusion matrix metrics in a readable, visually appealing format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Union[Dict[str, Any], Any]</code> <p>Results from StructuredModelEvaluator.evaluate() or ProcessEvaluation from bulk evaluator</p> required <code>field_filter</code> <code>Optional[str]</code> <p>Optional regex to filter fields to display</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>How to sort fields ('name', 'precision', 'recall', 'f1', etc.)</p> <code>'name'</code> <code>show_details</code> <code>bool</code> <p>Whether to show detailed metrics for each field</p> <code>True</code> <code>use_color</code> <code>bool</code> <p>Whether to use color in the output</p> <code>True</code> <code>output_file</code> <code>Optional[str]</code> <p>Optional file path to write the output to</p> <code>None</code> <code>nested_detail</code> <code>str</code> <p>Level of detail for nested objects:            'minimal' - Show only top-level fields            'standard' - Show nested fields with basic metrics (default)            'detailed' - Show comprehensive metrics for nested fields and their items</p> <code>'standard'</code> Source code in <code>stickler/structured_object_evaluator/utils/pretty_print.py</code> <pre><code>def print_confusion_matrix(\n    results: Union[Dict[str, Any], Any],\n    field_filter: Optional[str] = None,\n    sort_by: str = \"name\",\n    show_details: bool = True,\n    use_color: bool = True,\n    output_file: Optional[str] = None,\n    nested_detail: str = \"standard\",\n) -&gt; None:\n    \"\"\"\n    Pretty print confusion matrix metrics in a readable, visually appealing format.\n\n    Args:\n        results: Results from StructuredModelEvaluator.evaluate() or ProcessEvaluation from bulk evaluator\n        field_filter: Optional regex to filter fields to display\n        sort_by: How to sort fields ('name', 'precision', 'recall', 'f1', etc.)\n        show_details: Whether to show detailed metrics for each field\n        use_color: Whether to use color in the output\n        output_file: Optional file path to write the output to\n        nested_detail: Level of detail for nested objects:\n                       'minimal' - Show only top-level fields\n                       'standard' - Show nested fields with basic metrics (default)\n                       'detailed' - Show comprehensive metrics for nested fields and their items\n    \"\"\"\n    # Normalize results format\n    normalized_results = _normalize_results_format(results)\n    if normalized_results is None:\n        print(\"Error: Results do not contain recognizable confusion matrix metrics\")\n        return\n\n    # Use normalized results for processing\n    results = normalized_results\n\n    # Direct output to file if specified\n    if output_file:\n        try:\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                original_stdout = sys.stdout\n                sys.stdout = f\n                use_color = False  # Disable color for file output\n\n                # Print overall summary\n                _print_overall_summary(results, use_color)\n\n                # Print field-level metrics if requested\n                if show_details:\n                    _print_field_details(\n                        results, field_filter, sort_by, use_color, nested_detail\n                    )\n\n                    # Print matrix visualization\n                    _print_matrix_visualization(results, use_color)\n\n                # Restore stdout before context manager closes\n                sys.stdout = original_stdout\n        except Exception as e:\n            print(f\"Error opening output file: {e}\")\n            return\n    else:\n        # Print to stdout\n        # Print overall summary\n        _print_overall_summary(results, use_color)\n\n        # Print bulk evaluation info if available\n        if \"bulk_info\" in results:\n            _print_bulk_info_summary(results, use_color)\n\n        # Print field-level metrics if requested\n        if show_details:\n            _print_field_details(\n                results, field_filter, sort_by, use_color, nested_detail\n            )\n\n            # Print matrix visualization\n            _print_matrix_visualization(results, use_color)\n</code></pre>"},{"location":"SDK-Docs/utils/#stickler.structured_object_evaluator.utils.pretty_print.print_confusion_matrix_html","title":"<code>stickler.structured_object_evaluator.utils.pretty_print.print_confusion_matrix_html(results, field_filter=None, nested_detail='standard')</code>","text":"<p>Generate HTML representation of confusion matrix metrics for Jupyter notebooks.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, Any]</code> <p>Results dictionary from StructuredModelEvaluator.evaluate()</p> required <code>field_filter</code> <code>Optional[str]</code> <p>Optional regex to filter fields to display</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>HTML string representation of the confusion matrix</p> Source code in <code>stickler/structured_object_evaluator/utils/pretty_print.py</code> <pre><code>def print_confusion_matrix_html(\n    results: Dict[str, Any],\n    field_filter: Optional[str] = None,\n    nested_detail: str = \"standard\",\n) -&gt; str:\n    \"\"\"\n    Generate HTML representation of confusion matrix metrics for Jupyter notebooks.\n\n    Args:\n        results: Results dictionary from StructuredModelEvaluator.evaluate()\n        field_filter: Optional regex to filter fields to display\n\n    Returns:\n        str: HTML string representation of the confusion matrix\n    \"\"\"\n    # This is a placeholder for HTML output formatting\n    # Implement this if you need to display results in Jupyter notebooks\n    # with richer HTML formatting, tables, and visualizations\n\n    # For now, use the same output as the terminal version\n    from io import StringIO\n    import sys\n\n    # Capture output in a string\n    old_stdout = sys.stdout\n    mystdout = StringIO()\n    sys.stdout = mystdout\n\n    print_confusion_matrix(\n        results, field_filter, use_color=False, nested_detail=nested_detail\n    )\n\n    sys.stdout = old_stdout\n\n    # Return the captured output\n    return f\"&lt;pre&gt;{mystdout.getvalue()}&lt;/pre&gt;\"\n</code></pre>"}]}